{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Analysis of SV Features\n",
        "\n",
        "This notebook performs statistical analysis on the extracted genomic features\n",
        "to assess their discriminative power for TP vs FP classification.\n",
        "\n",
        "Input:\n",
        "- CSV files with computed features from feature extraction notebook\n",
        "\n",
        "Output:\n",
        "- Statistical test results\n",
        "- Feature distribution plots  \n",
        "- Feature ranking table for paper\n",
        "\n",
        "Analysis:\n",
        "- Mann-Whitney U tests for TP vs FP differences\n",
        "- Effect size calculations (Cohen's d)\n",
        "- Feature ranking by discriminative power"
      ],
      "metadata": {
        "id": "y9vSOkgHRkox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k7vwaLPRg5e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import mannwhitneyu, ks_2samp\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the feature data\n",
        "df = pd.read_csv('../data/processed/SV_Features_Dataset.csv')\n",
        "\n",
        "print(f\"Loaded {len(df):,} variants from {df['dataset'].nunique()} datasets\")\n",
        "print(f\"TP: {len(df[df['label'] == 'TP']):,}, FP: {len(df[df['label'] == 'FP']):,}\")\n",
        "\n",
        "FEATURES = [\n",
        "    'log_svlen', 'depth_ratio', 'depth_mad', 'ab', 'cn_slop',\n",
        "    'mq_drop', 'clip_frac', 'split_reads', 'read_len_med', 'strand_bias',\n",
        "    'gc_frac', 'homopolymer_max', 'lcr_mask',\n",
        "    'support_read', 'svtype_DEL'\n",
        "]\n",
        "\n",
        "FEATURE_CATEGORIES = {\n",
        "    'Size & Copy Number': ['log_svlen', 'depth_ratio', 'depth_mad', 'ab', 'cn_slop'],\n",
        "    'Read Quality & Mapping': ['mq_drop', 'clip_frac', 'split_reads', 'read_len_med', 'strand_bias'],\n",
        "    'Sequence Context': ['gc_frac', 'homopolymer_max', 'lcr_mask'],\n",
        "    'Caller-Specific': ['support_read', 'svtype_DEL']\n",
        "}"
      ],
      "metadata": {
        "id": "ZDZN3Jf7Roc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feature availability:\")\n",
        "for feature in FEATURES:\n",
        "    if feature in df.columns:\n",
        "        available = df[feature].notna().sum()\n",
        "        total = len(df)\n",
        "        pct = available / total * 100\n",
        "        print(f\"{feature:<18} {available:>8,}/{total:<8,} ({pct:>5.1f}%)\")\n",
        "    else:\n",
        "        print(f\"{feature:<18} {'MISSING':<20}\")"
      ],
      "metadata": {
        "id": "e0zbGwblSQRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_effect_size(tp_data, fp_data):\n",
        "    \"\"\"Calculate Cohen's d effect size\"\"\"\n",
        "    if len(tp_data) < 2 or len(fp_data) < 2:\n",
        "        return np.nan\n",
        "\n",
        "    tp_mean, tp_std = tp_data.mean(), tp_data.std()\n",
        "    fp_mean, fp_std = fp_data.mean(), fp_data.std()\n",
        "\n",
        "    pooled_std = np.sqrt(((len(tp_data) - 1) * tp_std**2 + (len(fp_data) - 1) * fp_std**2) /\n",
        "                        (len(tp_data) + len(fp_data) - 2))\n",
        "\n",
        "    if pooled_std == 0:\n",
        "        return np.nan\n",
        "\n",
        "    return abs(tp_mean - fp_mean) / pooled_std\n",
        "\n",
        "def analyze_feature_by_dataset(df, feature, dataset):\n",
        "    \"\"\"Analyze a single feature for a dataset\"\"\"\n",
        "    dataset_data = df[df['dataset'] == dataset]\n",
        "\n",
        "    tp_data = dataset_data[dataset_data['label'] == 'TP'][feature].dropna()\n",
        "    fp_data = dataset_data[dataset_data['label'] == 'FP'][feature].dropna()\n",
        "\n",
        "    if len(tp_data) < 50 or len(fp_data) < 50:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Statistical tests\n",
        "        mw_stat, mw_p = mannwhitneyu(tp_data, fp_data, alternative='two-sided')\n",
        "        ks_stat, ks_p = ks_2samp(tp_data, fp_data)\n",
        "        effect_size = calculate_effect_size(tp_data, fp_data)\n",
        "\n",
        "        return {\n",
        "            'dataset': dataset,\n",
        "            'feature': feature,\n",
        "            'tp_count': len(tp_data),\n",
        "            'fp_count': len(fp_data),\n",
        "            'tp_mean': tp_data.mean(),\n",
        "            'fp_mean': fp_data.mean(),\n",
        "            'effect_size': effect_size,\n",
        "            'mann_whitney_p': mw_p,\n",
        "            'ks_p_value': ks_p\n",
        "        }\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "Ako4p1e4SZ5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze all features across all datasets\n",
        "results = []\n",
        "for dataset in df['dataset'].unique():\n",
        "    print(f\"\\nAnalyzing {dataset}:\")\n",
        "    for feature in FEATURES:\n",
        "        if feature in df.columns:\n",
        "            result = analyze_feature_by_dataset(df, feature, dataset)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "                if result['mann_whitney_p'] < 0.001:\n",
        "                    sig = \"***\"\n",
        "                elif result['mann_whitney_p'] < 0.01:\n",
        "                    sig = \"**\"\n",
        "                elif result['mann_whitney_p'] < 0.05:\n",
        "                    sig = \"*\"\n",
        "                else:\n",
        "                    sig = \"\"\n",
        "                print(f\"  {feature:<18} p={result['mann_whitney_p']:.2e}, d={result['effect_size']:.3f} {sig}\")\n",
        "\n",
        "stats_df = pd.DataFrame(results)\n",
        "print(f\"\\nTotal statistical tests: {len(stats_df)}\")"
      ],
      "metadata": {
        "id": "sNU8M8xESacB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(stats_df) > 0:\n",
        "    # Summarize by feature across all datasets\n",
        "    feature_summary = stats_df.groupby('feature').agg({\n",
        "        'effect_size': 'mean',\n",
        "        'mann_whitney_p': 'min',\n",
        "        'dataset': 'count'\n",
        "    }).rename(columns={'dataset': 'n_datasets'})\n",
        "\n",
        "    feature_summary['highly_significant'] = (stats_df.groupby('feature')['mann_whitney_p'] < 0.001).sum()\n",
        "    feature_summary = feature_summary.sort_values('effect_size', ascending=False)\n",
        "\n",
        "    print(\"\\nTop discriminative features:\")\n",
        "    print(f\"{'Rank':<4} {'Feature':<18} {'Avg Effect':<12} {'Min P-value':<12} {'Datasets':<8}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, (feature, row) in enumerate(feature_summary.head(10).iterrows(), 1):\n",
        "        print(f\"{i:<4} {feature:<18} {row['effect_size']:<12.3f} {row['mann_whitney_p']:<12.2e} {row['n_datasets']:<8}\")\n",
        "\n",
        "    # Save results\n",
        "    stats_df.to_csv('../data/processed/feature_statistics.csv', index=False)\n",
        "    feature_summary.to_csv('../data/processed/feature_ranking.csv')\n",
        "\n",
        "else:\n",
        "    print(\"No statistical results generated\")"
      ],
      "metadata": {
        "id": "xhtp6aeQSbo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create distribution plots for all features\n",
        "n_features = len(FEATURES)\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(n_features / n_cols))\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(FEATURES):\n",
        "    ax = axes[i]\n",
        "\n",
        "    if feature not in df.columns:\n",
        "        ax.text(0.5, 0.5, f'{feature}\\nMISSING', ha='center', va='center', transform=ax.transAxes)\n",
        "        continue\n",
        "\n",
        "    tp_data = df[df['label'] == 'TP'][feature].dropna()\n",
        "    fp_data = df[df['label'] == 'FP'][feature].dropna()\n",
        "\n",
        "    if len(tp_data) == 0 or len(fp_data) == 0:\n",
        "        ax.text(0.5, 0.5, f'{feature}\\nNO DATA', ha='center', va='center', transform=ax.transAxes)\n",
        "        continue\n",
        "\n",
        "    # Check if binary feature\n",
        "    if feature in ['lcr_mask', 'svtype_DEL'] or tp_data.nunique() <= 5:\n",
        "        # Bar plot for binary features\n",
        "        tp_counts = tp_data.value_counts(normalize=True)\n",
        "        fp_counts = fp_data.value_counts(normalize=True)\n",
        "\n",
        "        all_values = sorted(set(tp_counts.index) | set(fp_counts.index))\n",
        "        tp_props = [tp_counts.get(v, 0) for v in all_values]\n",
        "        fp_props = [fp_counts.get(v, 0) for v in all_values]\n",
        "\n",
        "        x = np.arange(len(all_values))\n",
        "        width = 0.35\n",
        "\n",
        "        ax.bar(x - width/2, tp_props, width, label='TP', alpha=0.7)\n",
        "        ax.bar(x + width/2, fp_props, width, label='FP', alpha=0.7)\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(all_values)\n",
        "        ax.set_ylabel('Proportion')\n",
        "    else:\n",
        "        # Histogram for continuous features\n",
        "        bins = min(50, max(10, int(np.sqrt(min(len(tp_data), len(fp_data))))))\n",
        "\n",
        "        ax.hist(tp_data, bins=bins, alpha=0.6, label=f'TP (n={len(tp_data):,})', density=True)\n",
        "        ax.hist(fp_data, bins=bins, alpha=0.6, label=f'FP (n={len(fp_data):,})', density=True)\n",
        "\n",
        "        # Add mean lines\n",
        "        ax.axvline(tp_data.mean(), color='blue', linestyle='--', alpha=0.8)\n",
        "        ax.axvline(fp_data.mean(), color='orange', linestyle='--', alpha=0.8)\n",
        "\n",
        "        ax.set_ylabel('Density')\n",
        "\n",
        "    # Add statistical info\n",
        "    if len(tp_data) > 10 and len(fp_data) > 10:\n",
        "        try:\n",
        "            _, p_val = mannwhitneyu(tp_data, fp_data, alternative='two-sided')\n",
        "            effect_size = calculate_effect_size(tp_data, fp_data)\n",
        "\n",
        "            sig_stars = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
        "            title = f'{feature}\\np={p_val:.2e} {sig_stars}, d={effect_size:.3f}'\n",
        "        except:\n",
        "            title = feature\n",
        "    else:\n",
        "        title = feature\n",
        "\n",
        "    ax.set_title(title, fontsize=10)\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Remove empty subplots\n",
        "for i in range(n_features, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/feature_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jrOQPvtLSdnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(stats_df) > 0:\n",
        "    # Get top 8 features by effect size\n",
        "    top_features = feature_summary.head(8).index.tolist()\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, feature in enumerate(top_features):\n",
        "        ax = axes[i]\n",
        "\n",
        "        if feature not in df.columns:\n",
        "            continue\n",
        "\n",
        "        tp_data = df[df['label'] == 'TP'][feature].dropna()\n",
        "        fp_data = df[df['label'] == 'FP'][feature].dropna()\n",
        "\n",
        "        if len(tp_data) == 0 or len(fp_data) == 0:\n",
        "            continue\n",
        "\n",
        "        # Skip binary features for violin plots\n",
        "        if tp_data.nunique() <= 5:\n",
        "            continue\n",
        "\n",
        "        # Create combined data for seaborn\n",
        "        combined_data = pd.DataFrame({\n",
        "            'value': pd.concat([tp_data, fp_data]),\n",
        "            'label': ['TP'] * len(tp_data) + ['FP'] * len(fp_data)\n",
        "        })\n",
        "\n",
        "        sns.violinplot(data=combined_data, x='label', y='value', ax=ax)\n",
        "\n",
        "        # Add median points\n",
        "        tp_median = tp_data.median()\n",
        "        fp_median = fp_data.median()\n",
        "        ax.plot(0, tp_median, 'wo', markersize=8, markeredgecolor='black')\n",
        "        ax.plot(1, fp_median, 'wo', markersize=8, markeredgecolor='black')\n",
        "\n",
        "        ax.set_title(feature, fontsize=12)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle('Top Discriminative Features: Violin Plots', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../figures/top_features_violin.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-cJhAKUbSkTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(stats_df) > 0:\n",
        "    # Create summary table\n",
        "    print(\"Statistical significance of differences in linear features per TP/FP by dataset\")\n",
        "    print(\"\\nFeature significance across datasets:\")\n",
        "\n",
        "    for feature in FEATURES:\n",
        "        if feature in stats_df['feature'].values:\n",
        "            feature_data = stats_df[stats_df['feature'] == feature]\n",
        "\n",
        "            sig_count = sum(feature_data['mann_whitney_p'] < 0.01)\n",
        "            total_datasets = len(feature_data)\n",
        "            avg_effect = feature_data['effect_size'].mean()\n",
        "\n",
        "            print(f\"{feature:<18} {sig_count}/{total_datasets} datasets significant, avg effect: {avg_effect:.3f}\")\n",
        "\n",
        "    # Save table\n",
        "    paper_table = stats_df.pivot_table(\n",
        "        index='feature',\n",
        "        columns='dataset',\n",
        "        values=['mann_whitney_p', 'effect_size'],\n",
        "        aggfunc='first'\n",
        "    )\n",
        "    paper_table.to_csv('../data/processed/paper_summary_table.csv')\n",
        "\n",
        "    print(f\"\\nSummary: {len(stats_df[stats_df['mann_whitney_p'] < 0.01])} statistically significant feature-dataset combinations\")\n",
        "    print(f\"Out of {len(stats_df)} total tests ({len(stats_df[stats_df['mann_whitney_p'] < 0.01])/len(stats_df)*100:.1f}% significant)\")\n",
        "\n",
        "print(\"\\nAnalysis complete.\")\n",
        "print(\"Results saved to:\")\n",
        "print(\"  - ../data/processed/feature_statistics.csv\")\n",
        "print(\"  - ../data/processed/feature_ranking.csv\")\n",
        "print(\"  - ../figures/feature_distributions.png\")\n",
        "print(\"  - ../figures/top_features_violin.png\")"
      ],
      "metadata": {
        "id": "_llKjOQMSlk5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}