{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Feature Extraction for Structural Variants***\n",
        "\n",
        "This notebook extracts the 15 genomic features from BAM files and VCF files\n",
        "for SV classification. The output is saved as CSV files for downstream analysis.\n",
        "\n",
        "Input:\n",
        "- BAM files (alignment data)\n",
        "- VCF files (TP and FP structural variants)  \n",
        "- Reference genome files\n",
        "\n",
        "Output:\n",
        "- CSV files with computed features"
      ],
      "metadata": {
        "id": "csZfbeUDNfFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from cyvcf2 import VCF\n",
        "import pysam\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import mannwhitneyu, ks_2samp\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "S43eHnDnL4MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujrN7yEOINx9"
      },
      "outputs": [],
      "source": [
        "# Dataset configuration - UPDATE THESE PATHS FOR YOUR SETUP\n",
        "DATASETS = {\n",
        "    'HG002_GRCh37': {\n",
        "        'bam': '../data/raw/HG002_GRCh37_ONT-UL_UCSC_20200508.phased.bam',\n",
        "        'tp_comp_vcf': '../data/raw/bench-HG002_GRCh37_noqc/tp-comp.vcf.gz',\n",
        "        'fp_vcf': '../data/raw/bench-HG002_GRCh37_noqc/fp.vcf.gz',\n",
        "        'ref': '../data/raw/GRCh37.fna'\n",
        "    },\n",
        "    'HG002_GRCh38': {\n",
        "        'bam': '../data/raw/HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam',\n",
        "        'tp_comp_vcf': '../data/raw/bench-HG002_GRCh38-GIABv3_noqc/tp-comp.vcf.gz',\n",
        "        'fp_vcf': '../data/raw/bench-HG002_GRCh38-GIABv3_noqc/fp.vcf.gz',\n",
        "        'ref': '../data/raw/GRCh38.fa'\n",
        "    },\n",
        "    'HG005_GRCh38': {\n",
        "        'bam': '../data/raw/HG005_GRCh38_ONT-UL_UCSC_20200109.phased.bam',\n",
        "        'tp_comp_vcf': '../data/raw/bench-HG005_GRCh38_noqc/tp-comp.vcf.gz',\n",
        "        'fp_vcf': '../data/raw/bench-HG005_GRCh38_noqc/fp.vcf.gz',\n",
        "        'ref': '../data/raw/GRCh38.fa'\n",
        "    }\n",
        "}\n",
        "\n",
        "# NCBI chromosome mapping for GRCh37\n",
        "NCBI_CHROM_MAP = {\n",
        "    '1': 'NC_000001.10', '2': 'NC_000002.11', '3': 'NC_000003.11',\n",
        "    '4': 'NC_000004.11', '5': 'NC_000005.9', '6': 'NC_000006.11',\n",
        "    '7': 'NC_000007.13', '8': 'NC_000008.10', '9': 'NC_000009.11',\n",
        "    '10': 'NC_000010.10', '11': 'NC_000011.9', '12': 'NC_000012.11',\n",
        "    '13': 'NC_000013.10', '14': 'NC_000014.8', '15': 'NC_000015.9',\n",
        "    '16': 'NC_000016.9', '17': 'NC_000017.10', '18': 'NC_000018.9',\n",
        "    '19': 'NC_000019.9', '20': 'NC_000020.10', '21': 'NC_000021.8',\n",
        "    '22': 'NC_000022.10', 'X': 'NC_000023.10', 'Y': 'NC_000024.9',\n",
        "    'MT': 'NC_012920.1'\n",
        "}\n",
        "\n",
        "FEATURES = [\n",
        "    'log_svlen', 'depth_ratio', 'depth_mad', 'ab', 'cn_slop',\n",
        "    'mq_drop', 'clip_frac', 'split_reads', 'read_len_med', 'strand_bias',\n",
        "    'gc_frac', 'homopolymer_max', 'lcr_mask',\n",
        "    'support_read', 'svtype_DEL'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_existing_data(dataset_name):\n",
        "    \"\"\"Check for existing processed data\"\"\"\n",
        "    complete_file = f'data/processed/{dataset_name}_complete_features.csv'\n",
        "\n",
        "    if os.path.exists(complete_file):\n",
        "        try:\n",
        "            existing_data = pd.read_csv(complete_file)\n",
        "            existing_variants = set()\n",
        "            for _, row in existing_data.iterrows():\n",
        "                variant_id = f\"{row.chrom}:{row.pos}:{row.end}:{row.label}\"\n",
        "                existing_variants.add(variant_id)\n",
        "\n",
        "            print(f\"Found existing data for {dataset_name}: {len(existing_data):,} variants\")\n",
        "            return existing_data, existing_variants\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading existing file: {e}\")\n",
        "\n",
        "    print(f\"No existing data found for {dataset_name}\")\n",
        "    return None, set()\n",
        "\n",
        "def check_dataset_files(dataset_name, dataset_info):\n",
        "    \"\"\"Check if all required files exist\"\"\"\n",
        "    print(f\"\\nChecking {dataset_name}:\")\n",
        "\n",
        "    required_files = ['bam', 'tp_comp_vcf', 'fp_vcf', 'ref']\n",
        "    all_exist = True\n",
        "\n",
        "    for file_type in required_files:\n",
        "        filepath = dataset_info[file_type]\n",
        "        exists = os.path.exists(filepath)\n",
        "        status = \"OK\" if exists else \"MISSING\"\n",
        "        print(f\"  {file_type}: {status}\")\n",
        "        if not exists:\n",
        "            all_exist = False\n",
        "\n",
        "    return all_exist\n",
        "\n",
        "def load_variants_with_filtering(vcf_path, label, existing_variants):\n",
        "    \"\"\"Load variants, filtering out already processed ones\"\"\"\n",
        "    variants = []\n",
        "    vcf = VCF(vcf_path)\n",
        "\n",
        "    total_count = 0\n",
        "    new_count = 0\n",
        "\n",
        "    for variant in vcf:\n",
        "        total_count += 1\n",
        "\n",
        "        var_info = {\n",
        "            'label': label,\n",
        "            'chrom': variant.CHROM,\n",
        "            'pos': variant.POS,\n",
        "            'end': variant.end if hasattr(variant, 'end') else variant.POS + 1,\n",
        "            'svlen': variant.INFO.get('SVLEN', 0),\n",
        "            'svtype': variant.INFO.get('SVTYPE', 'UNK')\n",
        "        }\n",
        "\n",
        "        variant_id = f\"{var_info['chrom']}:{var_info['pos']}:{var_info['end']}:{label}\"\n",
        "\n",
        "        if variant_id not in existing_variants:\n",
        "            variants.append((variant, var_info))\n",
        "            new_count += 1\n",
        "\n",
        "    vcf.close()\n",
        "    print(f\"    {label}: {new_count:,} new / {total_count:,} total\")\n",
        "    return variants"
      ],
      "metadata": {
        "id": "cStbJVxiLNHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_size_features(bam_file, ref_file, chrom, start, end, svlen, svtype):\n",
        "    \"\"\"Compute size and copy number features\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # log_svlen\n",
        "    abs_svlen = abs(svlen) if svlen != 0 else 1\n",
        "    features['log_svlen'] = np.log10(abs_svlen)\n",
        "\n",
        "    try:\n",
        "        # Handle insertions differently\n",
        "        if svtype == 'INS':\n",
        "            window_size = max(100, abs(svlen) // 10)\n",
        "            sv_start = start - window_size // 2\n",
        "            sv_end = start + window_size // 2\n",
        "            sv_length = window_size\n",
        "        else:\n",
        "            sv_start = start\n",
        "            sv_end = end\n",
        "            sv_length = end - start\n",
        "\n",
        "        flank = 1000\n",
        "\n",
        "        # Get reads\n",
        "        sv_reads = list(bam_file.fetch(chrom, sv_start, sv_end))\n",
        "        left_reads = list(bam_file.fetch(chrom, max(0, sv_start - flank), sv_start))\n",
        "        right_reads = list(bam_file.fetch(chrom, sv_end, sv_end + flank))\n",
        "\n",
        "        # Calculate coverages\n",
        "        sv_coverage = len(sv_reads) / sv_length if sv_length > 0 else 0\n",
        "        left_coverage = len(left_reads) / flank\n",
        "        right_coverage = len(right_reads) / flank\n",
        "        control_coverage = (left_coverage + right_coverage) / 2\n",
        "\n",
        "        # depth_ratio\n",
        "        features['depth_ratio'] = sv_coverage / control_coverage if control_coverage > 0 else np.nan\n",
        "\n",
        "        # depth_mad\n",
        "        coverage_array = np.zeros(sv_length)\n",
        "        for read in sv_reads:\n",
        "            read_start = max(0, read.reference_start - sv_start)\n",
        "            read_end = min(sv_length, read.reference_end - sv_start)\n",
        "            if read_end > read_start:\n",
        "                coverage_array[read_start:read_end] += 1\n",
        "\n",
        "        if len(coverage_array) > 0 and sv_length > 0:\n",
        "            median_cov = np.median(coverage_array)\n",
        "            features['depth_mad'] = np.median(np.abs(coverage_array - median_cov))\n",
        "        else:\n",
        "            features['depth_mad'] = np.nan\n",
        "\n",
        "        # ab (allele balance)\n",
        "        total_coverage = sv_coverage + control_coverage\n",
        "        features['ab'] = sv_coverage / total_coverage if total_coverage > 0 else np.nan\n",
        "\n",
        "        # cn_slop (distal comparison)\n",
        "        try:\n",
        "            distal_start = max(0, start - 25000)\n",
        "            distal_end = start - 5000\n",
        "            if distal_end > distal_start:\n",
        "                distal_reads = list(bam_file.fetch(chrom, distal_start, distal_end))\n",
        "                distal_coverage = len(distal_reads) / (distal_end - distal_start)\n",
        "                features['cn_slop'] = sv_coverage / distal_coverage if distal_coverage > 0 else np.nan\n",
        "            else:\n",
        "                features['cn_slop'] = np.nan\n",
        "        except:\n",
        "            features['cn_slop'] = np.nan\n",
        "\n",
        "    except Exception as e:\n",
        "        features.update({\n",
        "            'depth_ratio': np.nan, 'depth_mad': np.nan,\n",
        "            'ab': np.nan, 'cn_slop': np.nan\n",
        "        })\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "c0rf82VKMCt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_read_quality_features(bam_file, chrom, start, end, svtype):\n",
        "    \"\"\"Compute read quality and mapping features\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    try:\n",
        "        # Handle insertions differently\n",
        "        if svtype == 'INS':\n",
        "            window_size = 200\n",
        "            sv_start = start - window_size // 2\n",
        "            sv_end = start + window_size // 2\n",
        "        else:\n",
        "            sv_start = start\n",
        "            sv_end = end\n",
        "\n",
        "        # Get reads\n",
        "        sv_reads = list(bam_file.fetch(chrom, sv_start, sv_end))\n",
        "        left_reads = list(bam_file.fetch(chrom, max(0, start-1000), start))\n",
        "        right_reads = list(bam_file.fetch(chrom, end, end+1000))\n",
        "\n",
        "        # mq_drop\n",
        "        sv_mq = [read.mapping_quality for read in sv_reads if read.mapping_quality is not None]\n",
        "        flank_mq = [read.mapping_quality for read in left_reads + right_reads if read.mapping_quality is not None]\n",
        "\n",
        "        sv_mq_median = np.median(sv_mq) if sv_mq else 0\n",
        "        flank_mq_median = np.median(flank_mq) if flank_mq else 0\n",
        "        features['mq_drop'] = flank_mq_median - sv_mq_median\n",
        "\n",
        "        # clip_frac, split_reads, read_len_med\n",
        "        clipped_reads = 0\n",
        "        split_read_count = 0\n",
        "        read_lengths = []\n",
        "\n",
        "        for read in sv_reads:\n",
        "            # Check clipping\n",
        "            if read.cigartuples:\n",
        "                for op, length in read.cigartuples:\n",
        "                    if op in [4, 5] and length >= 10:\n",
        "                        clipped_reads += 1\n",
        "                        break\n",
        "\n",
        "            # Check split reads\n",
        "            if read.has_tag('SA') or read.is_supplementary:\n",
        "                split_read_count += 1\n",
        "\n",
        "            # Read length\n",
        "            if read.query_length is not None:\n",
        "                read_lengths.append(read.query_length)\n",
        "\n",
        "        features['clip_frac'] = clipped_reads / len(sv_reads) if sv_reads else 0\n",
        "        features['split_reads'] = split_read_count\n",
        "        features['read_len_med'] = np.median(read_lengths) if read_lengths else np.nan\n",
        "\n",
        "        # strand_bias\n",
        "        forward_reads = sum(1 for read in sv_reads if not read.is_reverse)\n",
        "        reverse_reads = len(sv_reads) - forward_reads\n",
        "\n",
        "        if len(sv_reads) > 0:\n",
        "            forward_frac = forward_reads / len(sv_reads)\n",
        "            reverse_frac = reverse_reads / len(sv_reads)\n",
        "            features['strand_bias'] = abs(forward_frac - reverse_frac)\n",
        "        else:\n",
        "            features['strand_bias'] = np.nan\n",
        "\n",
        "    except Exception as e:\n",
        "        features.update({\n",
        "            'mq_drop': np.nan, 'clip_frac': np.nan, 'split_reads': 0,\n",
        "            'read_len_med': np.nan, 'strand_bias': np.nan\n",
        "        })\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "9BuCoDG7MddZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sequence_context_features(ref_file, chrom, start, end):\n",
        "    \"\"\"Compute sequence context features\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    try:\n",
        "        # Get sequence with flanks\n",
        "        flank = 500\n",
        "        seq_start = max(0, start - flank)\n",
        "        seq_end = end + flank\n",
        "\n",
        "        # Handle chromosome naming variations\n",
        "        sequence = \"\"\n",
        "        test_chroms = [\n",
        "            chrom,\n",
        "            chrom[3:] if chrom.startswith('chr') else f'chr{chrom}',\n",
        "            NCBI_CHROM_MAP.get(chrom.replace('chr', ''), None)\n",
        "        ]\n",
        "        test_chroms = [c for c in test_chroms if c is not None]\n",
        "\n",
        "        for test_chrom in test_chroms:\n",
        "            if test_chrom in ref_file.references:\n",
        "                sequence = ref_file.fetch(test_chrom, seq_start, seq_end).upper()\n",
        "                break\n",
        "\n",
        "        if sequence:\n",
        "            # gc_frac\n",
        "            gc_count = sequence.count('G') + sequence.count('C')\n",
        "            features['gc_frac'] = gc_count / len(sequence)\n",
        "\n",
        "            # homopolymer_max\n",
        "            max_homopolymer = 0\n",
        "            current_base = ''\n",
        "            current_count = 0\n",
        "\n",
        "            for base in sequence:\n",
        "                if base == current_base:\n",
        "                    current_count += 1\n",
        "                else:\n",
        "                    max_homopolymer = max(max_homopolymer, current_count)\n",
        "                    current_base = base\n",
        "                    current_count = 1\n",
        "            max_homopolymer = max(max_homopolymer, current_count)\n",
        "            features['homopolymer_max'] = max_homopolymer\n",
        "\n",
        "            # lcr_mask\n",
        "            distinct_bases = len(set(sequence))\n",
        "            features['lcr_mask'] = 1 if distinct_bases <= 2 else 0\n",
        "        else:\n",
        "            features.update({\n",
        "                'gc_frac': np.nan, 'homopolymer_max': np.nan, 'lcr_mask': np.nan\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        features.update({\n",
        "            'gc_frac': np.nan, 'homopolymer_max': np.nan, 'lcr_mask': np.nan\n",
        "        })\n",
        "\n",
        "    return features\n",
        "\n",
        "def compute_all_features_for_variant(variant, bam_file, ref_file):\n",
        "    \"\"\"Compute all 15 features for a single variant\"\"\"\n",
        "    # Basic variant info\n",
        "    chrom = variant.CHROM\n",
        "    start = variant.POS\n",
        "    end = variant.end if hasattr(variant, 'end') and variant.end else start + 1\n",
        "    svlen = variant.INFO.get('SVLEN', end - start)\n",
        "    svtype = variant.INFO.get('SVTYPE', 'UNK')\n",
        "\n",
        "    # Initialize features\n",
        "    features = {\n",
        "        'chrom': chrom, 'pos': start, 'end': end,\n",
        "        'svlen': svlen, 'svtype': svtype\n",
        "    }\n",
        "\n",
        "    # Compute feature categories\n",
        "    features.update(compute_size_features(bam_file, ref_file, chrom, start, end, svlen, svtype))\n",
        "    features.update(compute_read_quality_features(bam_file, chrom, start, end, svtype))\n",
        "    features.update(compute_sequence_context_features(ref_file, chrom, start, end))\n",
        "\n",
        "    # Caller features\n",
        "    features['support_read'] = variant.INFO.get('SUPPORT', np.nan)\n",
        "    features['svtype_DEL'] = 1 if svtype == 'DEL' else 0\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "vHMKRMgYMhjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(dataset_name, dataset_info):\n",
        "    \"\"\"Process a single dataset to extract features\"\"\"\n",
        "    print(f\"\\nProcessing {dataset_name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check existing data\n",
        "    existing_data, existing_variants = check_existing_data(dataset_name)\n",
        "\n",
        "    # Load new variants only\n",
        "    print(\"Loading variants...\")\n",
        "    tp_variants = load_variants_with_filtering(\n",
        "        dataset_info['tp_comp_vcf'], 'TP', existing_variants\n",
        "    )\n",
        "    fp_variants = load_variants_with_filtering(\n",
        "        dataset_info['fp_vcf'], 'FP', existing_variants\n",
        "    )\n",
        "\n",
        "    new_variants = tp_variants + fp_variants\n",
        "\n",
        "    if len(new_variants) == 0:\n",
        "        print(\"All variants already processed\")\n",
        "        return existing_data\n",
        "\n",
        "    print(f\"Processing {len(new_variants):,} new variants...\")\n",
        "\n",
        "    # Open files\n",
        "    try:\n",
        "        bam_file = pysam.AlignmentFile(dataset_info['bam'], 'rb')\n",
        "        ref_file = pysam.FastaFile(dataset_info['ref'])\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening files: {e}\")\n",
        "        return existing_data\n",
        "\n",
        "    # Process new variants\n",
        "    new_results = []\n",
        "    failed_count = 0\n",
        "\n",
        "    with tqdm(total=len(new_variants), desc=\"Computing features\") as pbar:\n",
        "        for i, (variant, var_info) in enumerate(new_variants):\n",
        "            try:\n",
        "                features = compute_all_features_for_variant(variant, bam_file, ref_file)\n",
        "                features.update(var_info)\n",
        "                features['dataset'] = dataset_name\n",
        "                new_results.append(features)\n",
        "\n",
        "                pbar.set_postfix({'Success': f\"{len(new_results)/(i+1)*100:.1f}%\"})\n",
        "            except Exception as e:\n",
        "                failed_count += 1\n",
        "                pbar.set_postfix({'Failed': failed_count})\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Close files\n",
        "    bam_file.close()\n",
        "    ref_file.close()\n",
        "\n",
        "    # Combine data\n",
        "    if new_results:\n",
        "        new_df = pd.DataFrame(new_results)\n",
        "\n",
        "        if existing_data is not None:\n",
        "            combined_df = pd.concat([existing_data, new_df], ignore_index=True)\n",
        "            print(f\"Combined: {len(existing_data):,} existing + {len(new_df):,} new = {len(combined_df):,} total\")\n",
        "        else:\n",
        "            combined_df = new_df\n",
        "            print(f\"New dataset: {len(new_df):,} variants\")\n",
        "\n",
        "        # Save complete dataset\n",
        "        os.makedirs('../data/processed', exist_ok=True)\n",
        "        output_file = f'../data/processed/{dataset_name}_complete_features.csv'\n",
        "        combined_df.to_csv(output_file, index=False)\n",
        "        print(f\"Saved: {output_file}\")\n",
        "\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"No new variants processed successfully\")\n",
        "        return existing_data"
      ],
      "metadata": {
        "id": "aeAMfMWWMdxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check datasets and process\n",
        "available_datasets = {}\n",
        "all_dataframes = []\n",
        "\n",
        "for dataset_name, dataset_info in DATASETS.items():\n",
        "    if check_dataset_files(dataset_name, dataset_info):\n",
        "        available_datasets[dataset_name] = dataset_info\n",
        "    else:\n",
        "        print(f\"Skipping {dataset_name} - missing files\")\n",
        "\n",
        "if not available_datasets:\n",
        "    print(\"No usable datasets found!\")\n",
        "else:\n",
        "    print(f\"\\nProcessing {len(available_datasets)} datasets\")\n",
        "\n",
        "    # Process each dataset\n",
        "    for dataset_name, dataset_info in available_datasets.items():\n",
        "        try:\n",
        "            df = process_dataset(dataset_name, dataset_info)\n",
        "            if df is not None and len(df) > 0:\n",
        "                all_dataframes.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {dataset_name}: {e}\")\n",
        "\n",
        "    if all_dataframes:\n",
        "        # Combine all data\n",
        "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "        print(f\"\\nFinal dataset summary:\")\n",
        "        print(f\"Total variants: {len(combined_df):,}\")\n",
        "        print(f\"Datasets: {list(combined_df['dataset'].unique())}\")\n",
        "        print(f\"TP variants: {len(combined_df[combined_df['label'] == 'TP']):,}\")\n",
        "        print(f\"FP variants: {len(combined_df[combined_df['label'] == 'FP']):,}\")\n",
        "\n",
        "        # Feature availability\n",
        "        print(f\"\\nFeature availability:\")\n",
        "        for feature in FEATURES:\n",
        "            if feature in combined_df.columns:\n",
        "                available = combined_df[feature].notna().sum()\n",
        "                total = len(combined_df)\n",
        "                pct = (available / total) * 100\n",
        "                print(f\"  {feature:<18} {available:>7,}/{total:<7,} ({pct:>5.1f}%)\")\n",
        "\n",
        "        # Save final dataset\n",
        "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        main_file = f'data/processed/complete_features_{timestamp}.csv'\n",
        "        combined_df.to_csv(main_file, index=False)\n",
        "        print(f\"\\nSaved complete dataset: {main_file}\")\n",
        "    else:\n",
        "        print(\"No data processed!\")"
      ],
      "metadata": {
        "id": "iNGbEN5HNNoH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}