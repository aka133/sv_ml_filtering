{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "VICReg latent extraction and analysis"
      ],
      "metadata": {
        "id": "epY25PlkNzjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration - GitHub repository structure\n",
        "DATA_DIR = '../data/processed/all_datasets_images_rgb'\n",
        "MODELS_DIR = '../data/processed/vicreg_experiments/models'\n",
        "SAVE_DIR = '../data/processed/vicreg_latents'\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
        "\n",
        "print(f\"Save directory: {SAVE_DIR}\")"
      ],
      "metadata": {
        "id": "DUN5p4d9OLe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "\n",
        "def find_champion_vicreg_model():\n",
        "    \"\"\"Find the best performing VICReg model automatically\"\"\"\n",
        "\n",
        "    print(\"Searching for champion VICReg model...\")\n",
        "\n",
        "    if not os.path.exists(MODELS_DIR):\n",
        "        print(f\"Models directory not found: {MODELS_DIR}\")\n",
        "        print(\"Run VICReg training first!\")\n",
        "        return None, None\n",
        "\n",
        "    best_acc = 0\n",
        "    best_path = None\n",
        "    best_info = None\n",
        "\n",
        "    # Search all saved models\n",
        "    for model_dir in os.listdir(MODELS_DIR):\n",
        "        model_path = os.path.join(MODELS_DIR, model_dir)\n",
        "        checkpoint_path = os.path.join(model_path, 'best_vicreg_model.pth')\n",
        "\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            try:\n",
        "                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "                # Only consider binary models for feature extraction\n",
        "                if checkpoint['num_classes'] == 2:\n",
        "                    acc = checkpoint['best_test_acc']\n",
        "                    if acc > best_acc:\n",
        "                        best_acc = acc\n",
        "                        best_path = checkpoint_path\n",
        "                        best_info = {\n",
        "                            'name': checkpoint['experiment_name'],\n",
        "                            'accuracy': acc,\n",
        "                            'auc': checkpoint.get('final_test_auc', 0),\n",
        "                            'path': checkpoint_path\n",
        "                        }\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {model_dir}: {str(e)[:50]}...\")\n",
        "                continue\n",
        "\n",
        "    if best_path is None:\n",
        "        print(\"No binary VICReg models found!\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Champion VICReg model found:\")\n",
        "    print(f\"   Name: {best_info['name']}\")\n",
        "    print(f\"   Accuracy: {best_info['accuracy']:.2f}%\")\n",
        "    print(f\"   AUC: {best_info['auc']:.3f}\")\n",
        "    print(f\"   Path: {best_path}\")\n",
        "\n",
        "    return best_path, best_info\n",
        "\n",
        "def load_champion_vicreg():\n",
        "    \"\"\"Load the champion VICReg model for feature extraction\"\"\"\n",
        "\n",
        "    champion_path, champion_info = find_champion_vicreg_model()\n",
        "    if champion_path is None:\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Loading champion VICReg...\")\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(champion_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "    # Recreate VICReg class (same as training)\n",
        "    class VICRegResNet(nn.Module):\n",
        "        def __init__(self, num_classes, dropout=0.2):\n",
        "            super().__init__()\n",
        "            self.num_classes = num_classes\n",
        "\n",
        "            print(\"Loading Facebook's VICReg ResNet50x2...\")\n",
        "            try:\n",
        "                self.backbone = torch.hub.load('facebookresearch/vicreg:main', 'resnet50x2', pretrained=True)\n",
        "                print(\"   Successfully loaded VICReg ResNet50x2\")\n",
        "            except Exception as e:\n",
        "                print(f\"   Failed to load VICReg model: {e}\")\n",
        "                print(\"   Falling back to standard Wide ResNet50-2...\")\n",
        "                self.backbone = torchvision.models.wide_resnet50_2(weights='IMAGENET1K_V1')\n",
        "\n",
        "            feature_dim = 4096  # VICReg ResNet50x2 feature dimension\n",
        "\n",
        "            # Remove original classifier\n",
        "            if hasattr(self.backbone, 'fc'):\n",
        "                self.backbone.fc = nn.Identity()\n",
        "            elif hasattr(self.backbone, 'head'):\n",
        "                self.backbone.head = nn.Identity()\n",
        "\n",
        "            # Freeze backbone\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # Classifier\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(feature_dim, num_classes)\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            with torch.no_grad():\n",
        "                features = self.backbone(x)\n",
        "            return self.classifier(features)\n",
        "\n",
        "    # Recreate and load model\n",
        "    model = VICRegResNet(\n",
        "        checkpoint['num_classes'],\n",
        "        checkpoint['model_config']['dropout_rate']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Extract backbone for feature extraction\n",
        "    feature_extractor = model.backbone\n",
        "    feature_extractor.to(device)\n",
        "    feature_extractor.eval()\n",
        "\n",
        "    print(f\"Loaded VICReg backbone for feature extraction\")\n",
        "\n",
        "    return feature_extractor, champion_info\n"
      ],
      "metadata": {
        "id": "yybkHmT4Oab8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class and data loading / feature extraction\n",
        "class FeatureExtractionDataset(Dataset):\n",
        "    \"\"\"Dataset for feature extraction with metadata tracking\"\"\"\n",
        "\n",
        "    def __init__(self, file_info_list, transform=None):\n",
        "        self.file_info = file_info_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        info = self.file_info[idx]\n",
        "\n",
        "        try:\n",
        "            # Load image\n",
        "            data = torch.load(info['filepath'], map_location='cpu')\n",
        "            if isinstance(data, dict):\n",
        "                image = data['image']\n",
        "            else:\n",
        "                image = data\n",
        "\n",
        "            # Handle channels (ensure RGB)\n",
        "            if image.shape[0] != 3:\n",
        "                if image.shape[0] < 3:\n",
        "                    padding = torch.zeros(3 - image.shape[0], *image.shape[1:])\n",
        "                    image = torch.cat([image, padding], dim=0)\n",
        "                else:\n",
        "                    image = image[:3]\n",
        "\n",
        "            # Normalize to [0,1]\n",
        "            if image.max() > 1:\n",
        "                image = image.float() / 255.0\n",
        "\n",
        "            success = True\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback for corrupted files\n",
        "            image = torch.zeros(3, 224, 224)\n",
        "            success = False\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(transforms.ToPILImage()(image))\n",
        "\n",
        "        return image, info, success\n",
        "\n",
        "def load_all_file_info():\n",
        "    \"\"\"Load info for ALL genomic data files\"\"\"\n",
        "\n",
        "    print(\"Scanning all genomic data files...\")\n",
        "\n",
        "    datasets = ['HG002_GRCh37', 'HG002_GRCh38', 'HG005_GRCh38']\n",
        "    all_file_info = []\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        dataset_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "        if not os.path.exists(dataset_path):\n",
        "            print(f\"Missing dataset: {dataset_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Scanning {dataset_name}...\")\n",
        "        filenames = [f for f in os.listdir(dataset_path) if f.endswith('.pt')]\n",
        "\n",
        "        for filename in tqdm(filenames, desc=f\"Processing {dataset_name}\"):\n",
        "            parts = filename[:-3].split('_')\n",
        "\n",
        "            if len(parts) >= 8:\n",
        "                try:\n",
        "                    label_str = parts[2]  # TP or FP\n",
        "                    svtype = parts[6]     # INS, DEL, etc.\n",
        "\n",
        "                    if label_str in ['TP', 'FP']:\n",
        "                        all_file_info.append({\n",
        "                            'filename': filename,\n",
        "                            'filepath': os.path.join(dataset_path, filename),\n",
        "                            'dataset': dataset_name,\n",
        "                            'label_str': label_str,\n",
        "                            'svtype': svtype,\n",
        "                            'binary_label': 1 if label_str == 'TP' else 0,\n",
        "                            'multiclass_label': 0 if label_str == 'FP' else (1 if svtype == 'DEL' else 2)\n",
        "                        })\n",
        "\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "    print(f\"Found {len(all_file_info)} total files\")\n",
        "\n",
        "    # Show breakdown\n",
        "    datasets_count = {}\n",
        "    labels_count = {}\n",
        "\n",
        "    for info in all_file_info:\n",
        "        datasets_count[info['dataset']] = datasets_count.get(info['dataset'], 0) + 1\n",
        "        labels_count[info['label_str']] = labels_count.get(info['label_str'], 0) + 1\n",
        "\n",
        "    print(\"Dataset breakdown:\")\n",
        "    for dataset, count in datasets_count.items():\n",
        "        print(f\"   {dataset}: {count:,} files\")\n",
        "\n",
        "    print(\"Label breakdown:\")\n",
        "    for label, count in labels_count.items():\n",
        "        print(f\"   {label}: {count:,} files\")\n",
        "\n",
        "    return all_file_info\n",
        "\n",
        "\n",
        "def extract_and_save_vicreg_features():\n",
        "    \"\"\"Extract and save VICReg features for all data samples\"\"\"\n",
        "\n",
        "    print(\"Starting comprehensive VICReg feature extraction...\")\n",
        "\n",
        "    # Load champion model\n",
        "    feature_extractor, champion_info = load_champion_vicreg()\n",
        "    if feature_extractor is None:\n",
        "        print(\"Failed to load champion model!\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Load all file info\n",
        "    all_file_info = load_all_file_info()\n",
        "\n",
        "    # Create transform (same as used in training)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.458, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = FeatureExtractionDataset(all_file_info, transform)\n",
        "\n",
        "    # Custom collate function to handle metadata\n",
        "    def collate_fn(batch):\n",
        "        images = torch.stack([item[0] for item in batch])\n",
        "        infos = [item[1] for item in batch]  # Keep as list of dicts\n",
        "        successes = torch.tensor([item[2] for item in batch])\n",
        "        return images, infos, successes\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # VICReg ResNet50x2 outputs 4096-dimensional features\n",
        "    feature_dim = 4096\n",
        "\n",
        "    # Prepare storage\n",
        "    total_samples = len(all_file_info)\n",
        "    features_array = np.zeros((total_samples, feature_dim), dtype=np.float32)\n",
        "    metadata_list = []\n",
        "\n",
        "    print(f\"Extracting features for {total_samples:,} samples...\")\n",
        "    print(f\"Feature dimension: {feature_dim}\")\n",
        "\n",
        "    sample_idx = 0\n",
        "    failed_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_images, batch_info, batch_success in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "            batch_images = batch_images.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            batch_features = feature_extractor(batch_images)\n",
        "            batch_features_np = batch_features.cpu().numpy()\n",
        "\n",
        "            # Handle variable batch sizes\n",
        "            actual_batch_size = batch_features_np.shape[0]\n",
        "            features_array[sample_idx:sample_idx + actual_batch_size] = batch_features_np\n",
        "\n",
        "            # Process metadata\n",
        "            for i, (info, success) in enumerate(zip(batch_info, batch_success)):\n",
        "                if i >= actual_batch_size:  # Safety check\n",
        "                    break\n",
        "\n",
        "                metadata_list.append({\n",
        "                    'sample_idx': sample_idx + i,\n",
        "                    'filename': info['filename'],\n",
        "                    'dataset': info['dataset'],\n",
        "                    'label_str': info['label_str'],\n",
        "                    'svtype': info['svtype'],\n",
        "                    'binary_label': info['binary_label'],\n",
        "                    'multiclass_label': info['multiclass_label'],\n",
        "                    'extraction_success': success.item(),\n",
        "                    'filepath': info['filepath']\n",
        "                })\n",
        "\n",
        "                if not success.item():\n",
        "                    failed_count += 1\n",
        "\n",
        "            sample_idx += actual_batch_size\n",
        "\n",
        "    print(f\"Feature extraction complete!\")\n",
        "    print(f\"   Successfully processed: {total_samples - failed_count:,}/{total_samples:,} samples\")\n",
        "    print(f\"   Failed files: {failed_count:,}\")\n",
        "\n",
        "    # Save features to HDF5\n",
        "    features_file = os.path.join(SAVE_DIR, 'vicreg_features.h5')\n",
        "    print(f\"Saving features to: {features_file}\")\n",
        "\n",
        "    with h5py.File(features_file, 'w') as f:\n",
        "        f.create_dataset('features', data=features_array, compression='gzip')\n",
        "        f.attrs['champion_model'] = champion_info['name']\n",
        "        f.attrs['champion_path'] = champion_info['path']\n",
        "        f.attrs['champion_accuracy'] = champion_info['accuracy']\n",
        "        f.attrs['champion_auc'] = champion_info['auc']\n",
        "        f.attrs['feature_dim'] = feature_dim\n",
        "        f.attrs['total_samples'] = total_samples\n",
        "        f.attrs['extraction_date'] = datetime.now().isoformat()\n",
        "\n",
        "    # Save metadata to CSV\n",
        "    metadata_file = os.path.join(SAVE_DIR, 'vicreg_metadata.csv')\n",
        "    print(f\"Saving metadata to: {metadata_file}\")\n",
        "\n",
        "    metadata_df = pd.DataFrame(metadata_list)\n",
        "    metadata_df.to_csv(metadata_file, index=False)\n",
        "\n",
        "    # Create summary\n",
        "    summary = {\n",
        "        'extraction_date': datetime.now().isoformat(),\n",
        "        'champion_model': {\n",
        "            'name': champion_info['name'],\n",
        "            'accuracy': champion_info['accuracy'],\n",
        "            'auc': champion_info['auc'],\n",
        "            'path': champion_info['path']\n",
        "        },\n",
        "        'data_summary': {\n",
        "            'total_samples': int(total_samples),\n",
        "            'successful_extractions': int(total_samples - failed_count),\n",
        "            'failed_extractions': int(failed_count),\n",
        "            'feature_dimension': int(feature_dim)\n",
        "        },\n",
        "        'files': {\n",
        "            'features_file': features_file,\n",
        "            'metadata_file': metadata_file,\n",
        "            'features_size_mb': round(features_array.nbytes / 1024**2, 1)\n",
        "        },\n",
        "        'data_breakdown': {\n",
        "            'datasets': {k: int(v) for k, v in metadata_df['dataset'].value_counts().items()},\n",
        "            'labels': {k: int(v) for k, v in metadata_df['label_str'].value_counts().items()},\n",
        "            'sv_types': {k: int(v) for k, v in metadata_df['svtype'].value_counts().items()}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    summary_file = os.path.join(SAVE_DIR, 'vicreg_extraction_summary.json')\n",
        "    print(f\"Saving summary to: {summary_file}\")\n",
        "\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nVICREG FEATURE EXTRACTION COMPLETE!\")\n",
        "    print(f\"Files saved in: {SAVE_DIR}\")\n",
        "    print(f\"   Features: vicreg_features.h5 ({summary['files']['features_size_mb']} MB)\")\n",
        "    print(f\"   Metadata: vicreg_metadata.csv\")\n",
        "    print(f\"   Summary: vicreg_extraction_summary.json\")\n",
        "\n",
        "    return features_array, metadata_df, summary"
      ],
      "metadata": {
        "id": "XgxumIgzOpc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for loading saved latents\n",
        "\n",
        "def load_saved_vicreg_features():\n",
        "    \"\"\"Load previously saved VICReg features\"\"\"\n",
        "\n",
        "    features_file = os.path.join(SAVE_DIR, 'vicreg_features.h5')\n",
        "    metadata_file = os.path.join(SAVE_DIR, 'vicreg_metadata.csv')\n",
        "    summary_file = os.path.join(SAVE_DIR, 'vicreg_extraction_summary.json')\n",
        "\n",
        "    if not os.path.exists(features_file):\n",
        "        print(f\"Features file not found: {features_file}\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\"Loading saved VICReg features...\")\n",
        "\n",
        "    # Load features\n",
        "    with h5py.File(features_file, 'r') as f:\n",
        "        features = f['features'][:]\n",
        "        attrs = dict(f.attrs)\n",
        "\n",
        "    # Load metadata\n",
        "    metadata_df = pd.read_csv(metadata_file)\n",
        "\n",
        "    # Load summary\n",
        "    with open(summary_file, 'r') as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(f\"Loaded {len(features):,} VICReg feature vectors\")\n",
        "    print(f\"   Feature dimension: {features.shape[1]}\")\n",
        "    print(f\"   Champion accuracy: {attrs.get('champion_accuracy', 'unknown'):.2f}%\")\n",
        "\n",
        "    return features, metadata_df, summary\n",
        "\n",
        "def get_features_by_split(features, metadata_df, split_type='holdout_HG005_GRCh38'):\n",
        "    \"\"\"Get train/test features for a specific data split\"\"\"\n",
        "\n",
        "    print(f\"Creating {split_type} split...\")\n",
        "\n",
        "    if split_type == '80_20':\n",
        "        # Random 80/20 split\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        train_indices, test_indices = train_test_split(\n",
        "            range(len(metadata_df)),\n",
        "            test_size=0.2,\n",
        "            stratify=metadata_df['label_str'],\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        # Leave-one-genome-out split\n",
        "        test_genome = split_type.replace('holdout_', '')\n",
        "        train_indices = metadata_df[metadata_df['dataset'] != test_genome].index.tolist()\n",
        "        test_indices = metadata_df[metadata_df['dataset'] == test_genome].index.tolist()\n",
        "\n",
        "    X_train = features[train_indices]\n",
        "    X_test = features[test_indices]\n",
        "    y_train = metadata_df.iloc[train_indices]['binary_label'].values\n",
        "    y_test = metadata_df.iloc[test_indices]['binary_label'].values\n",
        "\n",
        "    print(f\"   Train: {len(X_train)} samples\")\n",
        "    print(f\"   Test: {len(X_test)} samples\")\n",
        "    print(f\"   Train class balance: {np.bincount(y_train)}\")\n",
        "    print(f\"   Test class balance: {np.bincount(y_test)}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "NHD8oFOzO8lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis functions\n",
        "\n",
        "def analyze_vicreg_learned_features():\n",
        "    \"\"\"Analyze what VICReg learned through ML experiments\"\"\"\n",
        "\n",
        "    print(\"ANALYZING VICREG LEARNED FEATURES\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load features\n",
        "    features, metadata_df, summary = load_saved_vicreg_features()\n",
        "    if features is None:\n",
        "        print(\"No saved features found. Run feature extraction first.\")\n",
        "        return None\n",
        "\n",
        "    # Get labels\n",
        "    labels = metadata_df['binary_label'].values\n",
        "\n",
        "    print(f\"Loaded VICReg features:\")\n",
        "    print(f\"   Shape: {features.shape}\")\n",
        "    print(f\"   Champion accuracy: {summary['champion_model']['accuracy']:.2f}%\")\n",
        "\n",
        "    print(f\"Label distribution:\")\n",
        "    print(f\"   TP: {labels.sum()} ({100*labels.mean():.1f}%)\")\n",
        "    print(f\"   FP: {len(labels) - labels.sum()} ({100*(1-labels.mean()):.1f}%)\")\n",
        "\n",
        "    # Split data for analysis - 80/20 with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"\\nData split (80/20 stratified):\")\n",
        "    print(f\"   Train: {len(X_train):,} samples ({y_train.sum():,} TP, {len(y_train) - y_train.sum():,} FP)\")\n",
        "    print(f\"   Test:  {len(X_test):,} samples ({y_test.sum():,} TP, {len(y_test) - y_test.sum():,} FP)\")\n",
        "\n",
        "    # Scale features\n",
        "    print(f\"\\nScaling VICReg features...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Test different models on VICReg features\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(f\"\\nTesting models on VICReg learned features:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "        # Train on VICReg features\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "        # Metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "        # Detailed classification report\n",
        "        report = classification_report(y_test, y_pred, target_names=['FP', 'TP'], output_dict=True)\n",
        "\n",
        "        results[model_name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'auc': auc,\n",
        "            'y_true': y_test,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba,\n",
        "            'classification_report': report\n",
        "        }\n",
        "\n",
        "        print(f\"   Results:\")\n",
        "        print(f\"      Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
        "        print(f\"      AUC: {auc:.3f}\")\n",
        "        print(f\"      FP Precision: {report['FP']['precision']:.3f}\")\n",
        "        print(f\"      FP Recall: {report['FP']['recall']:.3f}\")\n",
        "        print(f\"      TP Precision: {report['TP']['precision']:.3f}\")\n",
        "        print(f\"      TP Recall: {report['TP']['recall']:.3f}\")\n",
        "\n",
        "        # Compare to original VICReg performance\n",
        "        original_acc = summary['champion_model']['accuracy'] / 100  # Convert to decimal\n",
        "        if accuracy >= original_acc - 0.01:  # Allow 1% tolerance\n",
        "            print(f\"      MATCHES VICReg! ({accuracy:.3f} ≈ {original_acc:.3f})\")\n",
        "        elif accuracy >= original_acc - 0.05:  # Within 5%\n",
        "            gap = original_acc - accuracy\n",
        "            print(f\"      Close to VICReg: {gap:.3f} gap ({gap*100:.1f}% points)\")\n",
        "        else:\n",
        "            gap = original_acc - accuracy\n",
        "            print(f\"      Gap to VICReg: {gap:.3f} ({gap*100:.1f}% points)\")\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\nVICREG LEARNED FEATURES ANALYSIS SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"{'Model':<20} | {'Accuracy':<10} | {'AUC':<10} | {'vs VICReg':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    original_acc = summary['champion_model']['accuracy'] / 100\n",
        "    for model_name, result in results.items():\n",
        "        acc = result['accuracy']\n",
        "        auc = result['auc']\n",
        "\n",
        "        if acc >= original_acc - 0.01:\n",
        "            comparison = \"MATCHES!\"\n",
        "        elif acc >= original_acc - 0.05:\n",
        "            comparison = \"Close\"\n",
        "        else:\n",
        "            comparison = f\"-{(original_acc - acc)*100:.1f}%\"\n",
        "\n",
        "        print(f\"{model_name:<20} | {acc:.3f}     | {auc:.3f}     | {comparison:<15}\")\n",
        "\n",
        "    # Key insight\n",
        "    best_acc = max(result['accuracy'] for result in results.values())\n",
        "    if best_acc >= 0.90:\n",
        "        print(f\"\\nKEY INSIGHT: VICReg backbone learned EXCELLENT representations ({best_acc:.1%})\")\n",
        "        print(f\"   Most of the {summary['champion_model']['accuracy']:.2f}% performance comes from learned features!\")\n",
        "    elif best_acc >= 0.80:\n",
        "        print(f\"\\nKEY INSIGHT: VICReg backbone learned GOOD representations ({best_acc:.1%})\")\n",
        "        print(f\"   Combination of learned features + final classifier = {summary['champion_model']['accuracy']:.2f}%\")\n",
        "    else:\n",
        "        print(f\"\\nKEY INSIGHT: VICReg performance mainly from final classifier\")\n",
        "        print(f\"   Learned features contribute {best_acc:.1%}, classifier adds the rest\")\n",
        "\n",
        "    return results, features, metadata_df\n",
        "\n",
        "def analyze_saved_features():\n",
        "    \"\"\"Analyze saved feature files\"\"\"\n",
        "\n",
        "    print(\"ANALYZING SAVED VICREG FEATURES\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if not os.path.exists(SAVE_DIR):\n",
        "        print(\"Features directory not found!\")\n",
        "        return\n",
        "\n",
        "    h5_files = [f for f in os.listdir(SAVE_DIR) if f.endswith('_features.h5')]\n",
        "\n",
        "    if not h5_files:\n",
        "        print(\"No saved feature files found!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(h5_files)} VICReg feature file(s):\")\n",
        "\n",
        "    for h5_file in h5_files:\n",
        "        features_path = os.path.join(SAVE_DIR, h5_file)\n",
        "        summary_path = os.path.join(SAVE_DIR, 'vicreg_extraction_summary.json')\n",
        "\n",
        "        # Load summary\n",
        "        if os.path.exists(summary_path):\n",
        "            with open(summary_path, 'r') as f:\n",
        "                summary = json.load(f)\n",
        "\n",
        "            print(f\"\\nVICREG FEATURES:\")\n",
        "            print(f\"   Champion: {summary['champion_model']['name']}\")\n",
        "            print(f\"   Accuracy: {summary['champion_model']['accuracy']:.2f}%\")\n",
        "            print(f\"   Samples: {summary['data_summary']['total_samples']:,}\")\n",
        "            print(f\"   Feature dim: {summary['data_summary']['feature_dimension']}\")\n",
        "            print(f\"   File size: {summary['files']['features_size_mb']} MB\")\n",
        "            print(f\"   Extraction date: {summary['extraction_date'][:10]}\")\n",
        "        else:\n",
        "            print(f\"\\nVICREG: Summary file missing\")"
      ],
      "metadata": {
        "id": "u3HV5quZPAi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr8PPrKZNwTu"
      },
      "outputs": [],
      "source": [
        "# Usage\n",
        "\n",
        "print(\"MAIN FUNCTIONS:\")\n",
        "print(\"   features, metadata_df, summary = extract_and_save_vicreg_features()\")\n",
        "print(\"   features, metadata_df, summary = load_saved_vicreg_features()\")\n",
        "print(\"   results, features, metadata_df = analyze_vicreg_learned_features()\")\n",
        "print()\n",
        "print(\"DATA SPLITS:\")\n",
        "print(\"   X_train, X_test, y_train, y_test = get_features_by_split(features, metadata_df, 'holdout_HG005_GRCh38')\")\n",
        "print()\n",
        "print(\"ANALYZE FEATURES:\")\n",
        "print(\"   analyze_saved_features()\")\n",
        "print()\n",
        "print(\"WORKFLOW:\")\n",
        "print(\"   1. Automatically finds champion VICReg model\")\n",
        "print(\"   2. Extracts 4096-dim features for all ~50K genomic samples\")\n",
        "print(\"   3. Saves features to HDF5 (efficient storage)\")\n",
        "print(\"   4. Analyzes what VICReg learned through ML experiments\")\n",
        "print(\"   5. Compares latent features to original VICReg performance\")\n",
        "\n",
        "# To run feature extraction:\n",
        "# features, metadata_df, summary = extract_and_save_vicreg_features()\n",
        "\n",
        "# To analyze learned features:\n",
        "# results, features, metadata_df = analyze_vicreg_learned_features()\n",
        "\n",
        "# To load saved features:\n",
        "# features, metadata_df, summary = load_saved_vicreg_features()\n",
        "\n",
        "# To analyze what's available:\n",
        "# analyze_saved_features()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features, metadata_df, summary = extract_and_save_vicreg_features()"
      ],
      "metadata": {
        "id": "PR-1_FPhPNBv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}