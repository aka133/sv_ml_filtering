{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ResNet Feature Extraction & Saving\n",
        "\n",
        "Extract latent features from trained ResNet model for all genomic data"
      ],
      "metadata": {
        "id": "WdUKJQ5BImen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration - GitHub repository structure\n",
        "DATA_DIR = '../data/processed/all_datasets_images_rgb'\n",
        "MODELS_DIR = '../data/processed/resnet_experiments/models'\n",
        "SAVE_DIR = '../data/processed/resnet_latents'\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
        "\n",
        "print(f\"Save directory: {SAVE_DIR}\")"
      ],
      "metadata": {
        "id": "1B8va2yZIubN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "\n",
        "def find_champion_model():\n",
        "    \"\"\"Find the best performing ResNet model automatically\"\"\"\n",
        "\n",
        "    print(\"Searching for champion ResNet model...\")\n",
        "\n",
        "    if not os.path.exists(MODELS_DIR):\n",
        "        print(f\"Models directory not found: {MODELS_DIR}\")\n",
        "        print(\"Run ResNet training first!\")\n",
        "        return None, None\n",
        "\n",
        "    best_acc = 0\n",
        "    best_path = None\n",
        "    best_info = None\n",
        "\n",
        "    # Search all saved models\n",
        "    for model_dir in os.listdir(MODELS_DIR):\n",
        "        model_path = os.path.join(MODELS_DIR, model_dir)\n",
        "        checkpoint_path = os.path.join(model_path, 'best_model.pth')\n",
        "\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            try:\n",
        "                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "                # Only consider binary models for feature extraction\n",
        "                if checkpoint['num_classes'] == 2:\n",
        "                    acc = checkpoint['best_test_acc']\n",
        "                    if acc > best_acc:\n",
        "                        best_acc = acc\n",
        "                        best_path = checkpoint_path\n",
        "                        best_info = {\n",
        "                            'name': checkpoint['experiment_name'],\n",
        "                            'architecture': checkpoint['architecture'],\n",
        "                            'accuracy': acc,\n",
        "                            'auc': checkpoint.get('final_test_auc', 0),\n",
        "                            'path': checkpoint_path\n",
        "                        }\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {model_dir}: {str(e)[:50]}...\")\n",
        "                continue\n",
        "\n",
        "    if best_path is None:\n",
        "        print(\"No binary ResNet models found!\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Champion model found:\")\n",
        "    print(f\"   Name: {best_info['name']}\")\n",
        "    print(f\"   Architecture: {best_info['architecture']}\")\n",
        "    print(f\"   Accuracy: {best_info['accuracy']:.2f}%\")\n",
        "    print(f\"   AUC: {best_info['auc']:.3f}\")\n",
        "    print(f\"   Path: {best_path}\")\n",
        "\n",
        "    return best_path, best_info\n",
        "\n",
        "def load_champion_resnet():\n",
        "    \"\"\"Load the champion ResNet model for feature extraction\"\"\"\n",
        "\n",
        "    champion_path, champion_info = find_champion_model()\n",
        "    if champion_path is None:\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Loading champion ResNet...\")\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(champion_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "    # Recreate ResNet class (same as training)\n",
        "    class FineTunedResNet(torch.nn.Module):\n",
        "        def __init__(self, architecture, num_classes, dropout=0.2):\n",
        "            super().__init__()\n",
        "            import torchvision\n",
        "\n",
        "            # Architecture mapping\n",
        "            arch_map = {\n",
        "                'resnet34': torchvision.models.resnet34,\n",
        "                'resnet50': torchvision.models.resnet50,\n",
        "                'resnet50x2': torchvision.models.wide_resnet50_2,\n",
        "                'resnet101x2': torchvision.models.wide_resnet101_2\n",
        "            }\n",
        "\n",
        "            feature_dims = {\n",
        "                'resnet34': 512,\n",
        "                'resnet50': 2048,\n",
        "                'resnet50x2': 2048,\n",
        "                'resnet101x2': 2048\n",
        "            }\n",
        "\n",
        "            self.backbone = arch_map[architecture](weights='IMAGENET1K_V1')\n",
        "            self.backbone.fc = torch.nn.Identity()\n",
        "\n",
        "            # Freeze backbone\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # Classifier\n",
        "            self.classifier = torch.nn.Sequential(\n",
        "                torch.nn.Dropout(dropout),\n",
        "                torch.nn.Linear(feature_dims[architecture], num_classes)\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            with torch.no_grad():\n",
        "                features = self.backbone(x)\n",
        "            return self.classifier(features)\n",
        "\n",
        "    # Recreate and load model\n",
        "    model = FineTunedResNet(\n",
        "        checkpoint['architecture'],\n",
        "        checkpoint['num_classes'],\n",
        "        checkpoint['model_config']['dropout_rate']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Extract backbone for feature extraction\n",
        "    feature_extractor = model.backbone\n",
        "    feature_extractor.to(device)\n",
        "    feature_extractor.eval()\n",
        "\n",
        "    print(f\"Loaded {checkpoint['architecture']} backbone for feature extraction\")\n",
        "\n",
        "    return feature_extractor, champion_info"
      ],
      "metadata": {
        "id": "KhpzPY9IIztC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractionDataset(Dataset):\n",
        "    \"\"\"Dataset for feature extraction with metadata tracking\"\"\"\n",
        "\n",
        "    def __init__(self, file_info_list, transform=None):\n",
        "        self.file_info = file_info_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        info = self.file_info[idx]\n",
        "\n",
        "        try:\n",
        "            # Load image\n",
        "            data = torch.load(info['filepath'], map_location='cpu')\n",
        "            if isinstance(data, dict):\n",
        "                image = data['image']\n",
        "            else:\n",
        "                image = data\n",
        "\n",
        "            # Handle channels (ensure RGB)\n",
        "            if image.shape[0] != 3:\n",
        "                if image.shape[0] < 3:\n",
        "                    padding = torch.zeros(3 - image.shape[0], *image.shape[1:])\n",
        "                    image = torch.cat([image, padding], dim=0)\n",
        "                else:\n",
        "                    image = image[:3]\n",
        "\n",
        "            # Normalize to [0,1]\n",
        "            if image.max() > 1:\n",
        "                image = image.float() / 255.0\n",
        "\n",
        "            success = True\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback for corrupted files\n",
        "            image = torch.zeros(3, 224, 224)\n",
        "            success = False\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(transforms.ToPILImage()(image))\n",
        "\n",
        "        return image, info, success"
      ],
      "metadata": {
        "id": "VIITOUzoI3Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and feature extraction functions\n",
        "\n",
        "def load_all_file_info():\n",
        "    \"\"\"Load info for ALL genomic data files\"\"\"\n",
        "\n",
        "    print(\"Scanning all genomic data files...\")\n",
        "\n",
        "    datasets = ['HG002_GRCh37', 'HG002_GRCh38', 'HG005_GRCh38']\n",
        "    all_file_info = []\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        dataset_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "        if not os.path.exists(dataset_path):\n",
        "            print(f\"Missing dataset: {dataset_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Scanning {dataset_name}...\")\n",
        "        filenames = [f for f in os.listdir(dataset_path) if f.endswith('.pt')]\n",
        "\n",
        "        for filename in tqdm(filenames, desc=f\"Processing {dataset_name}\"):\n",
        "            parts = filename[:-3].split('_')\n",
        "\n",
        "            if len(parts) >= 8:\n",
        "                try:\n",
        "                    label_str = parts[2]  # TP or FP\n",
        "                    svtype = parts[6]     # INS, DEL, etc.\n",
        "\n",
        "                    if label_str in ['TP', 'FP']:\n",
        "                        all_file_info.append({\n",
        "                            'filename': filename,\n",
        "                            'filepath': os.path.join(dataset_path, filename),\n",
        "                            'dataset': dataset_name,\n",
        "                            'label_str': label_str,\n",
        "                            'svtype': svtype,\n",
        "                            'binary_label': 1 if label_str == 'TP' else 0,\n",
        "                            'multiclass_label': 0 if label_str == 'FP' else (1 if svtype == 'DEL' else 2)\n",
        "                        })\n",
        "\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "    print(f\"Found {len(all_file_info)} total files\")\n",
        "\n",
        "    # Show breakdown\n",
        "    datasets_count = {}\n",
        "    labels_count = {}\n",
        "\n",
        "    for info in all_file_info:\n",
        "        datasets_count[info['dataset']] = datasets_count.get(info['dataset'], 0) + 1\n",
        "        labels_count[info['label_str']] = labels_count.get(info['label_str'], 0) + 1\n",
        "\n",
        "    print(\"Dataset breakdown:\")\n",
        "    for dataset, count in datasets_count.items():\n",
        "        print(f\"   {dataset}: {count:,} files\")\n",
        "\n",
        "    print(\"Label breakdown:\")\n",
        "    for label, count in labels_count.items():\n",
        "        print(f\"   {label}: {count:,} files\")\n",
        "\n",
        "    return all_file_info\n",
        "\n",
        "\n",
        "def extract_and_save_resnet_features():\n",
        "    \"\"\"Extract and save ResNet features for all data samples\"\"\"\n",
        "\n",
        "    print(\"Starting comprehensive ResNet feature extraction...\")\n",
        "\n",
        "    # Load champion model\n",
        "    feature_extractor, champion_info = load_champion_resnet()\n",
        "    if feature_extractor is None:\n",
        "        print(\"Failed to load champion model!\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Load all file info\n",
        "    all_file_info = load_all_file_info()\n",
        "\n",
        "    # Create transform (same as used in training)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.458, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = FeatureExtractionDataset(all_file_info, transform)\n",
        "\n",
        "    # Custom collate function to handle metadata\n",
        "    def collate_fn(batch):\n",
        "        images = torch.stack([item[0] for item in batch])\n",
        "        infos = [item[1] for item in batch]  # Keep as list of dicts\n",
        "        successes = torch.tensor([item[2] for item in batch])\n",
        "        return images, infos, successes\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # Determine feature dimension from champion architecture\n",
        "    feature_dims = {\n",
        "        'resnet34': 512,\n",
        "        'resnet50': 2048,\n",
        "        'resnet50x2': 2048,\n",
        "        'resnet101x2': 2048\n",
        "    }\n",
        "    feature_dim = feature_dims[champion_info['architecture']]\n",
        "\n",
        "    # Prepare storage\n",
        "    total_samples = len(all_file_info)\n",
        "    features_array = np.zeros((total_samples, feature_dim), dtype=np.float32)\n",
        "    metadata_list = []\n",
        "\n",
        "    print(f\"Extracting features for {total_samples:,} samples...\")\n",
        "    print(f\"Feature dimension: {feature_dim}\")\n",
        "\n",
        "    sample_idx = 0\n",
        "    failed_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_images, batch_info, batch_success in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "            batch_images = batch_images.to(device)\n",
        "\n",
        "            # Extract features\n",
        "            batch_features = feature_extractor(batch_images)\n",
        "            batch_features_np = batch_features.cpu().numpy()\n",
        "\n",
        "            # Handle variable batch sizes\n",
        "            actual_batch_size = batch_features_np.shape[0]\n",
        "            features_array[sample_idx:sample_idx + actual_batch_size] = batch_features_np\n",
        "\n",
        "            # Process metadata\n",
        "            for i, (info, success) in enumerate(zip(batch_info, batch_success)):\n",
        "                if i >= actual_batch_size:  # Safety check\n",
        "                    break\n",
        "\n",
        "                metadata_list.append({\n",
        "                    'sample_idx': sample_idx + i,\n",
        "                    'filename': info['filename'],\n",
        "                    'dataset': info['dataset'],\n",
        "                    'label_str': info['label_str'],\n",
        "                    'svtype': info['svtype'],\n",
        "                    'binary_label': info['binary_label'],\n",
        "                    'multiclass_label': info['multiclass_label'],\n",
        "                    'extraction_success': success.item(),\n",
        "                    'filepath': info['filepath']\n",
        "                })\n",
        "\n",
        "                if not success.item():\n",
        "                    failed_count += 1\n",
        "\n",
        "            sample_idx += actual_batch_size\n",
        "\n",
        "    print(f\"Feature extraction complete!\")\n",
        "    print(f\"   Successfully processed: {total_samples - failed_count:,}/{total_samples:,} samples\")\n",
        "    print(f\"   Failed files: {failed_count:,}\")\n",
        "\n",
        "    # Save features to HDF5\n",
        "    features_file = os.path.join(SAVE_DIR, f\"{champion_info['architecture']}_features.h5\")\n",
        "    print(f\"Saving features to: {features_file}\")\n",
        "\n",
        "    with h5py.File(features_file, 'w') as f:\n",
        "        f.create_dataset('features', data=features_array, compression='gzip')\n",
        "        f.attrs['champion_model'] = champion_info['name']\n",
        "        f.attrs['champion_path'] = champion_info['path']\n",
        "        f.attrs['architecture'] = champion_info['architecture']\n",
        "        f.attrs['champion_accuracy'] = champion_info['accuracy']\n",
        "        f.attrs['champion_auc'] = champion_info['auc']\n",
        "        f.attrs['feature_dim'] = feature_dim\n",
        "        f.attrs['total_samples'] = total_samples\n",
        "        f.attrs['extraction_date'] = datetime.now().isoformat()\n",
        "\n",
        "    # Save metadata to CSV\n",
        "    metadata_file = os.path.join(SAVE_DIR, f\"{champion_info['architecture']}_metadata.csv\")\n",
        "    print(f\"Saving metadata to: {metadata_file}\")\n",
        "\n",
        "    metadata_df = pd.DataFrame(metadata_list)\n",
        "    metadata_df.to_csv(metadata_file, index=False)\n",
        "\n",
        "    # Create summary\n",
        "    summary = {\n",
        "        'extraction_date': datetime.now().isoformat(),\n",
        "        'champion_model': {\n",
        "            'name': champion_info['name'],\n",
        "            'architecture': champion_info['architecture'],\n",
        "            'accuracy': champion_info['accuracy'],\n",
        "            'auc': champion_info['auc'],\n",
        "            'path': champion_info['path']\n",
        "        },\n",
        "        'data_summary': {\n",
        "            'total_samples': int(total_samples),\n",
        "            'successful_extractions': int(total_samples - failed_count),\n",
        "            'failed_extractions': int(failed_count),\n",
        "            'feature_dimension': int(feature_dim)\n",
        "        },\n",
        "        'files': {\n",
        "            'features_file': features_file,\n",
        "            'metadata_file': metadata_file,\n",
        "            'features_size_mb': round(features_array.nbytes / 1024**2, 1)\n",
        "        },\n",
        "        'data_breakdown': {\n",
        "            'datasets': {k: int(v) for k, v in metadata_df['dataset'].value_counts().items()},\n",
        "            'labels': {k: int(v) for k, v in metadata_df['label_str'].value_counts().items()},\n",
        "            'sv_types': {k: int(v) for k, v in metadata_df['svtype'].value_counts().items()}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    summary_file = os.path.join(SAVE_DIR, f\"{champion_info['architecture']}_extraction_summary.json\")\n",
        "    print(f\"Saving summary to: {summary_file}\")\n",
        "\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nFEATURE EXTRACTION COMPLETE!\")\n",
        "    print(f\"Files saved in: {SAVE_DIR}\")\n",
        "    print(f\"   Features: {os.path.basename(features_file)} ({summary['files']['features_size_mb']} MB)\")\n",
        "    print(f\"   Metadata: {os.path.basename(metadata_file)}\")\n",
        "    print(f\"   Summary: {os.path.basename(summary_file)}\")\n",
        "\n",
        "    return features_array, metadata_df, summary"
      ],
      "metadata": {
        "id": "9pX5B_HlJCkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "\n",
        "def load_saved_resnet_features(architecture=None):\n",
        "    \"\"\"Load previously saved ResNet features\"\"\"\n",
        "\n",
        "    # Auto-detect architecture if not specified\n",
        "    if architecture is None:\n",
        "        # Find any feature files\n",
        "        h5_files = [f for f in os.listdir(SAVE_DIR) if f.endswith('_features.h5')]\n",
        "        if not h5_files:\n",
        "            print(\"No saved feature files found!\")\n",
        "            return None, None, None\n",
        "\n",
        "        # Use the first one found\n",
        "        features_file = os.path.join(SAVE_DIR, h5_files[0])\n",
        "        architecture = h5_files[0].replace('_features.h5', '')\n",
        "        print(f\"Auto-detected architecture: {architecture}\")\n",
        "    else:\n",
        "        features_file = os.path.join(SAVE_DIR, f\"{architecture}_features.h5\")\n",
        "\n",
        "    metadata_file = os.path.join(SAVE_DIR, f\"{architecture}_metadata.csv\")\n",
        "    summary_file = os.path.join(SAVE_DIR, f\"{architecture}_extraction_summary.json\")\n",
        "\n",
        "    if not os.path.exists(features_file):\n",
        "        print(f\"Features file not found: {features_file}\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\"Loading saved features...\")\n",
        "\n",
        "    # Load features\n",
        "    with h5py.File(features_file, 'r') as f:\n",
        "        features = f['features'][:]\n",
        "        attrs = dict(f.attrs)\n",
        "\n",
        "    # Load metadata\n",
        "    metadata_df = pd.read_csv(metadata_file)\n",
        "\n",
        "    # Load summary\n",
        "    with open(summary_file, 'r') as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(f\"Loaded {len(features):,} feature vectors\")\n",
        "    print(f\"   Architecture: {attrs.get('architecture', 'unknown')}\")\n",
        "    print(f\"   Feature dimension: {features.shape[1]}\")\n",
        "    print(f\"   Champion accuracy: {attrs.get('champion_accuracy', 'unknown'):.2f}%\")\n",
        "\n",
        "    return features, metadata_df, summary\n",
        "\n",
        "def get_features_by_split(features, metadata_df, split_type='holdout_HG005_GRCh38'):\n",
        "    \"\"\"Get train/test features for a specific data split\"\"\"\n",
        "\n",
        "    print(f\"Creating {split_type} split...\")\n",
        "\n",
        "    if split_type == '80_20':\n",
        "        # Random 80/20 split\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        train_indices, test_indices = train_test_split(\n",
        "            range(len(metadata_df)),\n",
        "            test_size=0.2,\n",
        "            stratify=metadata_df['label_str'],\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        # Leave-one-genome-out split\n",
        "        test_genome = split_type.replace('holdout_', '')\n",
        "        train_indices = metadata_df[metadata_df['dataset'] != test_genome].index.tolist()\n",
        "        test_indices = metadata_df[metadata_df['dataset'] == test_genome].index.tolist()\n",
        "\n",
        "    X_train = features[train_indices]\n",
        "    X_test = features[test_indices]\n",
        "    y_train = metadata_df.iloc[train_indices]['binary_label'].values\n",
        "    y_test = metadata_df.iloc[test_indices]['binary_label'].values\n",
        "\n",
        "    print(f\"   Train: {len(X_train)} samples\")\n",
        "    print(f\"   Test: {len(X_test)} samples\")\n",
        "    print(f\"   Train class balance: {np.bincount(y_train)}\")\n",
        "    print(f\"   Test class balance: {np.bincount(y_test)}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def analyze_saved_features():\n",
        "    \"\"\"Analyze saved feature files\"\"\"\n",
        "\n",
        "    print(\"ANALYZING SAVED RESNET FEATURES\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if not os.path.exists(SAVE_DIR):\n",
        "        print(\"Features directory not found!\")\n",
        "        return\n",
        "\n",
        "    h5_files = [f for f in os.listdir(SAVE_DIR) if f.endswith('_features.h5')]\n",
        "\n",
        "    if not h5_files:\n",
        "        print(\"No saved feature files found!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(h5_files)} feature file(s):\")\n",
        "\n",
        "    for h5_file in h5_files:\n",
        "        architecture = h5_file.replace('_features.h5', '')\n",
        "        features_path = os.path.join(SAVE_DIR, h5_file)\n",
        "        summary_path = os.path.join(SAVE_DIR, f\"{architecture}_extraction_summary.json\")\n",
        "\n",
        "        # Load summary\n",
        "        if os.path.exists(summary_path):\n",
        "            with open(summary_path, 'r') as f:\n",
        "                summary = json.load(f)\n",
        "\n",
        "            print(f\"\\n{architecture.upper()} FEATURES:\")\n",
        "            print(f\"   Champion: {summary['champion_model']['name']}\")\n",
        "            print(f\"   Accuracy: {summary['champion_model']['accuracy']:.2f}%\")\n",
        "            print(f\"   Samples: {summary['data_summary']['total_samples']:,}\")\n",
        "            print(f\"   Feature dim: {summary['data_summary']['feature_dimension']}\")\n",
        "            print(f\"   File size: {summary['files']['features_size_mb']} MB\")\n",
        "            print(f\"   Extraction date: {summary['extraction_date'][:10]}\")\n",
        "        else:\n",
        "            print(f\"\\n{architecture.upper()}: Summary file missing\")"
      ],
      "metadata": {
        "id": "FxmeL-RQJraE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEiTnvQCIgwi"
      },
      "outputs": [],
      "source": [
        "print(\"MAIN FUNCTION:\")\n",
        "print(\"   features, metadata_df, summary = extract_and_save_resnet_features()\")\n",
        "print()\n",
        "print(\"LOAD SAVED FEATURES:\")\n",
        "print(\"   features, metadata_df, summary = load_saved_resnet_features()\")\n",
        "print(\"   X_train, X_test, y_train, y_test = get_features_by_split(features, metadata_df, 'holdout_HG005_GRCh38')\")\n",
        "print()\n",
        "print(\"ANALYZE FEATURES:\")\n",
        "print(\"   analyze_saved_features()\")\n",
        "print()\n",
        "print(\"WORKFLOW:\")\n",
        "print(\"   1. Finds best ResNet model\")\n",
        "print(\"   2. Extracts features for all ~50K genomic samples\")\n",
        "print(\"   3. Saves features to HDF5 (efficient storage)\")\n",
        "print(\"   4. Saves metadata to CSV (sample info)\")\n",
        "print(\"   5. Creates reusable feature datasets for ML experiments\")\n",
        "\n",
        "# To run feature extraction:\n",
        "# features, metadata_df, summary = extract_and_save_resnet_features()\n",
        "\n",
        "# To load saved features:\n",
        "# features, metadata_df, summary = load_saved_resnet_features()\n",
        "\n",
        "# To analyze what's available:\n",
        "# analyze_saved_features()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features, metadata_df, summary = extract_and_save_resnet_features()"
      ],
      "metadata": {
        "id": "fmua6_WbJxdB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}