{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ResNet Baseline Training & LogReg Classification"
      ],
      "metadata": {
        "id": "gh-wX4C2GDkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Configuration - GitHub repository structure\n",
        "DATA_DIR = '../data/processed/all_datasets_images_rgb'\n",
        "SAVE_DIR = '../data/processed/resnet_experiments'\n",
        "MODELS_DIR = '../data/processed/resnet_experiments/models'\n",
        "FIGURES_DIR = '../figures'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n"
      ],
      "metadata": {
        "id": "JLn180fxGJ7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard hyperparameters\n",
        "STANDARD_CONFIG = {\n",
        "    'learning_rate': 1e-4,\n",
        "    'batch_size': 32,\n",
        "    'weight_decay': 1e-4,\n",
        "    'dropout_rate': 0.2,\n",
        "    'epochs': 20,\n",
        "    'patience': 5,\n",
        "}\n",
        "\n",
        "# ResNet architectures to evaluate\n",
        "ARCHITECTURES = {\n",
        "    'resnet34': {\n",
        "        'model': lambda: torchvision.models.resnet34(weights='IMAGENET1K_V1'),\n",
        "        'feature_dim': 512\n",
        "    },\n",
        "    'resnet50': {\n",
        "        'model': lambda: torchvision.models.resnet50(weights='IMAGENET1K_V1'),\n",
        "        'feature_dim': 2048\n",
        "    },\n",
        "    'resnet50x2': {\n",
        "        'model': lambda: torchvision.models.wide_resnet50_2(weights='IMAGENET1K_V1'),\n",
        "        'feature_dim': 2048\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "iIIjC9EgGOca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDDFd9TVDdNx"
      },
      "outputs": [],
      "source": [
        "class GenomicDataset(Dataset):\n",
        "    \"\"\"Dataset for genomic structural variant images\"\"\"\n",
        "\n",
        "    def __init__(self, data_list, transform=None):\n",
        "        self.data = data_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        try:\n",
        "            # Load image\n",
        "            data = torch.load(item['filepath'], map_location='cpu')\n",
        "            if isinstance(data, dict):\n",
        "                image = data['image']\n",
        "            else:\n",
        "                image = data\n",
        "\n",
        "            # Handle channels (ensure RGB)\n",
        "            if image.shape[0] != 3:\n",
        "                if image.shape[0] < 3:\n",
        "                    padding = torch.zeros(3 - image.shape[0], *image.shape[1:])\n",
        "                    image = torch.cat([image, padding], dim=0)\n",
        "                else:\n",
        "                    image = image[:3]\n",
        "\n",
        "            # Normalize to [0,1]\n",
        "            if image.max() > 1:\n",
        "                image = image.float() / 255.0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {item['filepath']}: {e}\")\n",
        "            image = torch.zeros(3, 224, 224)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(transforms.ToPILImage()(image))\n",
        "\n",
        "        return image, torch.tensor(item['label'], dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading functions\n",
        "\n",
        "def load_all_genomic_data():\n",
        "    \"\"\"Load all genomic data files\"\"\"\n",
        "\n",
        "    print(\"Loading all genomic data...\")\n",
        "\n",
        "    all_data = []\n",
        "    datasets = ['HG002_GRCh37', 'HG002_GRCh38', 'HG005_GRCh38']\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        dataset_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "        if not os.path.exists(dataset_path):\n",
        "            print(f\"Missing dataset: {dataset_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Loading {dataset_name}...\")\n",
        "\n",
        "        filenames = [f for f in os.listdir(dataset_path) if f.endswith('.pt')]\n",
        "\n",
        "        tp_count = 0\n",
        "        fp_count = 0\n",
        "\n",
        "        for filename in tqdm(filenames, desc=f\"Processing {dataset_name}\"):\n",
        "            parts = filename[:-3].split('_')\n",
        "\n",
        "            if len(parts) >= 8:\n",
        "                try:\n",
        "                    label = parts[2]  # TP or FP\n",
        "                    svtype = parts[6]  # INS, DEL, etc.\n",
        "\n",
        "                    if label in ['TP', 'FP']:\n",
        "                        filepath = os.path.join(dataset_path, filename)\n",
        "\n",
        "                        all_data.append({\n",
        "                            'dataset': dataset_name,\n",
        "                            'filepath': filepath,\n",
        "                            'label_str': label,\n",
        "                            'svtype': svtype,\n",
        "                            'binary_label': 1 if label == 'TP' else 0,\n",
        "                            'multiclass_label': 0 if label == 'FP' else (1 if svtype == 'DEL' else 2)\n",
        "                        })\n",
        "\n",
        "                        if label == 'TP':\n",
        "                            tp_count += 1\n",
        "                        else:\n",
        "                            fp_count += 1\n",
        "\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "        print(f\"   {tp_count} TP, {fp_count} FP = {tp_count + fp_count} total\")\n",
        "\n",
        "    print(f\"\\nTotal dataset:\")\n",
        "    print(f\"   Samples: {len(all_data)}\")\n",
        "\n",
        "    # Count labels\n",
        "    tp_total = sum(1 for x in all_data if x['label_str'] == 'TP')\n",
        "    fp_total = sum(1 for x in all_data if x['label_str'] == 'FP')\n",
        "\n",
        "    print(f\"   TP: {tp_total} ({100*tp_total/len(all_data):.1f}%)\")\n",
        "    print(f\"   FP: {fp_total} ({100*fp_total/len(all_data):.1f}%)\")\n",
        "\n",
        "    # Count SV types in TP\n",
        "    from collections import Counter\n",
        "    svtype_counts = Counter(x['svtype'] for x in all_data if x['label_str'] == 'TP')\n",
        "    print(f\"   SV types: {dict(svtype_counts)}\")\n",
        "\n",
        "    return all_data\n",
        "\n",
        "def create_data_splits(all_data):\n",
        "    \"\"\"Create both 80/20 and leave-one-genome-out splits\"\"\"\n",
        "\n",
        "    splits = {}\n",
        "\n",
        "    # 1. Random 80/20 split\n",
        "    print(\"\\nCreating 80/20 split...\")\n",
        "    train_80, test_20 = train_test_split(\n",
        "        all_data,\n",
        "        test_size=0.2,\n",
        "        stratify=[x['label_str'] for x in all_data],\n",
        "        random_state=42\n",
        "    )\n",
        "    splits['80_20'] = {'train': train_80, 'test': test_20}\n",
        "    print(f\"   Train: {len(train_80)}, Test: {len(test_20)}\")\n",
        "\n",
        "    # 2. Leave-one-genome-out splits\n",
        "    print(\"\\nCreating leave-one-genome-out splits...\")\n",
        "    genomes = ['HG002_GRCh37', 'HG002_GRCh38', 'HG005_GRCh38']\n",
        "\n",
        "    for test_genome in genomes:\n",
        "        train_data = [x for x in all_data if x['dataset'] != test_genome]\n",
        "        test_data = [x for x in all_data if x['dataset'] == test_genome]\n",
        "\n",
        "        splits[f'holdout_{test_genome}'] = {'train': train_data, 'test': test_data}\n",
        "        print(f\"   {test_genome}: Train={len(train_data)}, Test={len(test_data)}\")\n",
        "\n",
        "    return splits\n",
        "\n",
        "def create_transforms():\n",
        "    \"\"\"Standard ImageNet transforms\"\"\"\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.458, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.458, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, test_transform\n"
      ],
      "metadata": {
        "id": "ee5WPY5hGjNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet Model\n",
        "\n",
        "class FineTunedResNet(nn.Module):\n",
        "    \"\"\"ResNet with frozen backbone and trainable classifier\"\"\"\n",
        "\n",
        "    def __init__(self, architecture, num_classes, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.architecture = architecture\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Load backbone\n",
        "        self.backbone = ARCHITECTURES[architecture]['model']()\n",
        "        feature_dim = ARCHITECTURES[architecture]['feature_dim']\n",
        "\n",
        "        # Remove original classifier\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        # Freeze backbone\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Trainable classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feature_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        # Count parameters\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "        print(f\"   {architecture}: {total_params:,} total, {trainable_params:,} trainable\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.backbone(x)\n",
        "        return self.classifier(features)\n"
      ],
      "metadata": {
        "id": "f1klufc-GqH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "\n",
        "def train_resnet_model(architecture, num_classes, train_data, test_data, experiment_name):\n",
        "    \"\"\"Train a single ResNet model and save it\"\"\"\n",
        "\n",
        "    print(f\"\\nTraining {architecture} ({num_classes}-class) - {experiment_name}\")\n",
        "\n",
        "    # Create model save directory\n",
        "    model_save_dir = os.path.join(MODELS_DIR, experiment_name)\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "    print(f\"Model will be saved to: {model_save_dir}\")\n",
        "\n",
        "    # Create transforms and datasets\n",
        "    train_transform, test_transform = create_transforms()\n",
        "\n",
        "    # Set label key based on num_classes\n",
        "    label_key = 'binary_label' if num_classes == 2 else 'multiclass_label'\n",
        "\n",
        "    # Update data with correct labels\n",
        "    train_data_labeled = [{**item, 'label': item[label_key]} for item in train_data]\n",
        "    test_data_labeled = [{**item, 'label': item[label_key]} for item in test_data]\n",
        "\n",
        "    train_dataset = GenomicDataset(train_data_labeled, train_transform)\n",
        "    test_dataset = GenomicDataset(test_data_labeled, test_transform)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=STANDARD_CONFIG['batch_size'],\n",
        "                             shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=STANDARD_CONFIG['batch_size'],\n",
        "                            shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Model\n",
        "    model = FineTunedResNet(architecture, num_classes, STANDARD_CONFIG['dropout_rate']).to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=STANDARD_CONFIG['learning_rate'],\n",
        "        weight_decay=STANDARD_CONFIG['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=3, verbose=False\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_test_acc = 0\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(STANDARD_CONFIG['epochs']):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "\n",
        "        # Testing\n",
        "        model.eval()\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        test_probs = []\n",
        "        test_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "\n",
        "                _, predicted = outputs.max(1)\n",
        "                test_total += labels.size(0)\n",
        "                test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                # For AUC (binary only)\n",
        "                if num_classes == 2:\n",
        "                    probs = F.softmax(outputs, dim=1)\n",
        "                    test_probs.extend(probs[:, 1].cpu().numpy())\n",
        "                    test_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "        test_acc = 100. * test_correct / test_total\n",
        "        test_auc = roc_auc_score(test_targets, test_probs) if num_classes == 2 and len(set(test_targets)) > 1 else 0\n",
        "\n",
        "        scheduler.step(test_acc)\n",
        "\n",
        "        # Save best model state\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "            print(f\"   New best: {test_acc:.2f}% - Model state saved!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'test_auc': test_auc\n",
        "        })\n",
        "\n",
        "        print(f\"   Epoch {epoch+1}: Train={train_acc:.1f}%, Test={test_acc:.1f}%, AUC={test_auc:.3f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= STANDARD_CONFIG['patience']:\n",
        "            print(f\"   Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Save the best model to disk\n",
        "    checkpoint_path = None\n",
        "    if best_model_state is not None:\n",
        "        checkpoint = {\n",
        "            'model_state_dict': best_model_state,\n",
        "            'architecture': architecture,\n",
        "            'num_classes': num_classes,\n",
        "            'experiment_name': experiment_name,\n",
        "            'best_test_acc': best_test_acc,\n",
        "            'final_test_auc': test_auc,\n",
        "            'config': STANDARD_CONFIG,\n",
        "            'history': history,\n",
        "            'model_config': {\n",
        "                'feature_dim': ARCHITECTURES[architecture]['feature_dim'],\n",
        "                'dropout_rate': STANDARD_CONFIG['dropout_rate']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        checkpoint_path = os.path.join(model_save_dir, 'best_model.pth')\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"   Model saved to: {checkpoint_path}\")\n",
        "\n",
        "        # Save model info\n",
        "        info_path = os.path.join(model_save_dir, 'model_info.json')\n",
        "        model_info = {\n",
        "            'experiment_name': experiment_name,\n",
        "            'architecture': architecture,\n",
        "            'num_classes': num_classes,\n",
        "            'best_test_acc': best_test_acc,\n",
        "            'final_test_auc': test_auc,\n",
        "            'checkpoint_path': checkpoint_path,\n",
        "            'saved_at': datetime.now().isoformat()\n",
        "        }\n",
        "        with open(info_path, 'w') as f:\n",
        "            json.dump(model_info, f, indent=2)\n",
        "\n",
        "        print(f\"   Model info saved to: {info_path}\")\n",
        "\n",
        "    print(f\"   Best test accuracy: {best_test_acc:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'architecture': architecture,\n",
        "        'num_classes': num_classes,\n",
        "        'experiment': experiment_name,\n",
        "        'best_test_acc': best_test_acc,\n",
        "        'final_test_auc': test_auc,\n",
        "        'history': history,\n",
        "        'model_path': checkpoint_path,\n",
        "        'model_save_dir': model_save_dir\n",
        "    }\n"
      ],
      "metadata": {
        "id": "1R73g0H2Gt3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_saved_resnet_model(checkpoint_path):\n",
        "    \"\"\"Load a saved ResNet model from checkpoint\"\"\"\n",
        "\n",
        "    print(f\"Loading model from: {checkpoint_path}\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "    # Recreate model\n",
        "    architecture = checkpoint['architecture']\n",
        "    num_classes = checkpoint['num_classes']\n",
        "\n",
        "    model = FineTunedResNet(\n",
        "        architecture,\n",
        "        num_classes,\n",
        "        checkpoint['model_config']['dropout_rate']\n",
        "    )\n",
        "\n",
        "    # Load weights\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "\n",
        "    print(f\"Loaded {architecture} ({num_classes}-class)\")\n",
        "    print(f\"   Best accuracy: {checkpoint['best_test_acc']:.2f}%\")\n",
        "    print(f\"   Final AUC: {checkpoint['final_test_auc']:.3f}\")\n",
        "\n",
        "    return model, checkpoint\n",
        "\n",
        "\n",
        "def analyze_saved_models():\n",
        "    \"\"\"Analyze all saved ResNet models\"\"\"\n",
        "\n",
        "    print(\"ANALYZING SAVED RESNET MODELS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if not os.path.exists(MODELS_DIR):\n",
        "        print(\"No models directory found\")\n",
        "        return None\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model_dir in os.listdir(MODELS_DIR):\n",
        "        model_path = os.path.join(MODELS_DIR, model_dir)\n",
        "        checkpoint_path = os.path.join(model_path, 'best_model.pth')\n",
        "\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            try:\n",
        "                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "                results.append({\n",
        "                    'Model': checkpoint['architecture'],\n",
        "                    'Classes': f\"{checkpoint['num_classes']}-class\",\n",
        "                    'Split': checkpoint['experiment_name'].split('_')[-1] if '_' in checkpoint['experiment_name'] else 'unknown',\n",
        "                    'Accuracy': checkpoint['best_test_acc'],\n",
        "                    'AUC': checkpoint.get('final_test_auc', 0),\n",
        "                    'Full_Name': checkpoint['experiment_name']\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {model_dir}: {str(e)[:100]}...\")\n",
        "\n",
        "    if not results:\n",
        "        print(\"No saved models found!\")\n",
        "        return None\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    print(f\"Found {len(df)} saved models\")\n",
        "\n",
        "    # Performance table\n",
        "    print(f\"\\nTOP 10 MODELS BY ACCURACY:\")\n",
        "    top_models = df.nlargest(10, 'Accuracy')[['Model', 'Classes', 'Split', 'Accuracy', 'AUC']]\n",
        "    print(top_models.to_string(index=False, float_format='%.2f'))\n",
        "\n",
        "    # Best by category\n",
        "    print(f\"\\nBEST BY CATEGORY:\")\n",
        "\n",
        "    # Best binary\n",
        "    binary_models = df[df['Classes'] == '2-class']\n",
        "    if len(binary_models) > 0:\n",
        "        binary_best = binary_models.nlargest(1, 'Accuracy').iloc[0]\n",
        "        print(f\"   Binary: {binary_best['Model']} ({binary_best['Split']}) - {binary_best['Accuracy']:.2f}%\")\n",
        "\n",
        "    # Best 3-class\n",
        "    multiclass_models = df[df['Classes'] == '3-class']\n",
        "    if len(multiclass_models) > 0:\n",
        "        multiclass_best = multiclass_models.nlargest(1, 'Accuracy').iloc[0]\n",
        "        print(f\"   3-class: {multiclass_best['Model']} ({multiclass_best['Split']}) - {multiclass_best['Accuracy']:.2f}%\")\n",
        "\n",
        "    # Overall champion\n",
        "    overall_best = df.nlargest(1, 'Accuracy').iloc[0]\n",
        "    print(f\"\\nOVERALL CHAMPION:\")\n",
        "    print(f\"   {overall_best['Full_Name']}: {overall_best['Accuracy']:.2f}% (AUC: {overall_best['AUC']:.3f})\")\n",
        "\n",
        "    # CSV-Filter comparison\n",
        "    csv_target = 94.94\n",
        "    gap = csv_target - overall_best['Accuracy']\n",
        "    print(f\"\\nCSV-FILTER COMPARISON:\")\n",
        "    print(f\"   Target: {csv_target}%\")\n",
        "    print(f\"   Your best: {overall_best['Accuracy']:.2f}%\")\n",
        "    print(f\"   Gap: {gap:.2f}%\")\n",
        "\n",
        "    if gap <= 0:\n",
        "        print(f\"   YOU BEAT CSV-FILTER!\")\n",
        "    elif gap <= 2:\n",
        "        print(f\"   Very close! Excellent performance on realistic data.\")\n",
        "    else:\n",
        "        print(f\"   Good performance - remember you're solving a harder problem!\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "NiIDG46jGnj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run experiments\n",
        "\n",
        "def run_all_resnet_experiments():\n",
        "    \"\"\"Run all ResNet experiments with model saving\"\"\"\n",
        "\n",
        "    print(\"RUNNING ALL RESNET EXPERIMENTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load data\n",
        "    all_data = load_all_genomic_data()\n",
        "    splits = create_data_splits(all_data)\n",
        "\n",
        "    # Experiment configuration\n",
        "    architectures = list(ARCHITECTURES.keys())\n",
        "    class_setups = [2, 3]  # Binary and 3-class\n",
        "    split_names = list(splits.keys())\n",
        "\n",
        "    total_experiments = len(architectures) * len(class_setups) * len(split_names)\n",
        "    print(f\"\\nPlanning {total_experiments} experiments:\")\n",
        "    print(f\"   Architectures: {architectures}\")\n",
        "    print(f\"   Class setups: {class_setups}\")\n",
        "    print(f\"   Data splits: {split_names}\")\n",
        "\n",
        "    # Run experiments\n",
        "    all_results = []\n",
        "\n",
        "    for architecture in architectures:\n",
        "        for num_classes in class_setups:\n",
        "            for split_name in split_names:\n",
        "                train_data = splits[split_name]['train']\n",
        "                test_data = splits[split_name]['test']\n",
        "\n",
        "                experiment_name = f\"{architecture}_{num_classes}class_{split_name}\"\n",
        "\n",
        "                try:\n",
        "                    result = train_resnet_model(architecture, num_classes, train_data, test_data, experiment_name)\n",
        "                    all_results.append(result)\n",
        "\n",
        "                    # Save intermediate results\n",
        "                    results_df = pd.DataFrame(all_results)\n",
        "                    results_df.to_csv(os.path.join(SAVE_DIR, 'all_results.csv'), index=False)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed {experiment_name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Clear memory\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\nEXPERIMENT SUMMARY:\")\n",
        "    print(f\"   Completed: {len(all_results)}/{total_experiments}\")\n",
        "\n",
        "    if all_results:\n",
        "        results_df = pd.DataFrame(all_results)\n",
        "\n",
        "        print(f\"\\nBEST RESULTS:\")\n",
        "        best_by_arch = results_df.groupby('architecture')['best_test_acc'].max()\n",
        "        for arch, acc in best_by_arch.items():\n",
        "            print(f\"   {arch}: {acc:.2f}%\")\n",
        "\n",
        "        overall_best = results_df.loc[results_df['best_test_acc'].idxmax()]\n",
        "        print(f\"\\nOVERALL BEST: {overall_best['best_test_acc']:.2f}%\")\n",
        "        print(f\"   Model: {overall_best['architecture']} ({overall_best['num_classes']}-class)\")\n",
        "        print(f\"   Split: {overall_best['experiment']}\")\n",
        "        print(f\"   Saved at: {overall_best['model_path']}\")\n",
        "\n",
        "        # CSV-Filter comparison\n",
        "        csv_filter_target = 94.94\n",
        "        if overall_best['best_test_acc'] >= csv_filter_target:\n",
        "            print(f\"BEAT CSV-FILTER! (+{overall_best['best_test_acc'] - csv_filter_target:.2f}%)\")\n",
        "        else:\n",
        "            print(f\"Gap to CSV-Filter: {csv_filter_target - overall_best['best_test_acc']:.2f}%\")\n",
        "\n",
        "        # List all saved models\n",
        "        print(f\"\\nSAVED MODELS:\")\n",
        "        for _, result in results_df.iterrows():\n",
        "            if result['model_path']:\n",
        "                print(f\"   {result['experiment']}: {result['model_path']}\")\n",
        "\n",
        "    return all_results"
      ],
      "metadata": {
        "id": "aSceJ6LbHYp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression on latents\n",
        "\n",
        "def get_champion_model_path():\n",
        "    \"\"\"Get the path to the best performing binary model\"\"\"\n",
        "\n",
        "    analysis_df = analyze_saved_models()\n",
        "    if analysis_df is None:\n",
        "        return None\n",
        "\n",
        "    # Get best binary model\n",
        "    binary_models = analysis_df[analysis_df['Classes'] == '2-class']\n",
        "    if len(binary_models) == 0:\n",
        "        print(\"No binary models found!\")\n",
        "        return None\n",
        "\n",
        "    best_binary = binary_models.nlargest(1, 'Accuracy').iloc[0]\n",
        "    champion_name = best_binary['Full_Name']\n",
        "    champion_path = os.path.join(MODELS_DIR, champion_name, 'best_model.pth')\n",
        "\n",
        "    print(f\"Champion model: {champion_name}\")\n",
        "    print(f\"   Accuracy: {best_binary['Accuracy']:.2f}%\")\n",
        "    print(f\"   Path: {champion_path}\")\n",
        "\n",
        "    return champion_path, champion_name\n",
        "\n",
        "def load_champion_data_split(champion_name):\n",
        "    \"\"\"Load the same data split used for the champion model\"\"\"\n",
        "\n",
        "    print(f\"Loading data split for champion: {champion_name}\")\n",
        "\n",
        "    # Parse split from champion name\n",
        "    if 'holdout_HG005_GRCh38' in champion_name:\n",
        "        test_genome = 'HG005_GRCh38'\n",
        "    elif 'holdout_HG002_GRCh38' in champion_name:\n",
        "        test_genome = 'HG002_GRCh38'\n",
        "    elif 'holdout_HG002_GRCh37' in champion_name:\n",
        "        test_genome = 'HG002_GRCh37'\n",
        "    elif '80_20' in champion_name:\n",
        "        test_genome = '80_20'\n",
        "    else:\n",
        "        print(f\"Unknown split in champion name: {champion_name}\")\n",
        "        return None, None\n",
        "\n",
        "    # Load all data and create same split\n",
        "    all_data = load_all_genomic_data()\n",
        "\n",
        "    if test_genome == '80_20':\n",
        "        train_data, test_data = train_test_split(\n",
        "            all_data,\n",
        "            test_size=0.2,\n",
        "            stratify=[x['label_str'] for x in all_data],\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        train_data = [x for x in all_data if x['dataset'] != test_genome]\n",
        "        test_data = [x for x in all_data if x['dataset'] == test_genome]\n",
        "\n",
        "    # Convert to binary labels\n",
        "    train_data = [{**item, 'label': item['binary_label']} for item in train_data]\n",
        "    test_data = [{**item, 'label': item['binary_label']} for item in test_data]\n",
        "\n",
        "    print(f\"   Split: {test_genome}\")\n",
        "    print(f\"   Train: {len(train_data)} samples\")\n",
        "    print(f\"   Test: {len(test_data)} samples\")\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "def extract_features_from_champion(feature_extractor, dataloader):\n",
        "    \"\"\"Extract latent features from ResNet backbone\"\"\"\n",
        "\n",
        "    print(\"Extracting latent features...\")\n",
        "\n",
        "    feature_extractor.eval()\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Extract features from backbone\n",
        "            features = feature_extractor(images)\n",
        "\n",
        "            all_features.append(features.cpu().numpy())\n",
        "            all_labels.append(labels.numpy())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    X = np.concatenate(all_features, axis=0)\n",
        "    y = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    print(f\"   Features shape: {X.shape}\")\n",
        "    print(f\"   Labels shape: {y.shape}\")\n",
        "    print(f\"   Class balance: {np.bincount(y)}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def train_logistic_regression_on_latents():\n",
        "    \"\"\"Train logistic regression on ResNet latents from champion model\"\"\"\n",
        "\n",
        "    print(\"TRAINING LOGISTIC REGRESSION ON RESNET LATENTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Get champion model\n",
        "    champion_path, champion_name = get_champion_model_path()\n",
        "    if champion_path is None:\n",
        "        print(\"No champion model found. Run ResNet training first.\")\n",
        "        return None\n",
        "\n",
        "    # Load champion model\n",
        "    champion_model, checkpoint = load_saved_resnet_model(champion_path)\n",
        "\n",
        "    # Extract backbone for feature extraction\n",
        "    feature_extractor = champion_model.backbone\n",
        "\n",
        "    # Load same data split as champion\n",
        "    train_data, test_data = load_champion_data_split(champion_name)\n",
        "    if train_data is None:\n",
        "        return None\n",
        "\n",
        "    # Create transforms and datasets\n",
        "    _, test_transform = create_transforms()  # Use test transform for both\n",
        "\n",
        "    train_dataset = GenomicDataset(train_data, test_transform)\n",
        "    test_dataset = GenomicDataset(test_data, test_transform)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Extract features\n",
        "    print(\"Extracting training features...\")\n",
        "    X_train, y_train = extract_features_from_champion(feature_extractor, train_loader)\n",
        "\n",
        "    print(\"Extracting test features...\")\n",
        "    X_test, y_test = extract_features_from_champion(feature_extractor, test_loader)\n",
        "\n",
        "    # Train logistic regression with regularization search\n",
        "    print(\"Training logistic regression...\")\n",
        "\n",
        "    C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "    best_score = 0\n",
        "    best_model = None\n",
        "    best_C = None\n",
        "\n",
        "    for C in C_values:\n",
        "        model = LogisticRegression(C=C, max_iter=1000, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "        print(f\"   C={C:6.3f}: Accuracy={accuracy:.4f}, AUC={auc:.4f}\")\n",
        "\n",
        "        if accuracy > best_score:\n",
        "            best_score = accuracy\n",
        "            best_model = model\n",
        "            best_C = C\n",
        "\n",
        "    # Final evaluation with best model\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    final_accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "    final_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "    print(f\"\\nRESULTS:\")\n",
        "    print(f\"   Best C: {best_C}\")\n",
        "    print(f\"   Logistic Regression Accuracy: {final_accuracy:.2f}%\")\n",
        "    print(f\"   Logistic Regression AUC: {final_auc:.3f}\")\n",
        "\n",
        "    # Compare to original ResNet\n",
        "    original_accuracy = checkpoint['best_test_acc']\n",
        "    original_auc = checkpoint['final_test_auc']\n",
        "\n",
        "    print(f\"\\nCOMPARISON:\")\n",
        "    print(f\"   Original ResNet: {original_accuracy:.2f}% (AUC: {original_auc:.3f})\")\n",
        "    print(f\"   Linear on Latents: {final_accuracy:.2f}% (AUC: {final_auc:.3f})\")\n",
        "    print(f\"   Difference: {final_accuracy - original_accuracy:.2f}%\")\n",
        "\n",
        "    if final_accuracy > original_accuracy:\n",
        "        print(\"   Linear regression on latents BEATS the original classifier!\")\n",
        "    elif abs(final_accuracy - original_accuracy) < 1:\n",
        "        print(\"   Linear regression performs similarly - latents capture most info!\")\n",
        "    else:\n",
        "        print(\"   Original classifier is better - nonlinear relationships matter\")\n",
        "\n",
        "    # Classification report\n",
        "    print(classification_report(y_test, y_pred, target_names=['FP', 'TP']))\n",
        "\n",
        "    return best_model, X_train, X_test, y_train, y_test, final_accuracy, final_auc"
      ],
      "metadata": {
        "id": "rCDTj8uPHfcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAIN FUNCTIONS:\")\n",
        "print(\"   results = run_all_resnet_experiments()\")\n",
        "print(\"   analysis_df = analyze_saved_models()\")\n",
        "print(\"   linear_model, X_train, X_test, y_train, y_test, acc, auc = train_logistic_regression_on_latents()\")\n",
        "print()\n",
        "print(\"EXPERIMENT DETAILS:\")\n",
        "print(\"   3 architectures (ResNet34/50/50x2)\")\n",
        "print(\"   2 class setups (binary TP/FP, 3-class FP/DEL/INS)\")\n",
        "print(\"   4 data splits (80/20 + 3 leave-one-genome-out)\")\n",
        "print(\"   = 32 total experiments\")\n",
        "print(\"   Champion model used for LogReg latent classification\")\n",
        "\n",
        "# To run all experiments:\n",
        "# results = run_all_resnet_experiments()\n",
        "\n",
        "# To analyze results:\n",
        "# analysis_df = analyze_saved_models()\n",
        "\n",
        "# To train LogReg on latents:\n",
        "# linear_model, X_train, X_test, y_train, y_test, acc, auc = train_logistic_regression_on_latents()"
      ],
      "metadata": {
        "id": "brSecrGjHLhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_all_resnet_experiments()"
      ],
      "metadata": {
        "id": "XANB3PjVHn10"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}