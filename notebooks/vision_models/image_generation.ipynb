{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CSV-Filter Image Generation Pipeline\n",
        "\n",
        "This notebook implements the CSV-Filter method for generating 4-channel CIGAR images from structural variants.\n",
        "\n",
        "Sourced from Xia, Z., Xiang, W., Wang, Q., Li, X., Li, Y., Gao, J., ... & Cui, Y. (2024). CSV-Filter: a deep learning-based comprehensive structural variant filtering method for both short and long reads. Bioinformatics, 40(9), btae539.\n"
      ],
      "metadata": {
        "id": "ie5H3tu4VSC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import pysam\n",
        "import torchvision\n",
        "import gzip\n",
        "import re\n",
        "import json\n",
        "from multiprocessing import Pool\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
        "print(f\"üîß Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
      ],
      "metadata": {
        "id": "nI90zbxnXAu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These functions are direct implementations from Xia et al. (2024) CSV-Filter utilities.py\n",
        "\n",
        "# CSV-Filter constants\n",
        "HEIGHT = 224\n",
        "resize = torchvision.transforms.Resize([HEIGHT, HEIGHT])\n",
        "\n",
        "def cigar_new_img_single_memory(bam_path, chromosome, begin, end):\n",
        "    \"\"\"\n",
        "    CSV-Filter function: Generate 4-channel CIGAR image [Match, Deletion, Insertion, Soft Clip]\n",
        "\n",
        "    Direct implementation from Xia et al. (2024) utilities.py\n",
        "\n",
        "    Args:\n",
        "        bam_path: Path to BAM file\n",
        "        chromosome: Chromosome name (e.g., \"1\", \"chr1\")\n",
        "        begin: Start position\n",
        "        end: End position\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 4-channel image (4, HEIGHT, HEIGHT)\n",
        "    \"\"\"\n",
        "    r_start = []\n",
        "    r_end = []\n",
        "    sam_file = pysam.AlignmentFile(bam_path, \"rb\")\n",
        "\n",
        "    # Get read boundaries\n",
        "    for read in sam_file.fetch(chromosome, begin, end):\n",
        "        r_start.append(read.reference_start)\n",
        "        r_end.append(read.reference_end)\n",
        "\n",
        "    if r_start:\n",
        "        ref_min = np.min(r_start)\n",
        "        ref_max = np.max(r_end)\n",
        "\n",
        "        # Process Match operations (Channel 0)\n",
        "        cigars_img = torch.zeros([1, len(r_start), ref_max - ref_min])\n",
        "        for i, read in enumerate(sam_file.fetch(chromosome, begin, end)):\n",
        "            max_terminal = read.reference_start - ref_min\n",
        "            for operation, length in read.cigar:\n",
        "                if operation == 0:  # Match\n",
        "                    cigars_img[0, i, max_terminal:max_terminal+length] = 255\n",
        "                    max_terminal += length\n",
        "                elif operation == 2:  # Deletion\n",
        "                    max_terminal += length\n",
        "                elif operation == 3 or operation == 7 or operation == 8:  # N, =, X\n",
        "                    max_terminal += length\n",
        "        cigars_img1 = resize(cigars_img)\n",
        "\n",
        "        # Process Deletion operations (Channel 1)\n",
        "        cigars_img[:, :, :] = 0\n",
        "        for i, read in enumerate(sam_file.fetch(chromosome, begin, end)):\n",
        "            max_terminal = read.reference_start - ref_min\n",
        "            for operation, length in read.cigar:\n",
        "                if operation == 0:  # Match\n",
        "                    max_terminal += length\n",
        "                elif operation == 2:  # Deletion\n",
        "                    cigars_img[0, i, max_terminal:max_terminal+length] = 255\n",
        "                    max_terminal += length\n",
        "                elif operation == 3 or operation == 7 or operation == 8:  # N, =, X\n",
        "                    max_terminal += length\n",
        "        cigars_img2 = resize(cigars_img)\n",
        "\n",
        "        # Process Insertion operations (Channel 2)\n",
        "        cigars_img[:, :, :] = 0\n",
        "        for i, read in enumerate(sam_file.fetch(chromosome, begin, end)):\n",
        "            max_terminal = read.reference_start - ref_min\n",
        "            for operation, length in read.cigar:\n",
        "                if operation == 0:  # Match\n",
        "                    max_terminal += length\n",
        "                elif operation == 2:  # Deletion\n",
        "                    max_terminal += length\n",
        "                elif operation == 1:  # Insertion\n",
        "                    cigars_img[0, i, max_terminal - int(length / 2):max_terminal + int(length / 2)] = 255\n",
        "                elif operation == 3 or operation == 7 or operation == 8:  # N, =, X\n",
        "                    max_terminal += length\n",
        "        cigars_img3 = resize(cigars_img)\n",
        "\n",
        "        # Process Soft clip operations (Channel 3)\n",
        "        cigars_img[:, :, :] = 0\n",
        "        for i, read in enumerate(sam_file.fetch(chromosome, begin, end)):\n",
        "            max_terminal = read.reference_start - ref_min\n",
        "            for operation, length in read.cigar:\n",
        "                if operation == 0:  # Match\n",
        "                    max_terminal += length\n",
        "                elif operation == 2:  # Deletion\n",
        "                    max_terminal += length\n",
        "                elif operation == 4:  # Soft clip\n",
        "                    cigars_img[0, i, max_terminal - int(length / 2):max_terminal + int(length / 2)] = 255\n",
        "                elif operation == 3 or operation == 7 or operation == 8:  # N, =, X\n",
        "                    max_terminal += length\n",
        "        cigars_img4 = resize(cigars_img)\n",
        "\n",
        "        # Combine all 4 channels\n",
        "        cigars_img = torch.empty([4, HEIGHT, HEIGHT])\n",
        "        cigars_img[0] = cigars_img1  # Match\n",
        "        cigars_img[1] = cigars_img2  # Deletion\n",
        "        cigars_img[2] = cigars_img3  # Insertion\n",
        "        cigars_img[3] = cigars_img4  # Soft clip\n",
        "\n",
        "    else:\n",
        "        cigars_img = torch.zeros([4, HEIGHT, HEIGHT])\n",
        "\n",
        "    sam_file.close()\n",
        "    return cigars_img\n",
        "\n",
        "def extract_variants_from_vcf(vcf_path, max_variants=None):\n",
        "    \"\"\"Extract structural variants from VCF file\"\"\"\n",
        "    variants = []\n",
        "    opener = gzip.open if vcf_path.endswith('.gz') else open\n",
        "\n",
        "    with opener(vcf_path, 'rt') as f:\n",
        "        for line_num, line in enumerate(f):\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "\n",
        "            if max_variants and len(variants) >= max_variants:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                fields = line.strip().split('\\t')\n",
        "                if len(fields) < 8:\n",
        "                    continue\n",
        "\n",
        "                chrom = fields[0]\n",
        "                pos = int(fields[1])\n",
        "                ref = fields[3]\n",
        "                alt = fields[4]\n",
        "                info = fields[7]\n",
        "\n",
        "                # Parse SV information\n",
        "                sv_type = None\n",
        "                svlen = None\n",
        "                end_pos = None\n",
        "\n",
        "                # Extract SVTYPE\n",
        "                svtype_match = re.search(r'SVTYPE=([^;]+)', info)\n",
        "                if svtype_match:\n",
        "                    sv_type = svtype_match.group(1)\n",
        "\n",
        "                # Extract SVLEN\n",
        "                svlen_match = re.search(r'SVLEN=([^;]+)', info)\n",
        "                if svlen_match:\n",
        "                    svlen = abs(int(svlen_match.group(1)))\n",
        "\n",
        "                # Extract END\n",
        "                end_match = re.search(r'END=([^;]+)', info)\n",
        "                if end_match:\n",
        "                    end_pos = int(end_match.group(1))\n",
        "\n",
        "                # Calculate end position if not provided\n",
        "                if end_pos is None:\n",
        "                    if svlen:\n",
        "                        end_pos = pos + svlen\n",
        "                    else:\n",
        "                        end_pos = pos + max(len(ref), len(alt))\n",
        "\n",
        "                # Calculate SVLEN if not provided\n",
        "                if svlen is None:\n",
        "                    svlen = end_pos - pos\n",
        "\n",
        "                # Filter for structural variants (>=50bp)\n",
        "                if svlen >= 50 and sv_type in ['DEL', 'INS', 'DUP', 'INV']:\n",
        "                    variants.append({\n",
        "                        'chrom': chrom,\n",
        "                        'pos': pos,\n",
        "                        'end': end_pos,\n",
        "                        'svtype': sv_type,\n",
        "                        'svlen': svlen,\n",
        "                        'ref': ref,\n",
        "                        'alt': alt\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    return variants\n",
        "\n",
        "print(\"CSV-Filter core functions loaded\")"
      ],
      "metadata": {
        "id": "BBmBXcRxXGv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define datasets with appropriate reference genomes\n",
        "DATASETS = {\n",
        "    'HG002_GRCh37': {\n",
        "        'tp_comp_vcf': '../data/raw/bench-HG002_GRCh37_noqc/tp-comp.vcf.gz',\n",
        "        'fp_vcf': '../data/raw/bench-HG002_GRCh37_noqc/fp.vcf.gz',\n",
        "        'ref': '../data/raw/GRCh37.fna'\n",
        "    },\n",
        "    'HG002_GRCh38': {\n",
        "        'tp_comp_vcf': '../data/raw/bench-HG002_GRCh38-GIABv3_noqc/tp-comp.vcf.gz',\n",
        "        'fp_vcf': '../data/raw/bench-HG002_GRCh38-GIABv3_noqc/fp.vcf.gz',\n",
        "        'ref': '../data/raw/GRCh38.fa'\n",
        "    },\n",
        "    'HG005_GRCh38': {\n",
        "        'tp_comp_vcf': '../data/raw/bench-HG005_GRCh38_noqc/tp-comp.vcf.gz',\n",
        "        'fp_vcf': '../data/raw/bench-HG005_GRCh38_noqc/fp.vcf.gz',\n",
        "        'ref': '../data/raw/GRCh38.fa'\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "CONFIG = {\n",
        "    'cache_dir': '/content/drive/MyDrive/SV_Diffusion/all_datasets_images',\n",
        "    'max_coverage': 1000,  # Skip variants with >1000 reads\n",
        "    'chunk_size': 50,      # Variants per worker\n",
        "    'num_workers': 4,      # Parallel workers\n",
        "    'sync_every': 1000     # Progress sync frequency\n",
        "}\n",
        "\n",
        "print(f\"Configured {len(DATASETS)} datasets\")\n",
        "print(f\"Output directory: {CONFIG['cache_dir']}\")"
      ],
      "metadata": {
        "id": "PEDfZVnrXUDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parallel Processing Functions\n",
        "\n",
        "def process_variant_chunk_parallel(args):\n",
        "    \"\"\"\n",
        "    Parallel worker function for processing variant chunks\n",
        "\n",
        "    Args:\n",
        "        args: (variants_chunk, bam_path, dataset_output_dir, dataset_name)\n",
        "\n",
        "    Returns:\n",
        "        dict: Processing statistics for this chunk\n",
        "    \"\"\"\n",
        "    variants_chunk, bam_path, dataset_output_dir, dataset_name = args\n",
        "\n",
        "    # Local results for this worker\n",
        "    worker_results = {\n",
        "        'processed': 0,\n",
        "        'skipped_existing': 0,\n",
        "        'skipped_coverage': 0,\n",
        "        'failed': 0,\n",
        "        'skipped_high_coverage': [],\n",
        "        'failed_variants': []\n",
        "    }\n",
        "\n",
        "    for variant, filename in variants_chunk:\n",
        "        try:\n",
        "            # Skip if file already exists\n",
        "            output_path = os.path.join(dataset_output_dir, filename)\n",
        "            if os.path.exists(output_path):\n",
        "                worker_results['skipped_existing'] += 1\n",
        "                continue\n",
        "\n",
        "            # Memory safety: check read count\n",
        "            sam_file = pysam.AlignmentFile(bam_path, \"rb\")\n",
        "            read_count = 0\n",
        "            for read in sam_file.fetch(variant['chrom'], variant['pos'], variant['end']):\n",
        "                read_count += 1\n",
        "                if read_count > CONFIG['max_coverage']:\n",
        "                    break\n",
        "            sam_file.close()\n",
        "\n",
        "            if read_count > CONFIG['max_coverage']:\n",
        "                worker_results['skipped_coverage'] += 1\n",
        "                worker_results['skipped_high_coverage'].append({\n",
        "                    'dataset': dataset_name,\n",
        "                    'variant': variant,\n",
        "                    'reason': f'High coverage: {read_count}+ reads',\n",
        "                    'filename': filename,\n",
        "                    'read_count': read_count\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Generate 4-channel CIGAR image using CSV-Filter\n",
        "            image = cigar_new_img_single_memory(\n",
        "                bam_path=bam_path,\n",
        "                chromosome=variant['chrom'],\n",
        "                begin=variant['pos'],\n",
        "                end=variant['end']\n",
        "            )\n",
        "\n",
        "            # Verify 4-channel format\n",
        "            assert image.shape[0] == 4, f\"Expected 4 channels, got {image.shape[0]}\"\n",
        "\n",
        "            # Save image with metadata\n",
        "            torch.save({\n",
        "                'image': image,\n",
        "                'metadata': variant,\n",
        "                'shape': image.shape,\n",
        "                'format': '4ch_cigar',\n",
        "                'method': 'CSV-Filter (Xia et al. 2024)'\n",
        "            }, output_path)\n",
        "\n",
        "            worker_results['processed'] += 1\n",
        "\n",
        "            # Memory cleanup\n",
        "            del image\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Garbage collect periodically\n",
        "            if worker_results['processed'] % 50 == 0:\n",
        "                import gc\n",
        "                gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            worker_results['failed'] += 1\n",
        "            worker_results['failed_variants'].append({\n",
        "                'dataset': dataset_name,\n",
        "                'variant': variant,\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "            # Memory cleanup on failure\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            import gc\n",
        "            gc.collect()\n",
        "\n",
        "    return worker_results"
      ],
      "metadata": {
        "id": "SvWnnmQJXfXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeeTffd9VQqy"
      },
      "outputs": [],
      "source": [
        "# Main Processing Pipeline\n",
        "\n",
        "def process_all_datasets():\n",
        "    \"\"\"\n",
        "    Main processing pipeline: Generate CSV-Filter images for all datasets\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"CSV-Filter Image Generation Pipeline\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(CONFIG['cache_dir'], exist_ok=True)\n",
        "\n",
        "    # Check existing files\n",
        "    print(f\"\\n Checking existing files...\")\n",
        "    total_existing = 0\n",
        "    for dataset_name in DATASETS.keys():\n",
        "        dataset_dir = os.path.join(CONFIG['cache_dir'], dataset_name)\n",
        "        if os.path.exists(dataset_dir):\n",
        "            existing_count = len([f for f in os.listdir(dataset_dir) if f.endswith('.pt')])\n",
        "            total_existing += existing_count\n",
        "            print(f\"   {dataset_name}: {existing_count} existing files\")\n",
        "        else:\n",
        "            print(f\"   {dataset_name}: 0 existing files\")\n",
        "\n",
        "    print(f\"   Total existing files: {total_existing}\")\n",
        "\n",
        "    # Estimate processing time\n",
        "    estimated_hours = max(1, (20000 - total_existing) // (CONFIG['num_workers'] * 1000))\n",
        "\n",
        "    proceed = input(f\"\\n Ready to process all datasets!\\n\"\n",
        "                   f\"Existing files: {total_existing} (will be skipped)\\n\"\n",
        "                   f\"Estimated time: ~{estimated_hours} hours\\n\"\n",
        "                   f\"Cache: {CONFIG['cache_dir']}\\n\"\n",
        "                   f\"Resume-safe: Never overwrites existing files\\n\"\n",
        "                   f\"Proceed? (y/n): \")\n",
        "\n",
        "    if proceed.lower() != 'y':\n",
        "        print(\"‚è∏Processing cancelled by user\")\n",
        "        return\n",
        "\n",
        "    print(f\"Starting resume-safe parallel processing...\")\n",
        "\n",
        "    # Global tracking\n",
        "    total_images_generated = 0\n",
        "    total_skipped = 0\n",
        "    failed_variants = []\n",
        "    skipped_high_coverage = []\n",
        "\n",
        "    # Process each dataset\n",
        "    for dataset_name, config in DATASETS.items():\n",
        "        print(f\"\\n Processing dataset: {dataset_name}\")\n",
        "\n",
        "        # Verify files exist\n",
        "        missing_files = [path for path in config.values() if not os.path.exists(path)]\n",
        "        if missing_files:\n",
        "            print(f\"    Missing files: {missing_files}\")\n",
        "            continue\n",
        "\n",
        "        # Extract variants from VCFs\n",
        "        print(f\"    Extracting variants...\")\n",
        "        tp_variants = extract_variants_from_vcf(config['tp_comp_vcf'])\n",
        "        fp_variants = extract_variants_from_vcf(config['fp_vcf'])\n",
        "\n",
        "        print(f\"    Found {len(tp_variants)} TP and {len(fp_variants)} FP variants\")\n",
        "\n",
        "        # Combine variants with labels\n",
        "        all_variants = []\n",
        "        for variant in tp_variants:\n",
        "            variant.update({'label': 'TP', 'dataset': dataset_name})\n",
        "            all_variants.append(variant)\n",
        "        for variant in fp_variants:\n",
        "            variant.update({'label': 'FP', 'dataset': dataset_name})\n",
        "            all_variants.append(variant)\n",
        "\n",
        "        # Create output directory\n",
        "        dataset_output_dir = os.path.join(CONFIG['cache_dir'], dataset_name)\n",
        "        os.makedirs(dataset_output_dir, exist_ok=True)\n",
        "\n",
        "        # Filter variants that need processing\n",
        "        existing_files = set(f for f in os.listdir(dataset_output_dir) if f.endswith('.pt'))\n",
        "        variants_to_process = []\n",
        "        skipped_count = 0\n",
        "\n",
        "        for variant in all_variants:\n",
        "            filename = f\"{variant['dataset']}_{variant['label']}_{variant['chrom']}_{variant['pos']}_{variant['end']}_{variant['svtype']}_{variant['svlen']}bp.pt\"\n",
        "\n",
        "            if filename in existing_files:\n",
        "                skipped_count += 1\n",
        "                total_skipped += 1\n",
        "            else:\n",
        "                variants_to_process.append((variant, filename))\n",
        "\n",
        "        print(f\"    Skipping {skipped_count} existing files\")\n",
        "        print(f\"    Processing {len(variants_to_process)} new variants\")\n",
        "\n",
        "        if len(variants_to_process) == 0:\n",
        "            print(f\"    All variants already processed!\")\n",
        "            continue\n",
        "\n",
        "        # Split into chunks for parallel processing\n",
        "        variant_chunks = []\n",
        "        for i in range(0, len(variants_to_process), CONFIG['chunk_size']):\n",
        "            chunk = variants_to_process[i:i + CONFIG['chunk_size']]\n",
        "            variant_chunks.append((chunk, config['bam'], dataset_output_dir, dataset_name))\n",
        "\n",
        "        print(f\"    Split into {len(variant_chunks)} chunks\")\n",
        "\n",
        "        # Process in parallel\n",
        "        dataset_stats = {'processed': 0, 'skipped_coverage': 0, 'failed': 0}\n",
        "\n",
        "        with Pool(processes=CONFIG['num_workers']) as pool:\n",
        "            chunk_results = [pool.apply_async(process_variant_chunk_parallel, (chunk_args,))\n",
        "                           for chunk_args in variant_chunks]\n",
        "\n",
        "            # Collect results with progress tracking\n",
        "            with tqdm(total=len(variants_to_process), desc=f\"{dataset_name}\", unit=\"variants\") as pbar:\n",
        "                for result in chunk_results:\n",
        "                    chunk_result = result.get()  # Wait for completion\n",
        "\n",
        "                    # Update statistics\n",
        "                    dataset_stats['processed'] += chunk_result['processed']\n",
        "                    dataset_stats['skipped_coverage'] += chunk_result['skipped_coverage']\n",
        "                    dataset_stats['failed'] += chunk_result['failed']\n",
        "\n",
        "                    # Collect failed/skipped items\n",
        "                    skipped_high_coverage.extend(chunk_result['skipped_high_coverage'])\n",
        "                    failed_variants.extend(chunk_result['failed_variants'])\n",
        "\n",
        "                    total_images_generated += chunk_result['processed']\n",
        "\n",
        "                    # Update progress\n",
        "                    completed = (chunk_result['processed'] +\n",
        "                               chunk_result['skipped_coverage'] +\n",
        "                               chunk_result['failed'])\n",
        "                    pbar.update(completed)\n",
        "                    pbar.set_postfix({\n",
        "                        'new': dataset_stats['processed'],\n",
        "                        'total': total_images_generated,\n",
        "                        'skipped': total_skipped,\n",
        "                        'failed': len(failed_variants)\n",
        "                    })\n",
        "\n",
        "        print(f\"    Completed: {dataset_stats['processed']} new images generated\")\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\n PROCESSING COMPLETE!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  New images generated: {total_images_generated}\")\n",
        "    print(f\"  Existing images skipped: {total_skipped}\")\n",
        "    print(f\"  Failed variants: {len(failed_variants)}\")\n",
        "    print(f\"  High-coverage variants skipped: {len(skipped_high_coverage)}\")\n",
        "    print(f\". Output directory: {CONFIG['cache_dir']}\")\n",
        "    print(f\"  Format: 4-channel CSV-Filter CIGAR\")\n",
        "\n",
        "    # Save skipped variants for later processing\n",
        "    if skipped_high_coverage:\n",
        "        skipped_dir = os.path.join(CONFIG['cache_dir'], 'skipped_variants')\n",
        "        os.makedirs(skipped_dir, exist_ok=True)\n",
        "\n",
        "        with open(os.path.join(skipped_dir, 'high_coverage_variants.json'), 'w') as f:\n",
        "            json.dump(skipped_high_coverage, f, indent=2)\n",
        "        print(f\"\\n Saved {len(skipped_high_coverage)} high-coverage variants for later processing\")\n",
        "\n",
        "    # Show final counts\n",
        "    print(f\"\\n Final file counts:\")\n",
        "    for dataset_name in DATASETS.keys():\n",
        "        dataset_dir = os.path.join(CONFIG['cache_dir'], dataset_name)\n",
        "        if os.path.exists(dataset_dir):\n",
        "            count = len([f for f in os.listdir(dataset_dir) if f.endswith('.pt')])\n",
        "            print(f\"    {dataset_name}: {count} total files\")\n",
        "\n",
        "    return {\n",
        "        'total_generated': total_images_generated,\n",
        "        'total_skipped': total_skipped,\n",
        "        'failed_variants': failed_variants,\n",
        "        'skipped_high_coverage': skipped_high_coverage,\n",
        "        'output_dir': CONFIG['cache_dir']\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test CSV-Filter Functions\n",
        "\n",
        "def test_csv_filter():\n",
        "    \"\"\"Test CSV-Filter functions on a sample variant\"\"\"\n",
        "    print(\"Testing CSV-Filter implementation...\")\n",
        "\n",
        "    # Test parameters\n",
        "    test_bam = DATASETS['HG002_GRCh37']['bam']\n",
        "    test_chrom = '1'\n",
        "    test_pos = 900035\n",
        "    test_end = test_pos + 79\n",
        "\n",
        "    if not os.path.exists(test_bam):\n",
        "        print(f\" Test BAM file not found: {test_bam}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Generate test image\n",
        "        image = cigar_new_img_single_memory(test_bam, test_chrom, test_pos, test_end)\n",
        "\n",
        "        print(f\" CSV-Filter test successful!\")\n",
        "        print(f\"    Image shape: {image.shape}\")\n",
        "        print(f\"    Data type: {image.dtype}\")\n",
        "        print(f\"    Value range: {image.min():.3f} - {image.max():.3f}\")\n",
        "\n",
        "        # Display sample\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "        axes = axes.flatten()\n",
        "        channel_names = ['Match (M)', 'Deletion (D)', 'Insertion (I)', 'Soft Clip (S)']\n",
        "\n",
        "        for i, (ax, name) in enumerate(zip(axes, channel_names)):\n",
        "            img_data = image[i].numpy()\n",
        "            im = ax.imshow(img_data, cmap='hot', aspect='auto')\n",
        "            ax.set_title(f'Channel {i}: {name}')\n",
        "            plt.colorbar(im, ax=ax, shrink=0.8)\n",
        "\n",
        "        plt.suptitle(f'CSV-Filter 4-Channel Test\\n{test_chrom}:{test_pos}-{test_end}', fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" CSV-Filter test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run test\n",
        "test_success = test_csv_filter()\n",
        "\n",
        "if test_success:\n",
        "    print(f\"\\n Ready to process datasets!\")\n",
        "    print(f\" Run: results = process_all_datasets()\")\n",
        "else:\n",
        "    print(f\"\\n Fix test errors before proceeding\")"
      ],
      "metadata": {
        "id": "wBgBVU3zX2ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Processing\n",
        "results = process_all_datasets()"
      ],
      "metadata": {
        "id": "jgW6uwbuYCdd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}