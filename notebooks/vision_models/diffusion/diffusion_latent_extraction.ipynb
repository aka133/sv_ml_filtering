{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Diffusion Latent Feature Extraction & Classification\n",
        "\n",
        "Extract internal representations from trained diffusion model and use as features for downstream classification."
      ],
      "metadata": {
        "id": "PG0fxog1AWUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import json\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Configuration - GitHub repository structure\n",
        "DATA_DIR = '../data/processed/all_datasets_images_rgb'\n",
        "SAVE_DIR = '../data/processed/experiment_results'\n",
        "LATENTS_DIR = '../data/processed/diffusion_latents'\n",
        "FIGURES_DIR = '../figures'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(LATENTS_DIR, exist_ok=True)\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "# Model file paths (from model training notebook)\n",
        "MODEL_FILENAME = 'diffusion_final_model_20250730_171711.pth'\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, MODEL_FILENAME)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "kNvR-UpjBC_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model Architecture (same as previous notebooks)\n",
        "\n",
        "class DiffusionSchedule:\n",
        "    \"\"\"Noise scheduling for diffusion process\"\"\"\n",
        "    def __init__(self, timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
        "        self.timesteps = timesteps\n",
        "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
        "\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "    def to(self, device):\n",
        "        for attr in ['betas', 'alphas', 'alphas_cumprod', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod']:\n",
        "            setattr(self, attr, getattr(self, attr).to(device))\n",
        "        return self\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_dim=128):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Linear(time_dim, out_ch)\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
        "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
        "        self.residual_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        h = self.norm1(self.conv1(x))\n",
        "        h = F.relu(h)\n",
        "        time_emb = self.time_mlp(time_emb)\n",
        "        h = h + time_emb[:, :, None, None]\n",
        "        h = self.norm2(self.conv2(h))\n",
        "        h = F.relu(h)\n",
        "        return h + self.residual_conv(x)\n",
        "\n",
        "class UNetLarge(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, time_dim=256):\n",
        "        super().__init__()\n",
        "        self.time_embedding = TimeEmbedding(time_dim)\n",
        "        self.conv_in = nn.Conv2d(in_channels, 128, 3, padding=1)\n",
        "        self.down1 = ResBlock(128, 128, time_dim)\n",
        "        self.down2 = ResBlock(128, 256, time_dim)\n",
        "        self.down3 = ResBlock(256, 512, time_dim)\n",
        "        self.down4 = ResBlock(512, 1024, time_dim)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.bottleneck = ResBlock(1024, 1024, time_dim)\n",
        "        self.up4 = ResBlock(1024 + 1024, 512, time_dim)\n",
        "        self.up3 = ResBlock(512 + 512, 256, time_dim)\n",
        "        self.up2 = ResBlock(256 + 256, 128, time_dim)\n",
        "        self.up1 = ResBlock(128 + 128, 128, time_dim)\n",
        "        self.conv_out = nn.Conv2d(128, out_channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        time_emb = self.time_embedding(timestep)\n",
        "        # Encoder\n",
        "        x1 = F.relu(self.conv_in(x))\n",
        "        x1 = self.down1(x1, time_emb)\n",
        "        x2 = self.pool(x1)\n",
        "        x2 = self.down2(x2, time_emb)\n",
        "        x3 = self.pool(x2)\n",
        "        x3 = self.down3(x3, time_emb)\n",
        "        x4 = self.pool(x3)\n",
        "        x4 = self.down4(x4, time_emb)\n",
        "        # Bottleneck\n",
        "        x_bottle = self.pool(x4)\n",
        "        x_bottle = self.bottleneck(x_bottle, time_emb)\n",
        "        # Decoder\n",
        "        x = F.interpolate(x_bottle, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, x4], dim=1)\n",
        "        x = self.up4(x, time_emb)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, x3], dim=1)\n",
        "        x = self.up3(x, time_emb)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.up2(x, time_emb)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, x1], dim=1)\n",
        "        x = self.up1(x, time_emb)\n",
        "        return self.conv_out(x)"
      ],
      "metadata": {
        "id": "FzvW1gpJBb3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_-5qIckAKiv"
      },
      "outputs": [],
      "source": [
        "# Dataset for feature extraction\n",
        "class FeatureExtractionDataset(Dataset):\n",
        "    def __init__(self, file_info_list):\n",
        "        self.file_info = file_info_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        info = self.file_info[idx]\n",
        "        try:\n",
        "            data = torch.load(info['filepath'], map_location='cpu')\n",
        "            if isinstance(data, dict):\n",
        "                image = data['image']\n",
        "            else:\n",
        "                image = data\n",
        "\n",
        "            # Handle channels and normalization\n",
        "            if image.shape[0] != 3:\n",
        "                if image.shape[0] < 3:\n",
        "                    padding = torch.zeros(3 - image.shape[0], *image.shape[1:])\n",
        "                    image = torch.cat([image, padding], dim=0)\n",
        "                else:\n",
        "                    image = image[:3]\n",
        "\n",
        "            if image.dtype == torch.uint8:\n",
        "                image = image.float() / 255.0\n",
        "            else:\n",
        "                image = torch.clamp(image / 255.0, 0.0, 1.0)\n",
        "            image = image * 2.0 - 1.0\n",
        "\n",
        "            success = True\n",
        "\n",
        "        except Exception as e:\n",
        "            image = torch.zeros(3, 224, 224) * 2.0 - 1.0\n",
        "            success = False\n",
        "\n",
        "        return image, info, success\n",
        "\n",
        "# Latent Feature Extractor\n",
        "\n",
        "class RawLatentExtractor:\n",
        "    \"\"\"Extract raw latent vectors (higher dimensional)\"\"\"\n",
        "\n",
        "    def __init__(self, model, schedule, device):\n",
        "        self.model = model\n",
        "        self.schedule = schedule\n",
        "        self.device = device\n",
        "        self.activations = {}\n",
        "\n",
        "    def register_hooks_for_raw_vectors(self):\n",
        "        \"\"\"Register hooks for raw vector extraction\"\"\"\n",
        "\n",
        "        def get_activation(name):\n",
        "            def hook(model, input, output):\n",
        "                if isinstance(output, torch.Tensor):\n",
        "                    self.activations[name] = output.detach()\n",
        "            return hook\n",
        "\n",
        "        # Focus on bottleneck and key transition points\n",
        "        self.model.bottleneck.register_forward_hook(get_activation('bottleneck'))\n",
        "        self.model.down4.register_forward_hook(get_activation('down4'))\n",
        "        self.model.up4.register_forward_hook(get_activation('up4'))\n",
        "\n",
        "    def clear_hooks(self):\n",
        "        \"\"\"Clear all registered hooks\"\"\"\n",
        "        for module in self.model.modules():\n",
        "            module._forward_hooks.clear()\n",
        "\n",
        "    def extract_raw_latents_single(self, image, timestep=25):\n",
        "        \"\"\"Extract raw latent vectors for a single image\"\"\"\n",
        "\n",
        "        self.activations.clear()\n",
        "\n",
        "        # Forward pass\n",
        "        t = torch.tensor([timestep], device=self.device).long()\n",
        "        noise = torch.randn_like(image)\n",
        "        noisy_image = self.schedule.q_sample(image, t, noise)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _ = self.model(noisy_image, t)\n",
        "\n",
        "        # Extract flattened vectors\n",
        "        features = {}\n",
        "        for name, activation in self.activations.items():\n",
        "            # Global average pooling to get fixed-size representation\n",
        "            if len(activation.shape) == 4:  # [B, C, H, W]\n",
        "                pooled = F.adaptive_avg_pool2d(activation, (1, 1))\n",
        "                flattened = pooled.view(pooled.shape[0], -1)\n",
        "                features[f'{name}_vector'] = flattened.cpu().numpy().flatten()\n",
        "\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Feature Extraction Functions\n",
        "\n",
        "def load_trained_diffusion_model():\n",
        "    \"\"\"Load the trained diffusion model from Notebook 1\"\"\"\n",
        "\n",
        "    print(\"Loading trained diffusion model...\")\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(f\"Model file not found: {MODEL_PATH}\")\n",
        "        print(\"Please run Notebook 1 (diffusion experiments) first to train the model\")\n",
        "        return None, None\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
        "\n",
        "    # Create model\n",
        "    model = UNetLarge(in_channels=3, out_channels=3, time_dim=256).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Create schedule\n",
        "    schedule_config = checkpoint['schedule_config']\n",
        "    schedule = DiffusionSchedule(\n",
        "        timesteps=schedule_config['timesteps'],\n",
        "        beta_start=schedule_config['beta_start'],\n",
        "        beta_end=schedule_config['beta_end']\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"   Model loaded successfully\")\n",
        "    print(f\"   Architecture: {checkpoint.get('architecture', 'Unknown')}\")\n",
        "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
        "\n",
        "    return model, schedule\n",
        "\n",
        "def load_all_genomic_data():\n",
        "    \"\"\"Load all genomic data files for feature extraction\"\"\"\n",
        "\n",
        "    print(\"Loading all genomic data files...\")\n",
        "\n",
        "    datasets = ['HG002_GRCh37', 'HG002_GRCh38', 'HG005_GRCh38']\n",
        "    all_file_info = []\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        dataset_path = os.path.join(DATA_DIR, dataset_name)\n",
        "        if not os.path.exists(dataset_path):\n",
        "            print(f\"   Dataset not found: {dataset_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"   Scanning {dataset_name}...\")\n",
        "        filenames = [f for f in os.listdir(dataset_path) if f.endswith('.pt')]\n",
        "\n",
        "        for filename in filenames:\n",
        "            filepath = os.path.join(dataset_path, filename)\n",
        "            parts = filename[:-3].split('_')\n",
        "\n",
        "            if len(parts) >= 8:\n",
        "                try:\n",
        "                    label_str = parts[2]  # TP or FP\n",
        "                    svtype = parts[6]     # INS, DEL, etc.\n",
        "\n",
        "                    if label_str in ['TP', 'FP']:\n",
        "                        all_file_info.append({\n",
        "                            'filename': filename,\n",
        "                            'filepath': filepath,\n",
        "                            'dataset': dataset_name,\n",
        "                            'label_str': label_str,\n",
        "                            'svtype': svtype,\n",
        "                            'binary_label': 1 if label_str == 'TP' else 0,\n",
        "                        })\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "    print(f\"   Total files found: {len(all_file_info)}\")\n",
        "\n",
        "    # Show distribution\n",
        "    tp_count = sum(1 for x in all_file_info if x['label_str'] == 'TP')\n",
        "    fp_count = len(all_file_info) - tp_count\n",
        "    print(f\"   TP: {tp_count:,} samples ({100*tp_count/len(all_file_info):.1f}%)\")\n",
        "    print(f\"   FP: {fp_count:,} samples ({100*fp_count/len(all_file_info):.1f}%)\")\n",
        "\n",
        "    return all_file_info\n",
        "\n",
        "def extract_and_save_diffusion_latents(max_samples=None):\n",
        "    \"\"\"Extract and save diffusion latent vectors for all data\"\"\"\n",
        "\n",
        "    print(f\"EXTRACTING AND SAVING DIFFUSION LATENT VECTORS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load model\n",
        "    model, schedule = load_trained_diffusion_model()\n",
        "    if model is None:\n",
        "        return None, None, None\n",
        "\n",
        "    # Load data\n",
        "    all_file_info = load_all_genomic_data()\n",
        "\n",
        "    # Limit samples if specified\n",
        "    if max_samples and len(all_file_info) > max_samples:\n",
        "        import random\n",
        "        random.seed(42)\n",
        "        all_file_info = random.sample(all_file_info, max_samples)\n",
        "        print(f\"Limited to {max_samples} samples for testing\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    def collate_fn(batch):\n",
        "        images = torch.stack([item[0] for item in batch])\n",
        "        infos = [item[1] for item in batch]\n",
        "        successes = [item[2] for item in batch]\n",
        "        return images, infos, successes\n",
        "\n",
        "    dataset = FeatureExtractionDataset(all_file_info)\n",
        "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "    # Create extractor\n",
        "    extractor = RawLatentExtractor(model, schedule, device)\n",
        "    extractor.register_hooks_for_raw_vectors()\n",
        "    # Test extraction to determine feature dimension\n",
        "    test_img = torch.zeros(1, 3, 224, 224).to(device) * 2.0 - 1.0\n",
        "    test_features = extractor.extract_raw_latents_single(test_img)\n",
        "    n_features = sum(len(v) for v in test_features.values())\n",
        "    print(f\"   Raw latent vectors: {n_features} features per sample\")\n",
        "\n",
        "    # Prepare storage\n",
        "    total_samples = len(all_file_info)\n",
        "    features_array = np.zeros((total_samples, n_features), dtype=np.float32)\n",
        "    metadata_list = []\n",
        "\n",
        "    sample_idx = 0\n",
        "    failed_count = 0\n",
        "\n",
        "    print(f\"Processing {total_samples:,} samples...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_images, batch_infos, batch_successes in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "            batch_images = batch_images.to(device)\n",
        "\n",
        "            # Process each image individually\n",
        "            for i, (image, info, success) in enumerate(zip(batch_images, batch_infos, batch_successes)):\n",
        "                if sample_idx >= total_samples:\n",
        "                    break\n",
        "\n",
        "                if success:\n",
        "                    try:\n",
        "                        img = image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "                        features_dict = extractor.extract_raw_latents_single(img, timestep=25)\n",
        "                        # Flatten all vectors\n",
        "                        features_vector = np.concatenate([features_dict[name] for name in sorted(features_dict.keys())])\n",
        "\n",
        "                        features_array[sample_idx] = features_vector\n",
        "                        extraction_success = True\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Feature extraction failed for {info['filename']}: {e}\")\n",
        "                        features_array[sample_idx] = np.zeros(n_features)\n",
        "                        failed_count += 1\n",
        "                        extraction_success = False\n",
        "                else:\n",
        "                    features_array[sample_idx] = np.zeros(n_features)\n",
        "                    failed_count += 1\n",
        "                    extraction_success = False\n",
        "\n",
        "                # Save metadata\n",
        "                metadata_list.append({\n",
        "                    'sample_idx': sample_idx,\n",
        "                    'filename': info['filename'],\n",
        "                    'dataset': info['dataset'],\n",
        "                    'label_str': info['label_str'],\n",
        "                    'svtype': info['svtype'],\n",
        "                    'binary_label': info['binary_label'],\n",
        "                    'extraction_success': extraction_success,\n",
        "                    'filepath': info['filepath']\n",
        "                })\n",
        "\n",
        "                sample_idx += 1\n",
        "\n",
        "    extractor.clear_hooks()\n",
        "\n",
        "    print(f\"Feature extraction complete!\")\n",
        "    print(f\"   Successfully processed: {total_samples - failed_count:,}/{total_samples:,} samples\")\n",
        "    print(f\"   Failed extractions: {failed_count:,}\")\n",
        "\n",
        "    # Save features to HDF5\n",
        "    features_file = os.path.join(LATENTS_DIR, f'diffusion_latent_features.h5')\n",
        "    print(f\"Saving features to: {features_file}\")\n",
        "\n",
        "    with h5py.File(features_file, 'w') as f:\n",
        "        f.create_dataset('features', data=features_array, compression='gzip')\n",
        "        f.attrs['model_path'] = MODEL_PATH\n",
        "        f.attrs['feature_dim'] = n_features\n",
        "        f.attrs['total_samples'] = total_samples\n",
        "        f.attrs['extraction_date'] = datetime.now().isoformat()\n",
        "\n",
        "    # Save metadata to CSV\n",
        "    metadata_file = os.path.join(LATENTS_DIR, f'diffusion_latent_metadata.csv')\n",
        "    print(f\"Saving metadata to: {metadata_file}\")\n",
        "\n",
        "    metadata_df = pd.DataFrame(metadata_list)\n",
        "    metadata_df.to_csv(metadata_file, index=False)\n",
        "\n",
        "    # Create summary\n",
        "    summary = {\n",
        "        'extraction_date': datetime.now().isoformat(),\n",
        "        'model_path': MODEL_PATH,\n",
        "        'total_samples': int(total_samples),\n",
        "        'successful_extractions': int(total_samples - failed_count),\n",
        "        'failed_extractions': int(failed_count),\n",
        "        'feature_dimension': int(n_features),\n",
        "        'features_file': features_file,\n",
        "        'metadata_file': metadata_file,\n",
        "        'datasets': {k: int(v) for k, v in metadata_df['dataset'].value_counts().items()},\n",
        "        'labels': {k: int(v) for k, v in metadata_df['label_str'].value_counts().items()},\n",
        "        'sv_types': {k: int(v) for k, v in metadata_df['svtype'].value_counts().items()}\n",
        "    }\n",
        "\n",
        "    summary_file = os.path.join(LATENTS_DIR, f'diffusion_latent_summary.json')\n",
        "    print(f\"Saving summary to: {summary_file}\")\n",
        "\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nDIFFUSION FEATURE EXTRACTION COMPLETE!\")\n",
        "    print(f\"Files saved in: {LATENTS_DIR}\")\n",
        "    print(f\"   Features: diffusion_latent_features.h5 ({features_array.nbytes / 1024**2:.1f} MB)\")\n",
        "    print(f\"   Metadata: diffusion_latent_metadata.csv\")\n",
        "    print(f\"   Summary: diffusion_latent_summary.json\")\n",
        "\n",
        "    return features_array, metadata_df, summary\n",
        "\n",
        "def load_saved_diffusion_features():\n",
        "    \"\"\"Load previously saved diffusion latent features\"\"\"\n",
        "\n",
        "    features_file = os.path.join(LATENTS_DIR, f'diffusion_latent_features.h5')\n",
        "    metadata_file = os.path.join(LATENTS_DIR, f'diffusion_latent_metadata.csv')\n",
        "\n",
        "    if not os.path.exists(features_file):\n",
        "        print(f\"Features file not found: {features_file}\")\n",
        "        print(\"Please run feature extraction first\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\"Loading saved diffusion latent features...\")\n",
        "\n",
        "    # Load features\n",
        "    with h5py.File(features_file, 'r') as f:\n",
        "        features = f['features'][:]\n",
        "        attrs = dict(f.attrs)\n",
        "\n",
        "    # Load metadata\n",
        "    metadata_df = pd.read_csv(metadata_file)\n",
        "\n",
        "    print(f\"   Loaded {len(features):,} diffusion feature vectors\")\n",
        "    print(f\"   Feature dimension: {features.shape[1]}\")\n",
        "\n",
        "    return features, metadata_df, attrs"
      ],
      "metadata": {
        "id": "gOuNkMz1C7Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Functions\n",
        "\n",
        "def train_classifiers_on_latents(test_size=0.2):\n",
        "    \"\"\"Train multiple classifiers on diffusion latent features\"\"\"\n",
        "\n",
        "    print(f\"TRAINING CLASSIFIERS ON DIFFUSION LATENTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load features\n",
        "    features, metadata_df, attrs = load_saved_diffusion_features()\n",
        "    if features is None:\n",
        "        return None\n",
        "\n",
        "    # Prepare data\n",
        "    X = features\n",
        "    y = metadata_df['binary_label'].values\n",
        "\n",
        "    print(f\"Dataset:\")\n",
        "    print(f\"   Total samples: {len(X):,}\")\n",
        "    print(f\"   Features: {X.shape[1]}\")\n",
        "    print(f\"   TP samples: {sum(y == 1):,}\")\n",
        "    print(f\"   FP samples: {sum(y == 0):,}\")\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTrain/Test split:\")\n",
        "    print(f\"   Training: {len(X_train):,} samples\")\n",
        "    print(f\"   Testing: {len(X_test):,} samples\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Define classifiers to test\n",
        "    classifiers = {\n",
        "        'Logistic Regression': LogisticRegression(\n",
        "            random_state=42, max_iter=1000, class_weight='balanced'\n",
        "        ),\n",
        "        'Random Forest': RandomForestClassifier(\n",
        "            n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'\n",
        "        ),\n",
        "        'Random Forest (Large)': RandomForestClassifier(\n",
        "            n_estimators=300, max_depth=15, random_state=42, n_jobs=-1, class_weight='balanced'\n",
        "        )\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(f\"\\nTraining classifiers...\")\n",
        "\n",
        "    for name, classifier in classifiers.items():\n",
        "        print(f\"   Training {name}...\")\n",
        "\n",
        "        # Train\n",
        "        classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = classifier.predict(X_test_scaled)\n",
        "        y_pred_proba = classifier.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "        # Calculate metrics\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        results[name] = {\n",
        "            'f1_score': f1,\n",
        "            'auc': auc,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'accuracy': accuracy,\n",
        "            'classifier': classifier,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba\n",
        "        }\n",
        "\n",
        "        print(f\"      F1: {f1:.3f}, AUC: {auc:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}\")\n",
        "\n",
        "    # Save results\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    results_summary = {\n",
        "        'timestamp': timestamp,\n",
        "        'feature_dimension': X.shape[1],\n",
        "        'training_samples': len(X_train),\n",
        "        'test_samples': len(X_test),\n",
        "        'classifier_results': {}\n",
        "    }\n",
        "\n",
        "    for name, result in results.items():\n",
        "        results_summary['classifier_results'][name] = {\n",
        "            'f1_score': float(result['f1_score']),\n",
        "            'auc': float(result['auc']),\n",
        "            'precision': float(result['precision']),\n",
        "            'recall': float(result['recall']),\n",
        "            'accuracy': float(result['accuracy'])\n",
        "        }\n",
        "\n",
        "    results_file = os.path.join(SAVE_DIR, f'diffusion_latents_classification_{timestamp}.json')\n",
        "    with open(results_file, 'w') as f:\n",
        "        json.dump(results_summary, f, indent=2)\n",
        "\n",
        "    print(f\"\\nResults saved: {results_file}\")\n",
        "\n",
        "    return results, X_test, y_test, scaler\n",
        "\n",
        "def compare_with_threshold_optimization(latent_results):\n",
        "    \"\"\"Compare latent feature results with threshold optimization\"\"\"\n",
        "\n",
        "    print(f\"\\nCOMPARISON WITH THRESHOLD OPTIMIZATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load threshold optimization results\n",
        "    import glob\n",
        "    threshold_files = glob.glob(os.path.join(SAVE_DIR, 'threshold_optimization_results_*.json'))\n",
        "\n",
        "    if not threshold_files:\n",
        "        print(\"No threshold optimization results found\")\n",
        "        print(\"Please run Notebook 2 (threshold optimization) first\")\n",
        "        return\n",
        "\n",
        "    latest_threshold_file = max(threshold_files, key=os.path.getctime)\n",
        "    with open(latest_threshold_file, 'r') as f:\n",
        "        threshold_results = json.load(f)\n",
        "\n",
        "    threshold_f1 = threshold_results['optimal_threshold_results']['f1']['f1']\n",
        "\n",
        "    print(f\"Threshold Optimization F1: {threshold_f1:.3f}\")\n",
        "    print(f\"\\nDiffusion Latent Features:\")\n",
        "\n",
        "    best_latent_f1 = 0\n",
        "    best_classifier = \"\"\n",
        "\n",
        "    for name, result in latent_results.items():\n",
        "        if isinstance(result, dict) and 'f1_score' in result:\n",
        "            f1 = result['f1_score']\n",
        "            improvement = f1 - threshold_f1\n",
        "            print(f\"   {name}: F1={f1:.3f} (diff: {improvement:+.3f})\")\n",
        "\n",
        "            if f1 > best_latent_f1:\n",
        "                best_latent_f1 = f1\n",
        "                best_classifier = name\n",
        "\n",
        "    print(f\"\\nBEST APPROACH:\")\n",
        "    improvement = best_latent_f1 - threshold_f1\n",
        "\n",
        "    if improvement > 0.005:  # Meaningful improvement\n",
        "        print(f\"   LATENT FEATURES WIN!\")\n",
        "        print(f\"   Best: {best_classifier} with F1={best_latent_f1:.3f}\")\n",
        "        print(f\"   Improvement: +{improvement:.3f} ({improvement/threshold_f1*100:.1f}%)\")\n",
        "        print(f\"   Conclusion: Diffusion internal representations are better features\")\n",
        "    elif improvement > -0.005:  # Essentially tied\n",
        "        print(f\"   ESSENTIALLY TIED\")\n",
        "        print(f\"   Threshold F1: {threshold_f1:.3f}\")\n",
        "        print(f\"   Latent F1: {best_latent_f1:.3f}\")\n",
        "        print(f\"   Conclusion: Both approaches are equally effective\")\n",
        "    else:\n",
        "        print(f\"   THRESHOLD OPTIMIZATION WINS\")\n",
        "        print(f\"   Threshold F1: {threshold_f1:.3f}\")\n",
        "        print(f\"   Best Latent F1: {best_latent_f1:.3f}\")\n",
        "        print(f\"   Conclusion: Simple threshold optimization is better\")\n",
        "\n",
        "def create_latent_results_visualization(latent_results):\n",
        "    \"\"\"Create visualization comparing latent feature classifiers\"\"\"\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Extract results for plotting\n",
        "    classifiers = []\n",
        "    f1_scores = []\n",
        "    auc_scores = []\n",
        "\n",
        "    for name, result in latent_results.items():\n",
        "        if isinstance(result, dict) and 'f1_score' in result:\n",
        "            classifiers.append(name)\n",
        "            f1_scores.append(result['f1_score'])\n",
        "            auc_scores.append(result['auc'])\n",
        "\n",
        "    # Plot 1: F1 Scores\n",
        "    bars1 = ax1.bar(classifiers, f1_scores, alpha=0.7, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
        "    ax1.set_ylabel('F1-Score')\n",
        "    ax1.set_title(f'F1-Score Comparison\\nDiffusion Latent Features')\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, score in zip(bars1, f1_scores):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 2: AUC Scores\n",
        "    bars2 = ax2.bar(classifiers, auc_scores, alpha=0.7, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
        "    ax2.set_ylabel('AUC Score')\n",
        "    ax2.set_title(f'AUC Comparison\\nDiffusion Latent Features')\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, score in zip(bars2, auc_scores):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    plot_file = os.path.join(FIGURES_DIR, f'diffusion_latents_comparison_{timestamp}.png')\n",
        "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Visualization saved: {plot_file}\")\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "fxv81xrLAWEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution functions\n",
        "\n",
        "def run_full_latent_pipeline(max_samples=None):\n",
        "    \"\"\"Run the complete latent feature pipeline\"\"\"\n",
        "\n",
        "    print(f\"RUNNING FULL DIFFUSION LATENT PIPELINE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Step 1: Extract and save features (if not already done)\n",
        "    print(\"STEP 1: Extract and save diffusion latent features\")\n",
        "    features_file = os.path.join(LATENTS_DIR, f'diffusion_latent_features.h5')\n",
        "\n",
        "    if os.path.exists(features_file):\n",
        "        print(\"   Features already extracted, loading from disk...\")\n",
        "        features, metadata_df, attrs = load_saved_diffusion_features()\n",
        "    else:\n",
        "        print(\"   Extracting features from diffusion model...\")\n",
        "        features, metadata_df, summary = extract_and_save_diffusion_latents(max_samples)\n",
        "\n",
        "    if features is None:\n",
        "        print(\"Feature extraction failed\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Train classifiers\n",
        "    print(\"\\nSTEP 2: Train classifiers on latent features\")\n",
        "    latent_results, X_test, y_test, scaler = train_classifiers_on_latents()\n",
        "\n",
        "    if latent_results is None:\n",
        "        print(\"Classifier training failed\")\n",
        "        return None\n",
        "\n",
        "    # Step 3: Compare with threshold optimization\n",
        "    print(\"\\nSTEP 3: Compare with threshold optimization\")\n",
        "    compare_with_threshold_optimization(latent_results)\n",
        "\n",
        "    # Step 4: Create visualizations\n",
        "    print(\"\\nSTEP 4: Create visualizations\")\n",
        "    fig = create_latent_results_visualization(latent_results)\n",
        "\n",
        "    print(f\"\\nFULL LATENT PIPELINE COMPLETE!\")\n",
        "    print(f\"Feature dimension: {features.shape[1]}\")\n",
        "    print(f\"Total samples: {len(features):,}\")\n",
        "\n",
        "    return latent_results, features, metadata_df"
      ],
      "metadata": {
        "id": "MvEquSeYC-ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"EXTRACTION AND SAVING:\")\n",
        "print(\"   # Extract raw latent vectors\")\n",
        "print(\"   features, metadata, summary = extract_and_save_diffusion_latents()\")\n",
        "print()\n",
        "print(\"LOADING AND CLASSIFICATION:\")\n",
        "print(\"   # Load saved features and train classifiers\")\n",
        "print(\"   results = train_classifiers_on_latents()\")\n",
        "print()\n",
        "print(\"FULL PIPELINE:\")\n",
        "print(\"   # Run complete pipeline\")\n",
        "print(\"   results, features, metadata = run_full_latent_pipeline()\")\n",
        "print()\n",
        "print(\"   # Test with limited samples first\")\n",
        "print(\"   results, features, metadata = run_full_latent_pipeline(max_samples=5000)\")"
      ],
      "metadata": {
        "id": "wbwBBftqDDK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results, features, metadata = run_full_latent_pipeline()"
      ],
      "metadata": {
        "id": "kO9s96vwDF50"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}