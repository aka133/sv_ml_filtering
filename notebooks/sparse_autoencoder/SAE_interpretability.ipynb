{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SAE Interpretability Analysis\n",
        "Analyze which Sparse Autoencoder features are biased towards true positive vs false positive structural variant calls, and investigate biological patterns in feature activations.\n",
        " - Run SAE training notebook first to generate trained model\n",
        " - Embedding extraction notebook for test data\n"
      ],
      "metadata": {
        "id": "IQUEqubs9ulp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from scipy.stats import chi2_contingency, fisher_exact, mannwhitneyu\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "K5GDzXVl-FeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test embeddings and SAE model\n",
        "print(\"Loading test embeddings and trained SAE model...\")\n",
        "\n",
        "# Load test data\n",
        "test_pkg = torch.load(\"../data/processed/sae_sv_embeddings.pt\", map_location=\"cpu\")\n",
        "test_emb = test_pkg[\"embeddings\"].float().to(device)\n",
        "test_sv_info = test_pkg[\"sv_info\"]\n",
        "\n",
        "# Load best SAE model\n",
        "best_model_path = pathlib.Path(\"../data/models/BEST_MODEL.json\")\n",
        "if best_model_path.exists():\n",
        "    with open(best_model_path) as f:\n",
        "        best_meta = json.load(f)\n",
        "    model_dir = pathlib.Path(best_meta[\"best_dir\"])\n",
        "    ckpt_path = model_dir / \"sae.pt\"\n",
        "else:\n",
        "    # Fallback to standard path\n",
        "    model_dir = pathlib.Path(\"../data/models/4096to4096_k64\")\n",
        "    ckpt_path = model_dir / \"sae.pt\"\n",
        "\n",
        "pkg = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "cfg = pkg[\"config\"]\n",
        "input_dim = int(cfg[\"input_dim\"])\n",
        "feature_dim = int(cfg[\"feature_dim\"])\n",
        "k_active = int(cfg[\"k\"])\n",
        "\n",
        "print(f\"SAE config: {input_dim} -> {feature_dim}, k={k_active}\")"
      ],
      "metadata": {
        "id": "mK08elve-IRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate SAE model class if needed\n",
        "class BatchTopKSAE(torch.nn.Module):\n",
        "    \"\"\"Sparse Autoencoder with Top-K activation\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, feature_dim, k_active):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.feature_dim = feature_dim\n",
        "        self.k_active = k_active\n",
        "\n",
        "        self.encoder = torch.nn.Linear(input_dim, feature_dim, bias=True)\n",
        "        self.decoder = torch.nn.Linear(feature_dim, input_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        k = min(self.k_active, self.feature_dim)\n",
        "        topk_values, topk_indices = torch.topk(features, k, dim=-1)\n",
        "        sparse_features = torch.zeros_like(features)\n",
        "        sparse_features.scatter_(-1, topk_indices, topk_values)\n",
        "        reconstructed = self.decoder(sparse_features)\n",
        "        return {\n",
        "            'reconstructed': reconstructed,\n",
        "            'sparse_features': sparse_features,\n",
        "            'dense_features': features\n",
        "        }\n",
        "\n",
        "    def get_feature_activations(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.encoder(x)\n",
        "            k = min(self.k_active, self.feature_dim)\n",
        "            _, topk_indices = torch.topk(features, k, dim=-1)\n",
        "            return topk_indices, features\n",
        "\n",
        "# Load and setup SAE\n",
        "sae = BatchTopKSAE(input_dim, feature_dim, k_active).to(device)\n",
        "sae.load_state_dict(pkg[\"model_state_dict\"], strict=False)\n",
        "sae.eval()"
      ],
      "metadata": {
        "id": "b1CkWr_g-K6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get whitening parameters\n",
        "mu = pkg[\"config\"].get(\"μ\", pkg[\"config\"].get(\"mu\"))\n",
        "sigma = pkg[\"config\"].get(\"σ\", pkg[\"config\"].get(\"std\"))\n",
        "if not isinstance(mu, torch.Tensor):\n",
        "    mu = torch.tensor(mu)\n",
        "if not isinstance(sigma, torch.Tensor):\n",
        "    sigma = torch.tensor(sigma)\n",
        "mu = mu.view(1, -1).float()\n",
        "sigma = (sigma.view(1, -1).float() + 1e-6)\n",
        "\n",
        "# Whiten test embeddings\n",
        "test_emb_w = (test_emb - mu) / sigma\n",
        "\n",
        "print(f\"Loaded {len(test_sv_info)} test samples\")\n",
        "\n",
        "# Extract feature activations\n",
        "print(\"Extracting SAE feature activations...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    acts, _ = sae.get_feature_activations(test_emb_w)\n",
        "\n",
        "# Create indicator matrix for analysis\n",
        "N, k_active = acts.shape\n",
        "ind_mat = torch.zeros(N, feature_dim, dtype=torch.bool, device=device)\n",
        "ind_mat.scatter_(1, acts, True)\n",
        "\n",
        "# Create labels (TP=True, FP=False)\n",
        "labels = torch.tensor([\n",
        "    sv.get('truvari_class') in ['TP', 'tp_comp_vcf']\n",
        "    for sv in test_sv_info\n",
        "], dtype=torch.bool).to(device)\n",
        "\n",
        "print(f\"Feature activations: {acts.shape}\")\n",
        "print(f\"Indicator matrix: {ind_mat.shape}\")\n",
        "print(f\"Labels: TP={labels.sum().item()}, FP={(~labels).sum().item()}\")"
      ],
      "metadata": {
        "id": "X84PATP0-N_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute TP/FP bias for each feature using chi-square test\n",
        "print(\"Computing feature bias statistics...\")\n",
        "\n",
        "tp_total = int(labels.sum())\n",
        "fp_total = int((~labels).sum())\n",
        "\n",
        "bias_records = []\n",
        "for atom in range(feature_dim):\n",
        "    tp_on = int(ind_mat[labels, atom].sum())\n",
        "    fp_on = int(ind_mat[~labels, atom].sum())\n",
        "\n",
        "    if tp_on + fp_on < 5:  # Skip rarely activated features\n",
        "        continue\n",
        "\n",
        "    tp_off = tp_total - tp_on\n",
        "    fp_off = fp_total - fp_on\n",
        "\n",
        "    try:\n",
        "        _, p_value, _, _ = chi2_contingency([[tp_on, fp_on], [tp_off, fp_off]], correction=False)\n",
        "        odds_ratio = (tp_on * fp_off) / max(1, fp_on * tp_off)\n",
        "        support = tp_on + fp_on\n",
        "\n",
        "        bias_records.append({\n",
        "            'atom': atom,\n",
        "            'TP_on': tp_on,\n",
        "            'FP_on': fp_on,\n",
        "            'TP_off': tp_off,\n",
        "            'FP_off': fp_off,\n",
        "            'odds': odds_ratio,\n",
        "            'support': support,\n",
        "            'p': p_value\n",
        "        })\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "stats_df = pd.DataFrame(bias_records)\n",
        "\n",
        "# Apply FDR correction\n",
        "if len(stats_df) > 0:\n",
        "    _, fdr_corrected, _, _ = multipletests(stats_df['p'], method='fdr_bh')\n",
        "    stats_df['fdr_corrected'] = fdr_corrected\n",
        "    stats_df['significant'] = fdr_corrected < 0.05\n",
        "\n",
        "print(f\"Analyzed {len(stats_df)} features with ≥5 activations\")\n",
        "\n",
        "# Feature bias landscape overview\n",
        "total = len(stats_df)\n",
        "tp_bias = (stats_df[\"odds\"] > 1).sum()\n",
        "fp_bias = (stats_df[\"odds\"] < 1).sum()\n",
        "strong_tp = (stats_df[\"odds\"] > 2).sum()\n",
        "strong_fp = (stats_df[\"odds\"] < 0.5).sum()\n",
        "significant = stats_df['significant'].sum()\n",
        "\n",
        "print(\" SAE Feature Bias Landscape\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Total features analyzed: {total}\")\n",
        "print(f\"TP-biased (OR > 1): {tp_bias} ({tp_bias/total*100:.1f}%)\")\n",
        "print(f\"FP-biased (OR < 1): {fp_bias} ({fp_bias/total*100:.1f}%)\")\n",
        "print(f\"Strong TP-bias (OR > 2): {strong_tp}\")\n",
        "print(f\"Strong FP-bias (OR < 0.5): {strong_fp}\")\n",
        "print(f\"Statistically significant: {significant}\")\n",
        "\n",
        "support_stats = stats_df[\"support\"].describe()\n",
        "print(f\"\\nSupport statistics:\")\n",
        "print(f\"  Mean: {support_stats['mean']:.1f}\")\n",
        "print(f\"  Median: {support_stats['50%']:.1f}\")\n",
        "print(f\"  Range: {support_stats['min']:.0f} - {support_stats['max']:.0f}\")"
      ],
      "metadata": {
        "id": "aomLzOF_-qCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top Biased Features Analysis\n",
        "\n",
        "def show_top_features(df, title, n=10, ascending=False):\n",
        "    \"\"\"Display top biased features\"\"\"\n",
        "    if df.empty:\n",
        "        print(f\"{title}: No features found\")\n",
        "        return\n",
        "\n",
        "    if ascending:\n",
        "        subset = df.nsmallest(n, \"odds\")\n",
        "    else:\n",
        "        subset = df.nlargest(n, \"odds\")\n",
        "\n",
        "    print(f\"\\n{title}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Atom':<6} {'TP_on':<6} {'FP_on':<6} {'Odds_Ratio':<12} {'Support':<8} {'P-value':<10} {'FDR_Sig'}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for _, row in subset.iterrows():\n",
        "        sig_marker = \"✓\" if row.get('significant', False) else \"✗\"\n",
        "        print(f\"{int(row['atom']):<6} {int(row['TP_on']):<6} {int(row['FP_on']):<6} \"\n",
        "              f\"{row['odds']:<12.3f} {int(row['support']):<8} {row['p']:<10.2e} {sig_marker}\")\n",
        "\n",
        "# Show top TP and FP biased features\n",
        "show_top_features(stats_df[stats_df[\"odds\"] > 1], \"Top TP-biased Features\", n=10)\n",
        "show_top_features(stats_df[stats_df[\"odds\"] < 1], \"Top FP-biased Features\", n=10, ascending=True)"
      ],
      "metadata": {
        "id": "yBCSCwtX--h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SV Type Bias Analysis\n",
        "\n",
        "def analyze_sv_type_bias(min_activations=10):\n",
        "    \"\"\"Analyze which features are biased towards specific SV types\"\"\"\n",
        "\n",
        "    print(\" SV Type Bias Analysis\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Get SV types for all samples\n",
        "    sv_types = [sv.get('svtype', 'UNK') for sv in test_sv_info]\n",
        "    sv_type_counts = Counter(sv_types)\n",
        "\n",
        "    print(\"Overall SV type distribution:\")\n",
        "    for svtype, count in sv_type_counts.most_common():\n",
        "        print(f\"  {svtype}: {count} ({count/len(sv_types)*100:.1f}%)\")\n",
        "\n",
        "    # Analyze INS vs DEL bias specifically\n",
        "    ins_indices = [i for i, svtype in enumerate(sv_types) if svtype == 'INS']\n",
        "    del_indices = [i for i, svtype in enumerate(sv_types) if svtype == 'DEL']\n",
        "\n",
        "    if len(ins_indices) > 0 and len(del_indices) > 0:\n",
        "        ins_mask = torch.zeros(len(sv_types), dtype=torch.bool, device=device)\n",
        "        ins_mask[ins_indices] = True\n",
        "\n",
        "        print(f\"\\n INS vs DEL Feature Bias (minimum {min_activations} activations):\")\n",
        "        print(\"-\" * 70)\n",
        "        print(f\"{'Atom':<6} {'INS_count':<10} {'DEL_count':<10} {'INS_rate':<10} {'Bias_score':<12}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        ins_del_results = []\n",
        "\n",
        "        for atom_id in stats_df['atom'].values:\n",
        "            # Get activations for this atom\n",
        "            atom_activations = (acts == atom_id).any(dim=1)\n",
        "\n",
        "            ins_activations = (atom_activations & ins_mask).sum().item()\n",
        "            del_activations = (atom_activations & ~ins_mask).sum().item()\n",
        "            total_activations = ins_activations + del_activations\n",
        "\n",
        "            if total_activations < min_activations:\n",
        "                continue\n",
        "\n",
        "            ins_rate = ins_activations / total_activations if total_activations > 0 else 0\n",
        "            bias_score = ins_rate - 0.5  # Bias relative to balanced (0.5)\n",
        "\n",
        "            ins_del_results.append({\n",
        "                'atom': atom_id,\n",
        "                'ins_count': ins_activations,\n",
        "                'del_count': del_activations,\n",
        "                'total': total_activations,\n",
        "                'ins_rate': ins_rate,\n",
        "                'bias_score': bias_score\n",
        "            })\n",
        "\n",
        "        # Sort by bias score and show extremes\n",
        "        ins_del_df = pd.DataFrame(ins_del_results)\n",
        "\n",
        "        if len(ins_del_df) > 0:\n",
        "            # Top INS-biased\n",
        "            top_ins = ins_del_df.nlargest(5, 'bias_score')\n",
        "            for _, row in top_ins.iterrows():\n",
        "                print(f\"{int(row['atom']):<6} {int(row['ins_count']):<10} {int(row['del_count']):<10} \"\n",
        "                      f\"{row['ins_rate']:<10.3f} {row['bias_score']:<12.3f}\")\n",
        "\n",
        "            print(\"  ...\")\n",
        "\n",
        "            # Top DEL-biased\n",
        "            top_del = ins_del_df.nsmallest(5, 'bias_score')\n",
        "            for _, row in top_del.iterrows():\n",
        "                print(f\"{int(row['atom']):<6} {int(row['ins_count']):<10} {int(row['del_count']):<10} \"\n",
        "                      f\"{row['ins_rate']:<10.3f} {row['bias_score']:<12.3f}\")\n",
        "\n",
        "            print(f\"\\nStrong INS-biased features (bias > 0.3): {(ins_del_df['bias_score'] > 0.3).sum()}\")\n",
        "            print(f\"Strong DEL-biased features (bias < -0.3): {(ins_del_df['bias_score'] < -0.3).sum()}\")\n",
        "\n",
        "            return ins_del_df\n",
        "\n",
        "    return None\n",
        "\n",
        "ins_del_bias_df = analyze_sv_type_bias()"
      ],
      "metadata": {
        "id": "z_QbVXrP_Fj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcKLksPB9ZJZ"
      },
      "outputs": [],
      "source": [
        "# Detailed Feature Investigation\n",
        "\n",
        "def investigate_feature(atom_id, stats_df, test_sv_info, acts, labels):\n",
        "    \"\"\"Detailed investigation of a specific feature\"\"\"\n",
        "\n",
        "    # Get feature statistics\n",
        "    feature_stats = stats_df[stats_df['atom'] == atom_id]\n",
        "    if len(feature_stats) == 0:\n",
        "        print(f\"Feature {atom_id} not found in statistics\")\n",
        "        return\n",
        "\n",
        "    stats = feature_stats.iloc[0]\n",
        "\n",
        "    print(f\" Feature {atom_id} Detailed Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Activations: TP={stats['TP_on']}, FP={stats['FP_on']}, Total={stats['support']}\")\n",
        "    print(f\"Odds Ratio: {stats['odds']:.3f}\")\n",
        "    print(f\"P-value: {stats['p']:.2e}\")\n",
        "    if 'significant' in stats:\n",
        "        print(f\"FDR Significant: {'Yes' if stats['significant'] else 'No'}\")\n",
        "\n",
        "    # Find samples where this feature fires\n",
        "    firing_mask = (acts == atom_id).any(dim=1)\n",
        "    firing_indices = torch.where(firing_mask)[0].cpu().numpy()\n",
        "\n",
        "    if len(firing_indices) == 0:\n",
        "        print(\"No activations found\")\n",
        "        return\n",
        "\n",
        "    # Analyze SV characteristics\n",
        "    firing_svs = [test_sv_info[idx] for idx in firing_indices]\n",
        "\n",
        "    # SV types\n",
        "    sv_types = [sv.get('svtype', 'UNK') for sv in firing_svs]\n",
        "    type_counts = Counter(sv_types)\n",
        "    print(f\"\\nSV Types:\")\n",
        "    for svtype, count in type_counts.most_common():\n",
        "        print(f\"  {svtype}: {count} ({count/len(sv_types)*100:.1f}%)\")\n",
        "\n",
        "    # Size distribution\n",
        "    sizes = [sv.get('svlen', 0) for sv in firing_svs]\n",
        "    sizes = [s for s in sizes if s > 0]\n",
        "    if sizes:\n",
        "        print(f\"\\nSize Statistics:\")\n",
        "        print(f\"  Mean: {np.mean(sizes):.0f} bp\")\n",
        "        print(f\"  Median: {np.median(sizes):.0f} bp\")\n",
        "        print(f\"  Range: {min(sizes):.0f} - {max(sizes):.0f} bp\")\n",
        "\n",
        "    # Chromosome distribution\n",
        "    chroms = [sv.get('chrom', 'unknown') for sv in firing_svs]\n",
        "    chrom_counts = Counter(chroms)\n",
        "    print(f\"\\nTop Chromosomes:\")\n",
        "    for chrom, count in chrom_counts.most_common(5):\n",
        "        print(f\"  {chrom}: {count} ({count/len(chroms)*100:.1f}%)\")\n",
        "\n",
        "    # Dataset distribution\n",
        "    datasets = [sv.get('dataset', 'unknown') for sv in firing_svs]\n",
        "    dataset_counts = Counter(datasets)\n",
        "    print(f\"\\nDatasets:\")\n",
        "    for dataset, count in dataset_counts.items():\n",
        "        print(f\"  {dataset}: {count} ({count/len(datasets)*100:.1f}%)\")\n",
        "\n",
        "    # Show a few example variants\n",
        "    print(f\"\\nExample Variants (first 5):\")\n",
        "    print(f\"{'Index':<8} {'Chrom':<8} {'Pos':<12} {'Type':<6} {'Size':<10} {'Class'}\")\n",
        "    print(\"-\" * 55)\n",
        "    for i, idx in enumerate(firing_indices[:5]):\n",
        "        sv = test_sv_info[idx]\n",
        "        is_tp = labels[idx].item()\n",
        "        class_label = 'TP' if is_tp else 'FP'\n",
        "        print(f\"{idx:<8} {sv.get('chrom', 'unk'):<8} {sv.get('pos', 0):<12} \"\n",
        "              f\"{sv.get('svtype', 'UNK'):<6} {sv.get('svlen', 0):<10} {class_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate top biased features\n",
        "print(\" Investigating Top Features\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Top TP-biased features\n",
        "top_tp_features = stats_df.nlargest(3, 'odds')['atom'].tolist()\n",
        "for atom_id in top_tp_features:\n",
        "    investigate_feature(atom_id, stats_df, test_sv_info, acts, labels)\n",
        "    print()\n",
        "\n",
        "# Top FP-biased features\n",
        "top_fp_features = stats_df.nsmallest(3, 'odds')['atom'].tolist()\n",
        "for atom_id in top_fp_features:\n",
        "    investigate_feature(atom_id, stats_df, test_sv_info, acts, labels)\n",
        "    print()"
      ],
      "metadata": {
        "id": "2amiN9SX_LLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Co-activation Analysis\n",
        "\n",
        "def analyze_feature_coactivation(top_features, acts, min_coactivation=5):\n",
        "    \"\"\"Analyze which features tend to activate together\"\"\"\n",
        "\n",
        "    print(\" Feature Co-activation Analysis\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    coactivation_matrix = {}\n",
        "\n",
        "    for i, feat1 in enumerate(top_features):\n",
        "        for j, feat2 in enumerate(top_features[i+1:], i+1):\n",
        "            # Count samples where both features activate\n",
        "            mask1 = (acts == feat1).any(dim=1)\n",
        "            mask2 = (acts == feat2).any(dim=1)\n",
        "            coactivation_count = (mask1 & mask2).sum().item()\n",
        "\n",
        "            if coactivation_count >= min_coactivation:\n",
        "                coactivation_matrix[(feat1, feat2)] = coactivation_count\n",
        "\n",
        "    if coactivation_matrix:\n",
        "        print(f\"Feature pairs with ≥{min_coactivation} co-activations:\")\n",
        "        sorted_pairs = sorted(coactivation_matrix.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for (feat1, feat2), count in sorted_pairs[:10]:\n",
        "            print(f\"  Features {feat1} & {feat2}: {count} co-activations\")\n",
        "    else:\n",
        "        print(\"No significant co-activations found\")\n",
        "\n",
        "# Analyze co-activation among top biased features\n",
        "top_biased_features = (stats_df.nlargest(10, 'odds')['atom'].tolist() +\n",
        "                      stats_df.nsmallest(10, 'odds')['atom'].tolist())\n",
        "analyze_feature_coactivation(top_biased_features, acts)\n"
      ],
      "metadata": {
        "id": "CokRrkFV_Xiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary\n",
        "\n",
        "def generate_interpretability_summary(stats_df):\n",
        "    \"\"\"Generate summary of interpretability findings\"\"\"\n",
        "\n",
        "    print(\" SAE Interpretability Summary\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Overall statistics\n",
        "    total_features = len(stats_df)\n",
        "    strong_tp = (stats_df['odds'] > 2).sum()\n",
        "    strong_fp = (stats_df['odds'] < 0.5).sum()\n",
        "    significant = stats_df.get('significant', pd.Series(dtype=bool)).sum()\n",
        "\n",
        "    print(f\"Features analyzed: {total_features}\")\n",
        "    print(f\"Strong TP-biased features (OR > 2): {strong_tp}\")\n",
        "    print(f\"Strong FP-biased features (OR < 0.5): {strong_fp}\")\n",
        "    print(f\"Statistically significant features: {significant}\")\n",
        "\n",
        "    # Key findings\n",
        "    print(f\"\\n Key Findings:\")\n",
        "\n",
        "    if strong_tp > 0:\n",
        "        top_tp_odds = stats_df['odds'].max()\n",
        "        print(f\"• Strongest TP bias: OR = {top_tp_odds:.2f}\")\n",
        "\n",
        "    if strong_fp > 0:\n",
        "        top_fp_odds = stats_df['odds'].min()\n",
        "        print(f\"• Strongest FP bias: OR = {top_fp_odds:.2f}\")\n",
        "\n",
        "    # Support analysis\n",
        "    high_support = (stats_df['support'] > stats_df['support'].quantile(0.75)).sum()\n",
        "    print(f\"• High-support features (>75th percentile): {high_support}\")\n",
        "\n",
        "    # Size specialization (if ins_del analysis was run)\n",
        "    if 'ins_del_bias_df' in globals() and ins_del_bias_df is not None:\n",
        "        strong_ins_bias = (ins_del_bias_df['bias_score'] > 0.3).sum()\n",
        "        strong_del_bias = (ins_del_bias_df['bias_score'] < -0.3).sum()\n",
        "        print(f\"• INS-specialized features: {strong_ins_bias}\")\n",
        "        print(f\"• DEL-specialized features: {strong_del_bias}\")\n",
        "\n",
        "# Save results\n",
        "results_dir = pathlib.Path(\"../data/models\")\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save feature statistics\n",
        "stats_df.to_csv(results_dir / \"sae_feature_bias_analysis.csv\", index=False)\n",
        "\n",
        "# Save feature details for top biased features\n",
        "top_features_analysis = {\n",
        "    'top_tp_biased': stats_df.nlargest(10, 'odds')[['atom', 'odds', 'support', 'TP_on', 'FP_on']].to_dict('records'),\n",
        "    'top_fp_biased': stats_df.nsmallest(10, 'odds')[['atom', 'odds', 'support', 'TP_on', 'FP_on']].to_dict('records'),\n",
        "}\n",
        "\n",
        "with open(results_dir / \"top_biased_features.json\", 'w') as f:\n",
        "    json.dump(top_features_analysis, f, indent=2)\n",
        "\n",
        "generate_interpretability_summary(stats_df)\n",
        "\n",
        "print(f\"\\n Results saved to {results_dir}\")\n",
        "print(f\" Interpretability analysis complete!\")"
      ],
      "metadata": {
        "id": "BIfPgYQq_NCG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}