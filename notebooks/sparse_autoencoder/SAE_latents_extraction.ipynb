{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SAE Data Extraction\n",
        "\n",
        "Extract SAE latent representations and save analysis-ready datasets"
      ],
      "metadata": {
        "id": "Sl73-fG7Rc3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os\n",
        "import pathlib\n",
        "import json\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "kQWtjuLpRiOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchTopKSAE(torch.nn.Module):\n",
        "    \"\"\"Sparse Autoencoder with Top-K activation\"\"\"\n",
        "    def __init__(self, input_dim, feature_dim, k_active):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.feature_dim = feature_dim\n",
        "        self.k_active = k_active\n",
        "        self.encoder = torch.nn.Linear(input_dim, feature_dim, bias=True)\n",
        "        self.decoder = torch.nn.Linear(feature_dim, input_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        k = min(self.k_active, self.feature_dim)\n",
        "        topk_values, topk_indices = torch.topk(features, k, dim=-1)\n",
        "        sparse_features = torch.zeros_like(features)\n",
        "        sparse_features.scatter_(-1, topk_indices, topk_values)\n",
        "        reconstructed = self.decoder(sparse_features)\n",
        "        return {\n",
        "            'reconstructed': reconstructed,\n",
        "            'sparse_features': sparse_features,\n",
        "            'dense_features': features\n",
        "        }\n",
        "\n",
        "    def get_feature_activations(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.encoder(x)\n",
        "            k = min(self.k_active, self.feature_dim)\n",
        "            _, topk_indices = torch.topk(features, k, dim=-1)\n",
        "            return topk_indices, features\n"
      ],
      "metadata": {
        "id": "l5oqf6gNRwSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test embeddings and SAE model\n",
        "print(\"Loading test embeddings and trained SAE model...\")\n",
        "\n",
        "# Load test data\n",
        "test_pkg = torch.load(\"../data/processed/sae_sv_embeddings.pt\", map_location=\"cpu\")\n",
        "test_emb = test_pkg[\"embeddings\"].float().to(device)\n",
        "test_sv_info = test_pkg[\"sv_info\"]\n",
        "\n",
        "# Load best SAE model\n",
        "import pathlib\n",
        "import json\n",
        "\n",
        "best_model_path = pathlib.Path(\"../data/models/BEST_MODEL.json\")\n",
        "if best_model_path.exists():\n",
        "    with open(best_model_path) as f:\n",
        "        best_meta = json.load(f)\n",
        "    model_dir = pathlib.Path(best_meta[\"best_dir\"])\n",
        "    ckpt_path = model_dir / \"sae.pt\"\n",
        "else:\n",
        "    raise ValueError(\"No best model found\")\n",
        "\n",
        "pkg = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "cfg = pkg[\"config\"]\n",
        "input_dim = int(cfg[\"input_dim\"])\n",
        "feature_dim = int(cfg[\"feature_dim\"])\n",
        "k_active = int(cfg[\"k\"])\n",
        "print(f\"SAE config: {input_dim} -> {feature_dim}, k={k_active}\")\n",
        "\n",
        "# Load and setup SAE\n",
        "sae = BatchTopKSAE(input_dim, feature_dim, k_active).to(device)\n",
        "sae.load_state_dict(pkg[\"model_state_dict\"], strict=False)\n",
        "sae.eval()\n",
        "print(\"SAE loaded\")"
      ],
      "metadata": {
        "id": "WRuLshufR9rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_sae_latents(embeddings, batch_size=128, desc=\"Processing\"):\n",
        "    n_samples = embeddings.shape[0]\n",
        "    all_dense_features = torch.zeros(n_samples, feature_dim, dtype=torch.bool)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, n_samples, batch_size), desc=desc):\n",
        "            end_idx = min(i + batch_size, n_samples)\n",
        "            batch_emb = embeddings[i:end_idx].to(device)\n",
        "            top_k_indices, _ = sae.get_feature_activations(batch_emb)\n",
        "\n",
        "            batch_dense = torch.zeros(batch_emb.shape[0], feature_dim, dtype=torch.bool)\n",
        "            batch_dense.scatter_(1, top_k_indices, True)\n",
        "            all_dense_features[i:end_idx] = batch_dense\n",
        "\n",
        "    return all_dense_features\n",
        "\n",
        "\n",
        "print(\"Extracting SAE latents...\")\n",
        "train_dense = extract_sae_latents(train_embeddings, desc=\"Train set\")\n",
        "test_dense = extract_sae_latents(test_emb, desc=\"Test set\")\n",
        "print(\"Latents extracted\")"
      ],
      "metadata": {
        "id": "wn0UZx04SBSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUD190MURTT9"
      },
      "outputs": [],
      "source": [
        "# For training data, load from separate file\n",
        "train_pkg = torch.load(\"../data/processed/layer26_sv_total_train_embeddings.pt\", map_location=\"cpu\")\n",
        "train_embeddings = train_pkg['embeddings'].float()\n",
        "train_sv_info = train_pkg['sv_info']\n",
        "\n",
        "train_labels = torch.tensor([sv['truvari_class'] == 'tp_comp_vcf' for sv in train_sv_info])\n",
        "test_labels = torch.tensor([sv['truvari_class'] == 'tp_comp_vcf' for sv in test_sv_info])\n",
        "\n",
        "print(f\"Train: {train_embeddings.shape[0]} samples ({train_labels.sum().item()} TP)\")\n",
        "print(f\"Test: {test_emb.shape[0]} samples ({test_labels.sum().item()} TP)\")\n",
        "\n",
        "\n",
        "# %%\n",
        "# Create datasets\n",
        "train_dense_np = train_dense.numpy().astype(np.uint8)\n",
        "test_dense_np = test_dense.numpy().astype(np.uint8)\n",
        "train_labels_np = train_labels.numpy().astype(np.uint8)\n",
        "test_labels_np = test_labels.numpy().astype(np.uint8)\n",
        "\n",
        "combined_dense = np.concatenate([train_dense_np, test_dense_np], axis=0)\n",
        "combined_labels = np.concatenate([train_labels_np, test_labels_np], axis=0)\n",
        "combined_sv_info = train_sv_info + test_sv_info\n",
        "\n",
        "# %%\n",
        "# Save everything\n",
        "save_dir = \"../data/sae_latents/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Train data\n",
        "torch.save({\n",
        "    'dense_features': train_dense_np,\n",
        "    'labels': train_labels_np,\n",
        "    'sv_info': train_sv_info,\n",
        "    'n_samples': len(train_sv_info),\n",
        "    'n_features': feature_dim,\n",
        "    'k_active': k_active\n",
        "}, f\"{save_dir}sae_latents_train.pt\")\n",
        "\n",
        "# Test data\n",
        "torch.save({\n",
        "    'dense_features': test_dense_np,\n",
        "    'labels': test_labels_np,\n",
        "    'sv_info': test_sv_info,\n",
        "    'n_samples': len(test_sv_info),\n",
        "    'n_features': feature_dim,\n",
        "    'k_active': k_active\n",
        "}, f\"{save_dir}sae_latents_test.pt\")\n",
        "\n",
        "# Combined data\n",
        "torch.save({\n",
        "    'dense_features': combined_dense,\n",
        "    'labels': combined_labels,\n",
        "    'sv_info': combined_sv_info,\n",
        "    'n_samples': len(combined_sv_info),\n",
        "    'n_features': feature_dim,\n",
        "    'k_active': k_active,\n",
        "    'train_indices': list(range(len(train_sv_info))),\n",
        "    'test_indices': list(range(len(train_sv_info), len(combined_sv_info)))\n",
        "}, f\"{save_dir}sae_latents_combined.pt\")\n",
        "\n",
        "# Sklearn-ready data\n",
        "with open(f\"{save_dir}sae_features_sklearn.pkl\", 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'X_train': train_dense_np,\n",
        "        'y_train': train_labels_np,\n",
        "        'X_test': test_dense_np,\n",
        "        'y_test': test_labels_np,\n",
        "        'X_combined': combined_dense,\n",
        "        'y_combined': combined_labels,\n",
        "        'feature_names': [f'atom_{i}' for i in range(feature_dim)]\n",
        "    }, f)\n",
        "\n",
        "print(\"All data saved!\")\n",
        "print(f\"Files saved to: {save_dir}\")\n",
        "print(f\"SAE dimensions: {input_dim} -> {feature_dim}, k={k_active}\")"
      ]
    }
  ]
}