{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SAE Training and Evaluation\n",
        "\n",
        "Train Sparse Autoencoder (SAE) on Evo2 embeddings and evaluate performance for\n",
        "distinguishing true positive vs false positive structural variants."
      ],
      "metadata": {
        "id": "QOMkU3BQzKrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import pathlib\n",
        "import datetime\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import chi2_contingency\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "m1clb-9gzQ79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global settings\n",
        "DRIVE_ROOT = \"../data/models\"\n",
        "EPOCHS = 400\n",
        "BATCH_SIZE = 32\n",
        "LR = 3e-4\n",
        "λ_L1 = 0.05\n",
        "β_KL = 0.5\n",
        "ρ_TARGET = 0.02  # Desired active-feature probability\n",
        "λ_ORTH = 0.01\n",
        "INPUT_DIM = 4096  # Evo-2 layer-26 embedding dimension\n",
        "FEATURE_DIM = 4096  # Selected based on interpretability\n",
        "K_ACTIVE = 64  # Selected based on interpretability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Class weights for TP/FP imbalance (TP: 88%, FP: 12%)\n",
        "CLASS_WEIGHTS = torch.tensor([1.0, 7.17]).to(device)  # Inverse frequency: FP weight = TP_count/FP_count ≈ 7.17\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model will be saved to: {DRIVE_ROOT}\")"
      ],
      "metadata": {
        "id": "8GL1Y0KR3Bm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAE Model Definition\n",
        "\n",
        "class BatchTopKSAE(torch.nn.Module):\n",
        "    \"\"\"Sparse Autoencoder with Top-K activation\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, feature_dim, k_active):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.feature_dim = feature_dim\n",
        "        self.k_active = k_active\n",
        "\n",
        "        # Encoder: input -> features\n",
        "        self.encoder = torch.nn.Linear(input_dim, feature_dim, bias=True)\n",
        "        # Decoder: features -> input (no bias)\n",
        "        self.decoder = torch.nn.Linear(feature_dim, input_dim, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        torch.nn.init.kaiming_uniform_(self.encoder.weight)\n",
        "        torch.nn.init.kaiming_uniform_(self.decoder.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        features = self.encoder(x)\n",
        "\n",
        "        # Top-K sparsification\n",
        "        batch_size = x.size(0)\n",
        "        k = min(self.k_active, self.feature_dim)\n",
        "\n",
        "        # Get top-k values and indices\n",
        "        topk_values, topk_indices = torch.topk(features, k, dim=-1)\n",
        "\n",
        "        # Create sparse feature tensor\n",
        "        sparse_features = torch.zeros_like(features)\n",
        "        sparse_features.scatter_(-1, topk_indices, topk_values)\n",
        "\n",
        "        # Decode\n",
        "        reconstructed = self.decoder(sparse_features)\n",
        "\n",
        "        return {\n",
        "            'reconstructed': reconstructed,\n",
        "            'sparse_features': sparse_features,\n",
        "            'dense_features': features\n",
        "        }\n",
        "\n",
        "    def get_feature_activations(self, x):\n",
        "        \"\"\"Get active feature indices for analysis\"\"\"\n",
        "        with torch.no_grad():\n",
        "            features = self.encoder(x)\n",
        "            k = min(self.k_active, self.feature_dim)\n",
        "            _, topk_indices = torch.topk(features, k, dim=-1)\n",
        "            return topk_indices, features"
      ],
      "metadata": {
        "id": "ONRWyP1g3VL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and whiten training embeddings\n",
        "print(\"Loading training embeddings...\")\n",
        "train_pkg = torch.load(\"../data/processed/sae_sv_embeddings.pt\", map_location=\"cpu\")\n",
        "train_emb = train_pkg[\"embeddings\"].float().to(device)  # [N_train, 4096]\n",
        "train_sv_info = train_pkg[\"sv_info\"]\n",
        "\n",
        "# Whiten embeddings\n",
        "mu, σ = train_emb.mean(0, keepdim=True), train_emb.std(0, keepdim=True) + 1e-6\n",
        "train_emb_w = (train_emb - mu) / σ\n",
        "train_loader = DataLoader(TensorDataset(train_emb_w), batch_size=BATCH_SIZE, shuffle=True)\n",
        "N_train = train_emb_w.size(0)\n",
        "print(f\"Loaded {N_train:,} training embeddings (standardized)\")\n",
        "\n",
        "# Create labels tensor (TP=1, FP=0)\n",
        "train_labels = torch.tensor([1 if sv['truvari_class'] == 'TP' else 0 for sv in train_sv_info], dtype=torch.long).to(device)"
      ],
      "metadata": {
        "id": "GHmjgJfv3YC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train SAE\n",
        "tag = f\"{INPUT_DIM}to{FEATURE_DIM}_k{K_ACTIVE}\"\n",
        "out_dir = pathlib.Path(DRIVE_ROOT) / tag\n",
        "ckpt_path = out_dir / \"sae.pt\"\n",
        "\n",
        "# Check if model already exists\n",
        "if ckpt_path.exists():\n",
        "    print(f\"Found existing SAE model at {ckpt_path}, skipping training\")\n",
        "    # Load existing model and metadata\n",
        "    sae = BatchTopKSAE(INPUT_DIM, FEATURE_DIM, K_ACTIVE).to(device).float()\n",
        "    pkg = torch.load(ckpt_path, map_location=device)\n",
        "    sae.load_state_dict(pkg[\"model_state_dict\"])\n",
        "    loss_curve = pkg[\"loss_curve\"]\n",
        "    meta_path = out_dir / \"meta.json\"\n",
        "    if meta_path.exists():\n",
        "        meta = json.load(open(meta_path))\n",
        "        atoms_used = meta[\"atoms_used\"]\n",
        "        sparsity = meta[\"sparsity_ratio\"]\n",
        "        train_secs = meta[\"train_minutes\"] * 60\n",
        "    else:\n",
        "        # Compute atoms used and sparsity if metadata is missing\n",
        "        sae.eval()\n",
        "        with torch.no_grad():\n",
        "            acts, _ = sae.get_feature_activations(train_emb_w)\n",
        "        atoms_used = int(torch.unique(acts).numel())\n",
        "        sparsity = K_ACTIVE / FEATURE_DIM\n",
        "        train_secs = 0  # Unknown without metadata\n",
        "    print(f\"   atoms-used: {atoms_used}/{FEATURE_DIM}  (sparsity ≈ {K_ACTIVE}/{FEATURE_DIM} = {sparsity:.4f})\")\n",
        "else:\n",
        "    print(f\"Training SAE {INPUT_DIM} → {FEATURE_DIM} (k={K_ACTIVE})\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    sae = BatchTopKSAE(INPUT_DIM, FEATURE_DIM, K_ACTIVE).to(device).float()\n",
        "    opt = torch.optim.Adam(sae.parameters(), lr=LR)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
        "\n",
        "    loss_curve = []\n",
        "    sae.train()\n",
        "    for ep in range(EPOCHS):\n",
        "        running_loss = 0.0\n",
        "        for i, (x,) in enumerate(train_loader):\n",
        "            x = x.to(device)\n",
        "            batch_labels = train_labels[i * BATCH_SIZE: (i + 1) * BATCH_SIZE].to(device)\n",
        "            out = sae(x)\n",
        "            s_raw = out[\"sparse_features\"]\n",
        "            s = F.relu(s_raw)  # Enforce non-negativity\n",
        "\n",
        "            # Losses\n",
        "            recon = F.mse_loss(out[\"reconstructed\"], x)\n",
        "            l1 = λ_L1 * s.mean()\n",
        "            act_mask = (s > 0).float()\n",
        "            p_hat = act_mask.mean(0)\n",
        "            kl = β_KL * (ρ_TARGET * torch.log(ρ_TARGET / p_hat.clamp(1e-4, 1-1e-4)) +\n",
        "                         (1 - ρ_TARGET) * torch.log((1 - ρ_TARGET) / (1 - p_hat.clamp(1e-4, 1-1e-4)))).mean()\n",
        "            W = sae.decoder.weight\n",
        "            orth = λ_ORTH * ((W @ W.T - torch.eye(W.size(0), device=W.device))**2).mean()\n",
        "\n",
        "            # Class-weighted reconstruction loss\n",
        "            weights = CLASS_WEIGHTS[batch_labels].view(-1, 1)\n",
        "            weighted_recon = (weights * F.mse_loss(out[\"reconstructed\"], x, reduction='none').mean(dim=1)).mean()\n",
        "\n",
        "            loss = weighted_recon + l1 + kl + orth\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Weight normalization\n",
        "            with torch.no_grad():\n",
        "                W.div_(W.norm(dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "        sched.step()\n",
        "        loss_curve.append(running_loss / len(train_loader))\n",
        "        if ep % 100 == 0 or ep == EPOCHS - 1:\n",
        "            print(f\"  epoch {ep:3d}  loss={loss_curve[-1]:.4f}\")\n",
        "\n",
        "    train_secs = time.time() - t0\n",
        "    print(f\"Training finished in {train_secs/60:.1f} min\")\n",
        "\n",
        "    # Compute sparsity and usage stats\n",
        "    sae.eval()\n",
        "    with torch.no_grad():\n",
        "        acts, _ = sae.get_feature_activations(train_emb_w)  # [N_train, k]\n",
        "    atoms_used = int(torch.unique(acts).numel())\n",
        "    sparsity = K_ACTIVE / FEATURE_DIM\n",
        "    print(f\"   atoms-used: {atoms_used}/{FEATURE_DIM}  (sparsity ≈ {K_ACTIVE}/{FEATURE_DIM} = {sparsity:.4f})\")\n",
        "\n",
        "    # Save model and config\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    torch.save({\n",
        "        \"model_state_dict\": sae.state_dict(),\n",
        "        \"config\": dict(input_dim=INPUT_DIM, feature_dim=FEATURE_DIM, k=K_ACTIVE, μ=mu.cpu(), σ=σ.cpu()),\n",
        "        \"loss_curve\": loss_curve\n",
        "    }, ckpt_path)\n",
        "\n",
        "    meta = dict(\n",
        "        feature_dim=FEATURE_DIM,\n",
        "        k_active=K_ACTIVE,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        l1=λ_L1,\n",
        "        kl_beta=β_KL,\n",
        "        kl_rho=ρ_TARGET,\n",
        "        orth=λ_ORTH,\n",
        "        atoms_used=atoms_used,\n",
        "        sparsity_ratio=sparsity,\n",
        "        train_minutes=round(train_secs/60, 2),\n",
        "        n_samples=N_train\n",
        "    )\n",
        "    json.dump(meta, open(out_dir / \"meta.json\", \"w\"), indent=2)\n",
        "    print(f\"Saved → {ckpt_path}\")\n",
        "\n",
        "# Clean up to free memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "PD7W--QT3azN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load testing embeddings for evaluation\n",
        "print(\"Loading testing embeddings...\")\n",
        "test_pkg = torch.load(\"../data/processed/sae_sv_embeddings.pt\", map_location=\"cpu\")\n",
        "test_emb = test_pkg[\"embeddings\"].float().to(device)  # [N_test, 4096]\n",
        "test_sv_info = test_pkg[\"sv_info\"]\n",
        "\n",
        "# Debug: Verify test_sv_info\n",
        "print(\"Debugging test_sv_info:\")\n",
        "print(f\"Total test samples: {len(test_sv_info)}\")\n",
        "truvari_classes = [sv['truvari_class'] for sv in test_sv_info]\n",
        "print(f\"Unique truvari_class values: {set(truvari_classes)}\")\n",
        "print(f\"truvari_class counts: {pd.Series(truvari_classes).value_counts().to_dict()}\")\n",
        "\n",
        "# Create labels (TP=1, FP=0) with explicit mapping\n",
        "test_labels = torch.tensor([\n",
        "    1 if sv['truvari_class'] in ['TP', 'tp_comp_vcf'] else 0\n",
        "    for sv in test_sv_info\n",
        "], dtype=torch.long).to(device)\n",
        "\n",
        "# Convert test_labels to boolean for all logical operations\n",
        "test_labels = test_labels.bool()\n",
        "\n",
        "# Verify labels\n",
        "print(f\"Length of test_labels: {len(test_labels)}\")\n",
        "print(f\"Label counts: {torch.unique(test_labels, return_counts=True)}\")\n",
        "print(f\"Length of test_emb: {test_emb.shape[0]}\")\n",
        "assert len(test_labels) == test_emb.shape[0], \"Mismatch between test embeddings and labels\"\n",
        "assert len(torch.unique(test_labels)) > 1, \"Test labels contain only one class\"\n",
        "\n",
        "# Whiten test embeddings using training mean and std\n",
        "test_emb_w = (test_emb - mu) / σ\n",
        "\n",
        "# Check if evaluation results exist\n",
        "pointer_file = pathlib.Path(DRIVE_ROOT) / \"BEST_MODEL.json\"\n",
        "csv_path = pathlib.Path(DRIVE_ROOT) / \"sae_eval_metrics.csv\"\n",
        "current_model_dir = str(out_dir)\n",
        "\n",
        "records = []\n",
        "baseline_f1 = None\n",
        "baseline_f1_std = None"
      ],
      "metadata": {
        "id": "3LSrCkFb3igA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if csv_path.exists():\n",
        "    print(f\"Found existing evaluation results at {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # Filter for current SAE and Goodfire SAE\n",
        "    relevant_models = [current_model_dir, \"Goodfire_Evo2_SAE\"]\n",
        "    df_filtered = df[df['model_dir'].isin(relevant_models)]\n",
        "    if len(df_filtered) == len(relevant_models):\n",
        "        print(f\"Found results for {current_model_dir} and Goodfire_Evo2_SAE, skipping evaluation\")\n",
        "        # Extract baseline metrics (same for all rows)\n",
        "        baseline_f1 = float(df['Baseline_F1'].iloc[0])\n",
        "        baseline_f1_std = float(df['Baseline_F1_std'].iloc[0])\n",
        "        records = df_filtered.to_dict('records')\n",
        "    else:\n",
        "        print(f\"Missing results for {current_model_dir}, computing evaluation\")\n",
        "else:\n",
        "    print(f\"No existing evaluation results at {csv_path}, computing evaluation\")\n",
        "\n",
        "# %%\n",
        "if not records:\n",
        "    # Logistic regression baseline on raw embeddings\n",
        "    print(\"Computing baseline F1 on raw embeddings...\")\n",
        "    clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
        "    try:\n",
        "        scores = cross_val_score(clf, test_emb_w.cpu().numpy(), test_labels.cpu().numpy(), cv=5, scoring='f1')\n",
        "        baseline_f1 = float(np.mean(scores))\n",
        "        baseline_f1_std = float(np.std(scores))\n",
        "        print(f\"Baseline F1 on raw embeddings: {baseline_f1:.3f} ± {baseline_f1_std:.3f}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Error in baseline F1 calculation: {e}\")\n",
        "        baseline_f1, baseline_f1_std = 0.0, 0.0\n",
        "\n",
        "    # Evaluate trained SAE\n",
        "    print(\"Evaluating trained SAE...\")\n",
        "    sae.eval()\n",
        "    with torch.no_grad():\n",
        "        acts, _ = sae.get_feature_activations(test_emb_w)  # Shape: (N_test, k_active)\n",
        "\n",
        "    N = acts.size(0)\n",
        "    ind_mat = torch.zeros(N, FEATURE_DIM, dtype=torch.bool, device=device)\n",
        "    ind_mat.scatter_(1, acts, True)\n",
        "\n",
        "    tp_total = int(test_labels.sum())\n",
        "    fp_total = int((~test_labels).sum())\n",
        "\n",
        "    valid, pvals, odds, delta = [], [], [], []\n",
        "    for atom in range(FEATURE_DIM):\n",
        "        tp_on = int(ind_mat[test_labels, atom].sum())\n",
        "        fp_on = int(ind_mat[~test_labels, atom].sum())\n",
        "        if tp_on + fp_on < 10:\n",
        "            continue\n",
        "        tp_off, fp_off = tp_total - tp_on, fp_total - fp_on\n",
        "        _, p, _, _ = chi2_contingency([[tp_on, fp_on], [tp_off, fp_off]], correction=False)\n",
        "        OR = (tp_on * fp_off) / max(1, fp_on * tp_off)\n",
        "        Δprop = abs(tp_on / tp_total - fp_on / fp_total)\n",
        "        valid.append(atom)\n",
        "        pvals.append(p)\n",
        "        odds.append(OR)\n",
        "        delta.append(Δprop)\n",
        "\n",
        "    FP_atoms = {a for a, o in zip(valid, odds) if o < 0.5}\n",
        "    TP_atoms = {a for a, o in zip(valid, odds) if o > 2.0}\n",
        "\n",
        "    fp_tensor = torch.tensor(sorted(FP_atoms), device=device)\n",
        "    best_f1, best_t, best_tp, best_fp = 0, None, None, None\n",
        "    best_precision, best_recall = 0, 0\n",
        "\n",
        "    for t in range(1, K_ACTIVE + 1):\n",
        "        fp_counts = torch.isin(acts, fp_tensor).sum(dim=1)\n",
        "        keep = (fp_counts < t)\n",
        "        tp_kept = int((keep & test_labels).sum())\n",
        "        fp_kept = int((keep & ~test_labels).sum())\n",
        "        p = precision_score(test_labels.cpu(), keep.cpu())\n",
        "        r = recall_score(test_labels.cpu(), keep.cpu())\n",
        "        f = f1_score(test_labels.cpu(), keep.cpu())\n",
        "        if f > best_f1:\n",
        "            best_f1, best_t, best_tp, best_fp = f, t, tp_kept, fp_kept\n",
        "            best_precision, best_recall = p, r\n",
        "\n",
        "    # Logistic regression on SAE activations\n",
        "    X = ind_mat.cpu().numpy().astype(\"uint8\")\n",
        "    y = test_labels.cpu().numpy().astype(\"uint8\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
        "\n",
        "    cv_auprc_scores = []\n",
        "    cv_precision_scores = []\n",
        "    cv_recall_scores = []\n",
        "    cv_f1_scores = []\n",
        "\n",
        "    for tr, val in cv.split(X_train, y_train):\n",
        "        lr.fit(X_train[tr], y_train[tr])\n",
        "        prob = lr.predict_proba(X_train[val])[:, 1]\n",
        "        pred = lr.predict(X_train[val])\n",
        "        cv_auprc_scores.append(average_precision_score(y_train[val], prob))\n",
        "        cv_precision_scores.append(precision_score(y_train[val], pred))\n",
        "        cv_recall_scores.append(recall_score(y_train[val], pred))\n",
        "        cv_f1_scores.append(f1_score(y_train[val], pred))\n",
        "\n",
        "    lr.fit(X_train, y_train)\n",
        "    prob_test = lr.predict_proba(X_test)[:, 1]\n",
        "    pred_test = lr.predict(X_test)\n",
        "    test_precision = precision_score(y_test, pred_test)\n",
        "    test_recall = recall_score(y_test, pred_test)\n",
        "    test_f1 = f1_score(y_test, pred_test)\n",
        "\n",
        "    records.append({\n",
        "        'model_dir': str(out_dir),\n",
        "        'feature_dim': FEATURE_DIM,\n",
        "        'k_active': K_ACTIVE,\n",
        "        'sparsity_ratio': K_ACTIVE / FEATURE_DIM,\n",
        "        'atoms_used': int(torch.unique(acts).numel()),\n",
        "        'FP_sig_atoms': len(FP_atoms),\n",
        "        'TP_sig_atoms': len(TP_atoms),\n",
        "        'TP_retained': best_tp,\n",
        "        'FP_remaining': best_fp,\n",
        "        'best_hard_F1': best_f1,\n",
        "        'best_hard_Precision': best_precision,\n",
        "        'best_hard_Recall': best_recall,\n",
        "        'best_t': best_t,\n",
        "        'LogReg_CV_AUPRC': float(np.mean(cv_auprc_scores)),\n",
        "        'LogReg_CV_Precision': float(np.mean(cv_precision_scores)),\n",
        "        'LogReg_CV_Recall': float(np.mean(cv_recall_scores)),\n",
        "        'LogReg_CV_F1': float(np.mean(cv_f1_scores)),\n",
        "        'LogReg_test_AUPRC': float(average_precision_score(y_test, prob_test)),\n",
        "        'LogReg_test_Precision': float(test_precision),\n",
        "        'LogReg_test_Recall': float(test_recall),\n",
        "        'LogReg_test_F1': float(test_f1),\n",
        "        'Baseline_F1': baseline_f1,\n",
        "        'Baseline_F1_std': baseline_f1_std\n",
        "    })\n",
        "\n",
        "    # Evaluate Goodfire Evo-2 SAE\n",
        "    print(\"Evaluating Goodfire Evo-2 SAE...\")\n",
        "    # Check if Goodfire results exist in df\n",
        "    if csv_path.exists() and 'df' in locals() and not df.empty and \"Goodfire_Evo2_SAE\" in df['model_dir'].values:\n",
        "        print(\"Goodfire Evo-2 SAE evaluation already in results, reusing\")\n",
        "        goodfire_record = df[df['model_dir'] == \"Goodfire_Evo2_SAE\"].iloc[0].to_dict()\n",
        "        records.append(goodfire_record)\n",
        "    else:\n",
        "        # Only try to evaluate Goodfire if the required objects exist from previous notebooks\n",
        "        try:\n",
        "            # Check if we have the necessary objects from the embedding extraction notebook\n",
        "            if 'evo2_model' in globals() and 'SV_Evo2_Encoder' in globals():\n",
        "                print(\"Found evo2_model and SV_Evo2_Encoder from previous notebooks\")\n",
        "                # Generate or load Goodfire activations\n",
        "                if 'evo2_activations' not in globals() or 'evo2_config' not in globals():\n",
        "                    print(\"Generating Goodfire Evo-2 activations for test set...\")\n",
        "                    sv_encoder = SV_Evo2_Encoder(evo2_model)\n",
        "                    with open('../data/processed/sae_sequences.json', 'r') as f:\n",
        "                        seq_data = json.load(f)\n",
        "                        test_sequences = seq_data['sequences']\n",
        "                    evo2_activations = sv_encoder.extract_embeddings_for_svs(test_sequences, batch_size=4, layer=26)\n",
        "                    evo2_config = {\"feature_dim\": 32768}  # Adjust based on Goodfire SAE specs\n",
        "\n",
        "                evo2_activations = evo2_activations[:len(test_sv_info)]  # Ensure alignment\n",
        "                F = evo2_config[\"feature_dim\"]\n",
        "                k_active = evo2_activations.shape[1]\n",
        "\n",
        "                ind_mat_goodfire = torch.zeros(len(test_sv_info), F, dtype=torch.bool, device=device)\n",
        "                row_idx = torch.arange(len(test_sv_info)).view(-1, 1).expand(-1, k_active)\n",
        "                ind_mat_goodfire[row_idx, evo2_activations] = True\n",
        "\n",
        "                atoms_used = int(ind_mat_goodfire.any(0).sum())\n",
        "                sparsity = ind_mat_goodfire.float().mean().item()\n",
        "\n",
        "                valid, pvals, odds, delta = [], [], [], []\n",
        "                for a in torch.nonzero(ind_mat_goodfire.any(0), as_tuple=False).flatten():\n",
        "                    tp_on = int(ind_mat_goodfire[test_labels, a].sum())\n",
        "                    fp_on = int(ind_mat_goodfire[~test_labels, a].sum())\n",
        "                    if tp_on + fp_on < 10:\n",
        "                        continue\n",
        "                    tp_off, fp_off = tp_total - tp_on, fp_total - fp_on\n",
        "                    _, p, _, _ = chi2_contingency([[tp_on, fp_on], [tp_off, fp_off]], correction=False)\n",
        "                    OR = (tp_on * fp_off) / max(1, fp_on * tp_off)\n",
        "                    Δprop = abs(tp_on / tp_total - fp_on / fp_total)\n",
        "                    valid.append(a.item())\n",
        "                    pvals.append(p)\n",
        "                    odds.append(OR)\n",
        "                    delta.append(Δprop)\n",
        "\n",
        "                df_evo2 = pd.DataFrame({\n",
        "                    'atom': valid,\n",
        "                    'TP_on': [int(ind_mat_goodfire[test_labels, a].sum()) for a in valid],\n",
        "                    'FP_on': [int(ind_mat_goodfire[~test_labels, a].sum()) for a in valid],\n",
        "                    'odds': odds,\n",
        "                    'Δprop': delta,\n",
        "                    'p': pvals\n",
        "                })\n",
        "                df_evo2[\"fdr\"] = multipletests(df_evo2[\"p\"], method=\"fdr_bh\")[1]\n",
        "\n",
        "                FP_atoms_goodfire = set(df_evo2.query(\"odds < 0.5\")[\"atom\"])\n",
        "                TP_atoms_goodfire = set(df_evo2.query(\"odds > 2.0\")[\"atom\"])\n",
        "\n",
        "                best_f1_goodfire, best_t_goodfire, best_tp_evo2, best_fp_evo2 = 0, None, None, None\n",
        "                best_precision_evo2, best_recall_evo2 = 0, 0\n",
        "\n",
        "                fp_tensor_goodfire = torch.tensor(sorted(FP_atoms_goodfire), device=evo2_activations.device)\n",
        "                for t in range(1, k_active + 1):\n",
        "                    fp_counts = torch.isin(evo2_activations, fp_tensor_goodfire).sum(1).cpu().numpy()\n",
        "                    keep = fp_counts < t\n",
        "                    tp_kept = int((keep & test_labels.cpu().numpy()).sum())\n",
        "                    fp_kept = int((keep & ~test_labels.cpu().numpy()).sum())\n",
        "                    p = precision_score(test_labels.cpu(), keep)\n",
        "                    r = recall_score(test_labels.cpu(), keep)\n",
        "                    f = f1_score(test_labels.cpu(), keep)\n",
        "                    if f > best_f1_goodfire:\n",
        "                        best_f1_goodfire, best_t_goodfire, best_tp_evo2, best_fp_evo2 = f, t, tp_kept, fp_kept\n",
        "                        best_precision_evo2, best_recall_evo2 = p, r\n",
        "\n",
        "                X_goodfire = ind_mat_goodfire.cpu().numpy().astype(\"uint8\")\n",
        "                y_goodfire = test_labels.cpu().numpy().astype(\"uint8\")\n",
        "                X_train_gf, X_test_gf, y_train_gf, y_test_gf = train_test_split(X_goodfire, y_goodfire, test_size=0.3, random_state=42, stratify=y_goodfire)\n",
        "                cv_gf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "                lr_gf = LogisticRegression(max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
        "\n",
        "                cv_auprc_scores_gf = []\n",
        "                cv_precision_scores_gf = []\n",
        "                cv_recall_scores_gf = []\n",
        "                cv_f1_scores_gf = []\n",
        "\n",
        "                for tr, val in cv_gf.split(X_train_gf, y_train_gf):\n",
        "                    lr_gf.fit(X_train_gf[tr], y_train_gf[tr])\n",
        "                    prob = lr_gf.predict_proba(X_train_gf[val])[:, 1]\n",
        "                    pred = lr_gf.predict(X_train_gf[val])\n",
        "                    cv_auprc_scores_gf.append(average_precision_score(y_train_gf[val], prob))\n",
        "                    cv_precision_scores_gf.append(precision_score(y_train_gf[val], pred))\n",
        "                    cv_recall_scores_gf.append(recall_score(y_train_gf[val], pred))\n",
        "                    cv_f1_scores_gf.append(f1_score(y_train_gf[val], pred))\n",
        "\n",
        "                lr_gf.fit(X_train_gf, y_train_gf)\n",
        "                prob_test_gf = lr_gf.predict_proba(X_test_gf)[:, 1]\n",
        "                pred_test_gf = lr_gf.predict(X_test_gf)\n",
        "                test_precision_evo2 = precision_score(y_test_gf, pred_test_gf)\n",
        "                test_recall_evo2 = recall_score(y_test_gf, pred_test_gf)\n",
        "\n",
        "                records.append({\n",
        "                    'model_dir': \"Goodfire_Evo2_SAE\",\n",
        "                    'feature_dim': F,\n",
        "                    'k_active': k_active,\n",
        "                    'sparsity_ratio': k_active / F,\n",
        "                    'atoms_used': atoms_used,\n",
        "                    'FP_sig_atoms': len(FP_atoms_goodfire),\n",
        "                    'TP_sig_atoms': len(TP_atoms_goodfire),\n",
        "                    'TP_retained': best_tp_evo2,\n",
        "                    'FP_remaining': best_fp_evo2,\n",
        "                    'best_hard_F1': best_f1_goodfire,\n",
        "                    'best_hard_Precision': best_precision_evo2,\n",
        "                    'best_hard_Recall': best_recall_evo2,\n",
        "                    'best_t': best_t_goodfire,\n",
        "                    'LogReg_CV_AUPRC': float(np.mean(cv_auprc_scores_gf)),\n",
        "                    'LogReg_CV_Precision': float(np.mean(cv_precision_scores_gf)),\n",
        "                    'LogReg_CV_Recall': float(np.mean(cv_recall_scores_gf)),\n",
        "                    'LogReg_CV_F1': float(np.mean(cv_f1_scores_gf)),\n",
        "                    'LogReg_test_AUPRC': float(average_precision_score(y_test_gf, prob_test_gf)),\n",
        "                    'LogReg_test_Precision': float(test_precision_evo2),\n",
        "                    'LogReg_test_Recall': float(test_recall_evo2),\n",
        "                    'LogReg_test_F1': float(f1_score(y_test_gf, pred_test_gf)),\n",
        "                    'Baseline_F1': baseline_f1,\n",
        "                    'Baseline_F1_std': baseline_f1_std\n",
        "                })\n",
        "                print(\"Completed Goodfire Evo-2 SAE evaluation\")\n",
        "            else:\n",
        "                print(\"evo2_model or SV_Evo2_Encoder not available from previous notebooks\")\n",
        "                print(\"Skipping Goodfire SAE evaluation\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating Goodfire SAE: {e}\")\n",
        "            print(\"Skipping Goodfire evaluation\")"
      ],
      "metadata": {
        "id": "GMGbAmu52We8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary table and results\n",
        "df = pd.DataFrame(records).sort_values(\n",
        "    [\"best_hard_F1\", \"TP_retained\", \"FP_sig_atoms\", \"sparsity_ratio\"],\n",
        "    ascending=[False, False, True, True]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# Add baseline-only row if computed\n",
        "if baseline_f1 is not None:\n",
        "    baseline_record = {\n",
        "        'model_dir': 'Raw_Embeddings',\n",
        "        'feature_dim': INPUT_DIM,\n",
        "        'k_active': None,\n",
        "        'sparsity_ratio': None,\n",
        "        'atoms_used': None,\n",
        "        'FP_sig_atoms': None,\n",
        "        'TP_sig_atoms': None,\n",
        "        'TP_retained': None,\n",
        "        'FP_remaining': None,\n",
        "        'best_hard_F1': None,\n",
        "        'best_hard_Precision': None,\n",
        "        'best_hard_Recall': None,\n",
        "        'best_t': None,\n",
        "        'LogReg_CV_AUPRC': None,\n",
        "        'LogReg_CV_F1': None,\n",
        "        'LogReg_CV_Precision': None,\n",
        "        'LogReg_CV_Recall': None,\n",
        "        'LogReg_test_AUPRC': None,\n",
        "        'LogReg_test_F1': None,\n",
        "        'LogReg_test_Precision': None,\n",
        "        'LogReg_test_Recall': None,\n",
        "        'Baseline_F1': baseline_f1,\n",
        "        'Baseline_F1_std': baseline_f1_std\n",
        "    }\n",
        "    records.append(baseline_record)\n",
        "    df = pd.DataFrame(records).sort_values(\n",
        "        [\"best_hard_F1\", \"TP_retained\", \"FP_sig_atoms\", \"sparsity_ratio\"],\n",
        "        ascending=[False, False, True, True]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "display(Markdown(\"## Trained SAE + Goodfire Evo-2 + Raw Embeddings – Evaluation Summary\"))\n",
        "column_order = [\n",
        "    'model_dir', 'feature_dim', 'k_active', 'sparsity_ratio', 'atoms_used',\n",
        "    'FP_sig_atoms', 'TP_sig_atoms', 'TP_retained', 'FP_remaining',\n",
        "    'best_hard_F1', 'best_hard_Precision', 'best_hard_Recall', 'best_t',\n",
        "    'LogReg_CV_AUPRC', 'LogReg_CV_F1', 'LogReg_CV_Precision', 'LogReg_CV_Recall',\n",
        "    'LogReg_test_AUPRC', 'LogReg_test_F1', 'LogReg_test_Precision', 'LogReg_test_Recall',\n",
        "    'Baseline_F1', 'Baseline_F1_std'\n",
        "]\n",
        "existing_columns = [col for col in column_order if col in df.columns]\n",
        "df_display = df[existing_columns]\n",
        "display(df_display)"
      ],
      "metadata": {
        "id": "pdsP2QFS3s_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick the winner (exclude Raw_Embeddings for best model selection)\n",
        "df_models_for_best = df[df['model_dir'] != 'Raw_Embeddings']\n",
        "if not df_models_for_best.empty:\n",
        "    best_row = df_models_for_best.iloc[0]\n",
        "    BEST_PATH = best_row[\"model_dir\"]\n",
        "    print(f\"Best model: {BEST_PATH}\")\n",
        "    display_metrics = [\n",
        "        \"feature_dim\", \"k_active\", \"best_hard_F1\", \"best_hard_Precision\", \"best_hard_Recall\",\n",
        "        \"TP_retained\", \"FP_remaining\", \"LogReg_test_F1\", \"LogReg_test_Precision\",\n",
        "        \"LogReg_test_Recall\", \"Baseline_F1\"\n",
        "    ]\n",
        "    existing_display_metrics = [col for col in display_metrics if col in best_row.index]\n",
        "    print(best_row[existing_display_metrics])\n",
        "\n",
        "    # Save results\n",
        "    pointer_file = pathlib.Path(DRIVE_ROOT) / \"BEST_MODEL.json\"\n",
        "    json.dump(dict(best_dir=BEST_PATH, metrics=best_row.to_dict()), open(pointer_file, \"w\"), indent=2)\n",
        "    print(f\"Pointer saved → {pointer_file}\")\n",
        "else:\n",
        "    print(\"No models available for best model selection\")\n",
        "\n",
        "csv_path = pathlib.Path(DRIVE_ROOT) / \"sae_eval_metrics.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"Metrics CSV → {csv_path}\")"
      ],
      "metadata": {
        "id": "OPMkle2Q3xo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Summary statistics (exclude Raw_Embeddings for hard filter and logistic regression metrics)\n",
        "if not df.empty:\n",
        "    df_models = df[df['model_dir'] != 'Raw_Embeddings']\n",
        "    print(f\"PRECISION/RECALL SUMMARY:\")\n",
        "    print(\"=\"*50)\n",
        "    if not df_models.empty:\n",
        "        print(f\"Hard Filter Approach:\")\n",
        "        print(f\"  Best Precision: {df_models['best_hard_Precision'].max():.3f}\")\n",
        "        print(f\"  Best Recall: {df_models['best_hard_Recall'].max():.3f}\")\n",
        "        print(f\"  Best F1: {df_models['best_hard_F1'].max():.3f}\")\n",
        "        print(f\"Logistic Regression (Test Set):\")\n",
        "        print(f\"  Best Precision: {df_models['LogReg_test_Precision'].max():.3f}\")\n",
        "        print(f\"  Best Recall: {df_models['LogReg_test_Recall'].max():.3f}\")\n",
        "        print(f\"  Best F1: {df_models['LogReg_test_F1'].max():.3f}\")\n",
        "\n",
        "    if 'Baseline_F1' in df.columns and not df['Baseline_F1'].isna().all():\n",
        "        print(f\"Raw Embeddings Baseline:\")\n",
        "        print(f\"  Baseline F1: {df['Baseline_F1'].iloc[0]:.3f} ± {df['Baseline_F1_std'].iloc[0]:.3f}\")\n",
        "else:\n",
        "    print(\"No evaluation results to summarize\")\n",
        "\n",
        "# Clean up\n",
        "try:\n",
        "    del sae, acts, ind_mat, test_emb\n",
        "except NameError:\n",
        "    pass\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nSAE training and evaluation complete!\")"
      ],
      "metadata": {
        "id": "IIL7tXtc30eI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}