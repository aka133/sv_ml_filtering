{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SAE Embedding Extraction\n",
        "\n",
        "Extract embeddings from structural variant calls across multiple datasets for SAE training.\n",
        "This notebook processes multiple GIAB datasets, extracts sequences, and computes Evo2 embeddings."
      ],
      "metadata": {
        "id": "ijL64G9Kv_Sk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV6a2VoEv-NA"
      },
      "outputs": [],
      "source": [
        "# Install Evo2\n",
        "# !pip install evo2\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import cyvcf2\n",
        "import pysam\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch..__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# %%\n",
        "# Configure paths\n",
        "processed_dir = Path(\"../data/processed\")\n",
        "processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Environment variables for Evo2\n",
        "os.environ['NVTE_DISABLE_FP8'] = '1'\n",
        "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define datasets with appropriate reference genomes\n",
        "DATASETS = {\n",
        "    'HG002_GRCh37': {\n",
        "        'tp_comp_vcf': '../data/raw/bench-HG002_GRCh37_noqc/tp-comp.vcf.gz',\n",
        "        'fp_vcf': '../data/raw/bench-HG002_GRCh37_noqc/fp.vcf.gz',\n",
        "        'ref': '../data/raw/GRCh37.fna'\n",
        "    },\n",
        "    'HG002_GRCh38': {\n",
        "        'tp_comp_vcf': '../data/raw/bench-HG002_GRCh38-GIABv3_noqc/tp-comp.vcf.gz',\n",
        "        'fp_vcf': '../data/raw/bench-HG002_GRCh38-GIABv3_noqc/fp.vcf.gz',\n",
        "        'ref': '../data/raw/GRCh38.fa'\n",
        "    },\n",
        "    'HG005_GRCh38': {\n",
        "        'tp_comp_vcf': '../data/raw/bench-HG005_GRCh38_noqc/tp-comp.vcf.gz',\n",
        "        'fp_vcf': '../data/raw/bench-HG005_GRCh38_noqc/fp.vcf.gz',\n",
        "        'ref': '../data/raw/GRCh38.fa'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Environment configured\")\n",
        "print(\"Datasets:\", list(DATASETS.keys()))"
      ],
      "metadata": {
        "id": "Uj5wRla8wFnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if sequence files already exist\n",
        "sequences_path = processed_dir / \"sae_sequences.json\"\n",
        "\n",
        "if sequences_path.exists():\n",
        "    print(\"Found existing sequence file, loading...\")\n",
        "\n",
        "    with open(sequences_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        sequences = data['sequences']\n",
        "        sv_info = data['sv_info']\n",
        "\n",
        "    sequences_loaded = True\n",
        "    print(f\"Loaded {len(sequences)} sequences\")\n",
        "\n",
        "else:\n",
        "    sequences_loaded = False\n",
        "    print(\"No existing sequence file found - will extract sequences\")"
      ],
      "metadata": {
        "id": "800u4al7wKTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not sequences_loaded:\n",
        "    print(\"Categorizing structural variants across all datasets...\")\n",
        "\n",
        "    all_sv_categories = defaultdict(list)\n",
        "    all_variant_labels = {}\n",
        "    global_index = 0\n",
        "    total_scanned = 0\n",
        "\n",
        "    valid_chroms = {f'chr{i}' for i in range(1, 23)} | {'chrX', 'chrY'}\n",
        "\n",
        "    for dataset_name, files in DATASETS.items():\n",
        "        print(f\"Processing dataset: {dataset_name}\")\n",
        "\n",
        "        vcf_files = {\n",
        "            'TP': files['tp_comp_vcf'],\n",
        "            'FP': files['fp_vcf']\n",
        "        }\n",
        "\n",
        "        for label, vcf_path in vcf_files.items():\n",
        "            try:\n",
        "                print(f\"  Processing {label} file...\")\n",
        "                vcf = cyvcf2.VCF(vcf_path)\n",
        "\n",
        "                for i, variant in enumerate(vcf):\n",
        "                    total_scanned += 1\n",
        "\n",
        "                    svlen = abs(variant.INFO.get('SVLEN', 0))\n",
        "                    if svlen < 50:\n",
        "                        global_index += 1\n",
        "                        continue\n",
        "\n",
        "                    if variant.CHROM not in valid_chroms:\n",
        "                        global_index += 1\n",
        "                        continue\n",
        "\n",
        "                    svtype = variant.INFO.get('SVTYPE', 'UNK')\n",
        "\n",
        "                    # Categorize by type and size\n",
        "                    if svtype == 'INS':\n",
        "                        category = 'INS_large' if svlen >= 1000 else 'INS_small'\n",
        "                    elif svtype == 'DEL':\n",
        "                        category = 'DEL_large' if svlen >= 1000 else 'DEL_small'\n",
        "                    elif svtype in ['DUP', 'INV', 'TRA']:\n",
        "                        category = svtype\n",
        "                    else:\n",
        "                        category = 'OTHER'\n",
        "\n",
        "                    all_sv_categories[category].append((global_index, dataset_name))\n",
        "                    all_variant_labels[global_index] = (label, vcf_path, i, dataset_name)\n",
        "                    global_index += 1\n",
        "\n",
        "                vcf.close()\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                print(f\"  Warning: {vcf_path} not found for {dataset_name}, skipping...\")\n",
        "                continue\n",
        "\n",
        "    print(f\"Scanned {total_scanned} total variants across all datasets\")\n",
        "    print(f\"Valid variants: {sum(len(indices) for indices in all_sv_categories.values())}\")\n",
        "\n",
        "    print(\"\\nBy SV type:\")\n",
        "    for category, indices in all_sv_categories.items():\n",
        "        print(f\"  {category}: {len(indices)} variants\")\n",
        "\n",
        "    # Count by class\n",
        "    tp_count = sum(1 for idx in all_variant_labels.keys() if all_variant_labels[idx][0] == 'TP')\n",
        "    fp_count = sum(1 for idx in all_variant_labels.keys() if all_variant_labels[idx][0] == 'FP')\n",
        "    print(f\"\\nBy class: TP={tp_count}, FP={fp_count}\")"
      ],
      "metadata": {
        "id": "6yQcZ-Q8wLUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sequences(indices, set_name):\n",
        "    \"\"\"Extract genomic sequences for given variant indices\"\"\"\n",
        "    sequences = []\n",
        "    sv_info = []\n",
        "    window_size = 1000\n",
        "\n",
        "    # Group indices by dataset and file\n",
        "    file_indices = defaultdict(list)\n",
        "    for idx in indices:\n",
        "        label, vcf_path, file_idx, dataset_name = all_variant_labels[idx]\n",
        "        file_indices[(vcf_path, dataset_name)].append((idx, file_idx))\n",
        "\n",
        "    for (vcf_path, dataset_name), indices_list in file_indices.items():\n",
        "        label = 'TP' if 'tp-comp' in vcf_path else 'FP'\n",
        "        print(f\"  Processing {len(indices_list)} variants from {label} in {dataset_name}\")\n",
        "\n",
        "        # Load reference genome for this dataset\n",
        "        fasta = pysam.FastaFile(DATASETS[dataset_name]['ref'])\n",
        "        vcf = cyvcf2.VCF(vcf_path)\n",
        "        file_indices_dict = {file_idx: global_idx for global_idx, file_idx in indices_list}\n",
        "\n",
        "        for i, variant in enumerate(vcf):\n",
        "            if i not in file_indices_dict:\n",
        "                continue\n",
        "\n",
        "            svlen = abs(variant.INFO.get('SVLEN', 0))\n",
        "            chrom = variant.CHROM\n",
        "            pos = variant.POS\n",
        "            start = max(0, pos - window_size // 2)\n",
        "            end = pos + window_size // 2\n",
        "\n",
        "            try:\n",
        "                sequence = fasta.fetch(chrom, start, end).upper()\n",
        "\n",
        "                # Quality filters\n",
        "                if sequence.count('N') > len(sequence) * 0.1:\n",
        "                    continue\n",
        "                if len(sequence) < window_size * 0.8:\n",
        "                    continue\n",
        "\n",
        "                sequences.append(sequence)\n",
        "                sv_info.append({\n",
        "                    'chrom': chrom,\n",
        "                    'pos': pos,\n",
        "                    'end': pos + svlen,\n",
        "                    'svlen': svlen,\n",
        "                    'svtype': variant.INFO.get('SVTYPE', 'UNK'),\n",
        "                    'id': variant.ID or f\"{chrom}_{pos}\",\n",
        "                    'qual': variant.QUAL if variant.QUAL is not None else 60.0,\n",
        "                    'truvari_class': label,\n",
        "                    'dataset': dataset_name\n",
        "                })\n",
        "\n",
        "                if len(sequences) % 500 == 0:\n",
        "                    print(f\"    Processed {len(sequences)} sequences...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        vcf.close()\n",
        "        fasta.close()\n",
        "\n",
        "    return sequences, sv_info\n",
        "\n",
        "# %%\n",
        "if not sequences_loaded:\n",
        "    print(\"Extracting all sequences from datasets...\")\n",
        "\n",
        "    # Get all variant indices\n",
        "    all_indices = list(all_variant_labels.keys())\n",
        "    sequences, sv_info = extract_sequences(all_indices, \"all datasets\")\n",
        "\n",
        "    # Save sequences and metadata\n",
        "    print(\"Saving sequences and metadata...\")\n",
        "    with open(sequences_path, 'w') as f:\n",
        "        json.dump({'sequences': sequences, 'sv_info': sv_info}, f)\n"
      ],
      "metadata": {
        "id": "qIkDYizHwMbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total sequences: {len(sequences)}\")\n",
        "\n",
        "# Count by class and dataset\n",
        "class_counts = defaultdict(int)\n",
        "dataset_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for sv in sv_info:\n",
        "    class_counts[sv['truvari_class']] += 1\n",
        "    dataset_counts[sv['dataset']][sv['truvari_class']] += 1\n",
        "\n",
        "print(f\"\\nBy class:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"  {class_name}: {count}\")\n",
        "\n",
        "print(f\"\\nBy dataset and class:\")\n",
        "for dataset, counts in dataset_counts.items():\n",
        "    total = sum(counts.values())\n",
        "    print(f\"  {dataset}: {total} total\")\n",
        "    for class_name, count in counts.items():\n",
        "        print(f\"    {class_name}: {count}\")\n",
        "\n",
        "# Count by SV type\n",
        "type_counts = defaultdict(int)\n",
        "for sv in sv_info:\n",
        "    svtype, svlen = sv['svtype'], sv['svlen']\n",
        "    category = 'INS_large' if svtype == 'INS' and svlen >= 1000 else 'INS_small' if svtype == 'INS' else \\\n",
        "               'DEL_large' if svtype == 'DEL' and svlen >= 1000 else 'DEL_small' if svtype == 'DEL' else svtype\n",
        "    type_counts[category] += 1\n",
        "\n",
        "print(f\"\\nBy SV type:\")\n",
        "for category, count in type_counts.items():\n",
        "    print(f\"  {category}: {count}\")\n"
      ],
      "metadata": {
        "id": "2p6T5geDwN1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Evo2 model\n",
        "evo2_available = False\n",
        "\n",
        "try:\n",
        "    from evo2 import Evo2\n",
        "    print(\"Evo2 imported successfully\")\n",
        "\n",
        "    print(\"Loading Evo2 model...\")\n",
        "    evo2_model = Evo2('evo2_7b_base')\n",
        "\n",
        "    # Test model with a short sequence\n",
        "    test_sequence = 'ATCGATCGATCGATCG'\n",
        "    input_ids = torch.tensor(\n",
        "        evo2_model.tokenizer.tokenize(test_sequence),\n",
        "        dtype=torch.int\n",
        "    ).unsqueeze(0)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs, _ = evo2_model(input_ids)\n",
        "\n",
        "    print(f\"Evo2 working! Output shape: {outputs[0].shape}\")\n",
        "    evo2_available = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not load Evo2: {e}\")\n",
        "    evo2_available = False"
      ],
      "metadata": {
        "id": "c7FOg2g4wPUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SV_Evo2_Encoder:\n",
        "    \"\"\"Extract Evo2 embeddings from layer 26 for SV sequences\"\"\"\n",
        "\n",
        "    def __init__(self, evo2_wrapper):\n",
        "        self.evo2 = evo2_wrapper\n",
        "        self.evo2.model.eval()\n",
        "        self.layer_26_activations = None\n",
        "        self.hook_registered = False\n",
        "        print(\"SV_Evo2_Encoder initialized\")\n",
        "\n",
        "    def _register_hook(self):\n",
        "        \"\"\"Register hook to capture layer 26 activations\"\"\"\n",
        "        if self.hook_registered:\n",
        "            return\n",
        "\n",
        "        def hook_fn(module, input, output):\n",
        "            # Handle tuple output - take the first element (hidden states)\n",
        "            if isinstance(output, tuple):\n",
        "                self.layer_26_activations = output[0].detach()\n",
        "            else:\n",
        "                self.layer_26_activations = output.detach()\n",
        "\n",
        "        # Register hook on layer 26\n",
        "        if hasattr(self.evo2.model, 'blocks') and len(self.evo2.model.blocks) > 26:\n",
        "            self.evo2.model.blocks[26].register_forward_hook(hook_fn)\n",
        "            self.hook_registered = True\n",
        "            print(\"Registered hook for layer 26\")\n",
        "        else:\n",
        "            print(\"Could not find layer 26, using output embeddings instead\")\n",
        "\n",
        "    def extract_embeddings_for_svs(self, sequences, batch_size=8):\n",
        "        \"\"\"Extract layer 26 embeddings for SV sequences\"\"\"\n",
        "        self._register_hook()\n",
        "        embeddings = []\n",
        "\n",
        "        print(f\"Extracting Evo2 layer 26 embeddings for {len(sequences)} sequences...\")\n",
        "\n",
        "        for i in range(0, len(sequences), batch_size):\n",
        "            batch_sequences = sequences[i:i+batch_size]\n",
        "            batch_embeddings = []\n",
        "\n",
        "            for j, sequence in enumerate(batch_sequences):\n",
        "                if (i + j) % 100 == 0:\n",
        "                    print(f\"  Processing {i + j + 1}/{len(sequences)}\")\n",
        "\n",
        "                # Tokenize sequence\n",
        "                input_ids = torch.tensor(\n",
        "                    self.evo2.tokenizer.tokenize(sequence),\n",
        "                    dtype=torch.int\n",
        "                ).unsqueeze(0)\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    input_ids = input_ids.cuda()\n",
        "\n",
        "                # Reset hook storage\n",
        "                self.layer_26_activations = None\n",
        "\n",
        "                # Forward pass - hook will capture layer 26\n",
        "                with torch.no_grad():\n",
        "                    outputs, _ = self.evo2(input_ids)\n",
        "\n",
        "                    # Use layer 26 activations if available, otherwise use outputs\n",
        "                    if self.layer_26_activations is not None:\n",
        "                        embeddings_tensor = self.layer_26_activations\n",
        "                    else:\n",
        "                        embeddings_tensor = outputs[0]  # Fallback to output\n",
        "\n",
        "                    # Global average pooling to get fixed-size representation\n",
        "                    pooled = embeddings_tensor.mean(dim=1).squeeze(0)\n",
        "                    batch_embeddings.append(pooled.cpu())\n",
        "\n",
        "            embeddings.extend(batch_embeddings)\n",
        "\n",
        "        final_embeddings = torch.stack(embeddings)\n",
        "        print(f\"Extracted embeddings shape: {final_embeddings.shape}\")\n",
        "        return final_embeddings"
      ],
      "metadata": {
        "id": "ekfV6u1zwQep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if evo2_available:\n",
        "    # Initialize encoder\n",
        "    sv_encoder = SV_Evo2_Encoder(evo2_model)\n",
        "\n",
        "    # Extract embeddings from all sequences\n",
        "    print(\"Extracting Evo2 layer 26 embeddings for all sequences...\")\n",
        "    sv_embeddings = sv_encoder.extract_embeddings_for_svs(sequences, batch_size=4)\n",
        "\n",
        "    # Save embeddings\n",
        "    torch.save({\n",
        "        'embeddings': sv_embeddings,\n",
        "        'sv_info': sv_info,\n",
        "        'layer': 26,\n",
        "        'model': 'evo2_7b_base'\n",
        "    }, processed_dir / \"sae_sv_embeddings.pt\")\n",
        "    print(f\"Saved embeddings: {sv_embeddings.shape}\")"
      ],
      "metadata": {
        "id": "Dngk7mnZwSIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding extraction complete!\")\n",
        "print(f\"\\nDataset summary:\")\n",
        "print(f\"  Total sequences: {len(sequences)}\")\n",
        "print(f\"  Total embeddings: {sv_embeddings.shape}\")\n",
        "\n",
        "print(f\"\\nFiles saved to {processed_dir}:\")\n",
        "print(\"  - sae_sequences.json (sequences and metadata)\")\n",
        "print(\"  - sae_sv_embeddings.pt (embeddings and metadata)\")"
      ],
      "metadata": {
        "id": "A4pF8RG1wTOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
